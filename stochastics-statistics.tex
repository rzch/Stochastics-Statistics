% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "report" class.

\documentclass[11pt]{report} % use larger type; default would be 10pt

\usepackage[dark, book]{mystyle}

%%% END Article customizations

%%% The "real" document content comes below...
\lhead{\chaptertitle}\chead{}\rhead{\thetitle}
\lfoot{}\cfoot{\thepage}\rfoot{}

\pretitle{\begin{center}\Huge\bf}
\posttitle{\normalfont\end{center}}
\title{Stochastics \& Statistics}
\author{R. C.}
%\date{\vspace{-5ex}} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

%for PDF authoring
\hypersetup{
    pdftitle={\thetitle},
    pdfauthor={\theauthor},
%    pdfsubject={},
%    pdfkeywords={keyword1, keyword2},
    bookmarksnumbered=true,     
    bookmarksopen=false,         
    bookmarksopenlevel=1,       
	hidelinks=true,         
    pdfstartview=,           
    pdfpagemode=UseNone,
%    pdfpagelayout=TwoPageRight
}


\begin{document}
\maketitle
\thispagestyle{fancy}

\tableofcontents

\part{Fundamentals}

\chapter{Introductory Probability}

\section{Probability Laws}

\subsection{Events}

When expressing probabilities, we express probabilities of occurrences of events. Notionally, we can think of some random `experiment' which leads to outcomes. Certain outcomes may be associated with a particular event, so we can represent an event as a set which contains as its elements the outcomes associated with it.

\subsubsection{Union}

Given events $A$ and $B$, the event of $A$ or $B$ occurring is given by the set union $A \cup B$.

\subsubsection{Intersection}

Given events $A$ and $B$, the event of $A$ and $B$ both occurring is given by the set intersection $A \cap B$.

\subsubsection{Complement}

The complementary event of $A$ is denoted $\overline{A}$. This is the event that $A$ does not happen. \\

The relative complement of event $A$ with respect to event $B$ is denoted $B\setminus A$. This is the event of $B$ occurring, but not $A$ occurring. We have the relation
\begin{equation}
B\setminus A = \overline{B}\cap A
\end{equation}

\subsubsection{Venn Diagrams}

\subsection{Classical Definition of Probability}

In an experiment with outcomes given by the set $\Omega$, the outcomes associated with event $A$ is a subset of $\Omega$ (ie. $A \subseteq \Omega$). Let $\left|\Omega\right|$ be the number of outcomes in $\Omega$, and similarly let $\left|A\right|$ be the number of outcomes in $A$. Then by the classical definition of probability, the probability of event $A$, denoted $\operatorname{Pr}\left(A\right)$, is
\begin{equation}
\operatorname{Pr}\left(A\right) = \dfrac{\left|A\right|}{\left|\Omega\right|}
\end{equation}

This means that probabilities are always between $0$ and $1$ (inclusive). We can interpret this probability by saying if we were able to conduct an infinite number of experiments, then the proportion of experiments which resulted in the occurrence of $A$ would be $\operatorname{Pr}\left(A\right)$.

\subsection{Addition Rule of Probability}

The probability of the intersection of $A$ and $B$ is written $\operatorname{Pr}\left(A \cap B\right)$, or alternatively may be denoted $\operatorname{Pr}\left(A, B\right)$. To obtain the probability of the union between $A$ and $B$, we have the formula
\begin{equation}
\operatorname{Pr}\left(A \cup B\right) = \operatorname{Pr}\left(A\right) + \operatorname{Pr}\left( B\right) - \operatorname{Pr}\left(A \cap B\right) 
\end{equation}

\subsection{Complementary Probabilities}

The complementary probability of $\operatorname{Pr}\left(A\right)$ is denoted $\operatorname{Pr}\left(\overline{A}\right)$. We have that
\begin{equation}
\operatorname{Pr}\left(\overline{A}\right) = 1 - \operatorname{Pr}\left(A\right)
\end{equation}

\subsection{Mutual Exclusivity}

Events $A$ and $B$ are mutually exclusive if they cannot both happen together. That means
\begin{equation}
\operatorname{Pr}\left(A \cap B \right) = 0
\end{equation}
Hence the addition rule of probability in the case of mutual exclusivity simplifies to
\begin{equation}
\operatorname{Pr}\left(A \cup B\right) = \operatorname{Pr}\left(A\right) + \operatorname{Pr}\left( B\right)
\end{equation}

\subsection{Conditional Probability}

The probability of event $A$ occurring given event $B$ has occurred is denoted by $P\left(A\middle|B\right)$ and is known as a conditional probability. The conditional probability can be calculated by
\begin{equation}
P\left(A\middle|B\right) = \dfrac{P\left(A \cap B\right)}{P\left(B\right)}
\end{equation}
The term $P\left(B\right)$ can be thought of as a `normalising constant' for $P\left(A \cap B\right)$, as we only want to consider the space of events where $B$ has occurred.

\subsection{Chain Rule of Probability}

Given the conditional probability $\operatorname{Pr}\left(A\middle|B\right)$ and the marginal probability $\operatorname{Pr}\left(B\right)$, the chain rule of probability says that
\begin{equation}
\operatorname{Pr}\left(A\cap B\right) = \operatorname{Pr}\left(A\middle|B\right)\operatorname{Pr}\left(B\right)
\end{equation}

\subsection{Law of Total Probability}

\subsubsection{Probability Tree Diagrams}

\subsection{Bayes' Theorem}

By definitions of conditional probability, we have
\begin{gather}
P\left(A\middle|B\right) = \dfrac{P\left(A\cap B\right)}{P\left(B\right)} \\
P\left(B\middle|A\right) = \dfrac{P\left(A\cap B\right)}{P\left(A\right)}
\end{gather}
Rearranging both gives
\begin{gather}
P\left(A\cap B\right) = P\left(A\middle|B\right)P\left(B\right) \\
P\left(A\cap B\right) = P\left(B\middle|A\right)P\left(A\right)
\end{gather}
Then equating them gives Bayes' theorem
\begin{gather}
P\left(A\middle|B\right)P\left(B\right) = P\left(B\middle|A\right)P\left(A\right) \\
P\left(A\middle|B\right) = \dfrac{P\left(B\middle|A\right)P\left(A\right) }{P\left(B\right)}
\end{gather}

Consider a probability tree for three events $A$, $B$ and $C$.
\begin{figure}[H]
\includegraphics[width=0.7\textwidth]{Figures/multiple_conditional}\centering
\caption{A probability tree for three events expressed in conditional probabilities.}
\end{figure}
By multiplying probabilities along the tree, we can see that
\begin{equation}
P\left(A,B,C\right) = P\left(A\middle|B, C\right)P\left(B\middle|C\right)P\left(C\right)
\end{equation}
The two branches of $\left.B\middle|C\right.$ and $\left.A\middle|B,C\right.$ are the branches of $A$ and $B$ conditional on $C$. Hence we can also state that
\begin{equation}
P\left(A,B\middle|C\right) = P\left(A\middle|B,C\right)P\left(B\middle|C\right)
\end{equation}
A way of writing Bayes' theorem for three events is
\begin{equation}
P\left(A, B\middle|C\right) = \dfrac{P\left(C\middle|A,B\right)P\left(A,B\right)}{P\left(C\right)}
\end{equation}
We can use the facts $P\left(A,B\right) = P\left(A\right)P\left(B\middle|A\right)$ and $P\left(C\middle|A,B\right) = \dfrac{P\left(B,C\middle|A\right)}{P\left(B\middle|A\right)}$, where the latter is a conditional probability except everything is conditioned on $A$. Applying these gives
\begin{align}
P\left(A, B\middle|C\right) &= \dfrac{P\left(B,C\middle|A\right)}{P\left(B\middle|A\right)}\dfrac{P\left(A\right)P\left(B\middle|A\right)}{P\left(C\right)} \\
&= \dfrac{P\left(B,C\middle|A\right)P\left(A\right)}{P\left(C\right)}
\end{align}

\subsection{DeMorgan's Laws}

DeMorgan's laws can be arrived at via some logical postulates.
\begin{itemize}
\item The event of $A$ or $B$ not happening is the same as neither $A$ happening nor $B$ happening.
\item The event of both $A$ and $B$ not happening is the same as either $A$ not happening or $B$ not happening. 
\end{itemize}
This can be written down using set notation:
\begin{gather}
\overline{A \cup B} = \overline{A} \cap \overline{B} \\
\overline{A \cap B} = \overline{A} \cup \overline{B}
\end{gather}
And in terms of probability:
\begin{gather}
\operatorname{Pr}\left(\overline{A \cup B}\right) = \operatorname{Pr}\left(\overline{A} \cap \overline{B}\right) \\
\operatorname{Pr}\left(\overline{A \cap B}\right) = \operatorname{Pr}\left(\overline{A} \cup \overline{B}\right)
\end{gather}
Or by using complements:
\begin{gather}
1 - \operatorname{Pr}\left(A \cup B\right) = \operatorname{Pr}\left(\overline{A} \cap \overline{B}\right) \\
1 - \operatorname{Pr}\left(A \cap B\right) = \operatorname{Pr}\left(\overline{A} \cup \overline{B}\right)
\end{gather}

\section{Introductory Combinatorics}

\section{Random Variables}

\subsection{Conditional Random Variables}

\section{Probability Distributions}

\subsection{Continuous Distributions}

\subsection{Discrete Distributions}

\subsection{Cumulative Distributions}

\subsection{Conditional Distributions}

\section{Expectation}

\subsection{Linearity of Expectation}

\subsection{Conditional Expectation}

\subsection{Law of Iterated Expectations}

\section{Variance}

\subsection{Standard Deviation}

\subsection{Covariance}

\subsection{Correlation}

\subsection{Subadditivity of Standard Deviations}

Suppose $X$ and $Y$ are random variables, and we form $Z = X + Y$. Then we have
\begin{equation}
\operatorname{sd}\left(Z\right) \leq \operatorname{sd}\left(X\right) + \operatorname{sd}\left(Y\right)
\end{equation}
We can show subadditivity as follows. 
\begin{gather}
\sqrt{\operatorname{Var}\left(Z\right)} \leq \sqrt{\operatorname{Var}\left(X\right)} + \sqrt{\operatorname{Var}\left(Y\right)} \\
\sqrt{\operatorname{Var}\left(X\right) + 2\operatorname{Cov}\left(X, Y\right) +\operatorname{Var}\left(Y\right)} \leq \sqrt{\operatorname{Var}\left(X\right)} + \sqrt{\operatorname{Var}\left(Y\right)} \\
\operatorname{Var}\left(X\right) + 2\operatorname{Cov}\left(X, Y\right) +\operatorname{Var}\left(Y\right) \leq \left(\sqrt{\operatorname{Var}\left(X\right)} + \sqrt{\operatorname{Var}\left(Y\right)}\right)^{2} \\
\operatorname{Var}\left(X\right) + 2\operatorname{Cov}\left(X, Y\right) +\operatorname{Var}\left(Y\right) \leq \operatorname{Var}\left(X\right) + 2\sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)}
+ \operatorname{Var}\left(Y\right) \\
\operatorname{Cov}\left(X, Y\right) \leq \sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)}
\end{gather}
We know the last inequality holds, because correlations are upper bounded by $1$, ie.
\begin{equation}
\operatorname{Corr}\left(X, Y\right) = \dfrac{\operatorname{Cov}\left(X, Y\right)}{\sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)}} \leq 1
\end{equation}

\subsection{Conditional Variance}

\subsection{Law of Total Variance}

The Law of Total Variance says that for random variables $X$ and $Y$
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}\left[\operatorname{Var}\left(Y\middle|X\right)\right] + \operatorname{Var}\left(\mathbb{E}\left[Y\middle|X\right]\right)
\end{equation}
To show this, first start with the variance of $Y$ in terms of expectations involving $Y$.
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}\left[Y^{2}\right] - \mathbb{E}\left[Y\right]^{2}
\end{equation}
By applying the Law of Iterated Expectations:
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}_{X}\left[\mathbb{E}\left[Y^{2}\middle|X\right]\right] - \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]\right]^{2}
\end{equation}
Now use the definition of conditional variance $\operatorname{Var}\left(Y\middle|X\right) = \mathbb{E}\left[Y^{2}\middle|X\right] - \mathbb{E}\left[Y\middle|X\right]^{2}$ to get
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}_{X}\left[\operatorname{Var}\left(Y\middle|X\right) + \mathbb{E}\left[Y\middle|X\right]^{2}\right] - \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]\right]^{2}
\end{equation}
Using the linearity of expectations:
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}_{X}\left[\operatorname{Var}\left(Y\middle|X\right)\right] + \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]^{2}\right] - \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]\right]^{2}
\end{equation}
Then finally recognising $\operatorname{Var}\left(\mathbb{E}\left[Y\middle|X\right]\right) = \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]^{2}\right] - \mathbb{E}_{X}\left[\mathbb{E}\left[Y\middle|X\right]\right]^{2}$ yields
\begin{equation}
\operatorname{Var}\left(Y\right) = \mathbb{E}\left[\operatorname{Var}\left(Y\middle|X\right)\right] + \operatorname{Var}\left(\mathbb{E}\left[Y\middle|X\right]\right)
\end{equation}

\subsection{Conditional Covariance}

The conditional covariance between random variables $X$ and $Y$ on $Z$ is denoted $\operatorname{Cov}\left(X, Y\middle| Z\right)$. It is taken as the covariance between the conditional random variables $X|Z$ and $Y|Z$.

\begin{equation}
\operatorname{Cov}\left(X, Y\middle| Z\right) = \mathbb{E}\left[\left(X - \mathbb{E}\left[X\middle| Z\right]\right)\left(Y - \mathbb{E}\left[Y\middle| Z\right]\right)\middle|Z\right]
\end{equation}
Expanding this out and using the Law of Iterated Expectations, we can write
\begin{align}
\operatorname{Cov}\left(X, Y\middle| Z\right) &= \mathbb{E}\left[XY - X\mathbb{E}\left[Y\middle|Z\right] - Y\mathbb{E}\left[X\middle|Z\right] + \mathbb{E}\left[X\middle| Z\right]\mathbb{E}\left[Y\middle| Z\right]\middle|Z\right] \\
&= \mathbb{E}\left[XY\middle|Z\right] - \mathbb{E}\left[X\mathbb{E}\left[Y\middle|Z\right]\middle|Z\right] - \mathbb{E}\left[Y\mathbb{E}\left[X\middle|Z\right]\middle|Z\right] +\mathbb{E}\left[\mathbb{E}\left[X\middle| Z\right]\mathbb{E}\left[Y\middle| Z\right]\middle|Z\right]
\end{align}
The same way $\mathbb{E}\left[XY\middle|X\right] = X\mathbb{E}\left[Y\middle|X\right]$, we can take out terms ${E}\left[Y\middle|Z\right]$ and ${E}\left[X\middle|Z\right]$ from their outer expectations.
\begin{align}
\operatorname{Cov}\left(X, Y\middle| Z\right) &= \mathbb{E}\left[XY\middle|Z\right] - \mathbb{E}\left[X\middle|Z\right]\mathbb{E}\left[Y\middle|Z\right] - \mathbb{E}\left[Y\middle|Z\right]\mathbb{E}\left[X\middle|Z\right] +\mathbb{E}\left[1\middle|Z\right]\mathbb{E}\left[X\middle| Z\right]\mathbb{E}\left[Y\middle| Z\right] \\
 &= \mathbb{E}\left[XY\middle|Z\right] - \mathbb{E}\left[X\middle|Z\right]\mathbb{E}\left[Y\middle|Z\right]
\end{align}

\subsection{Law of Total Covariance}
The Law of Total Covariance says that
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\operatorname{Cov}\left(X, Y\middle|Z\right)\right] + \operatorname{Cov}\left(\mathbb{E}\left[X\middle|Z\right],\mathbb{E}\left[Y\middle|Z\right]\right)
\end{equation}
To derive this, first begin with the definition of covariance.
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]
\end{equation}
Using the Law of Iterated Expectations, rewrite the expectations as
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\mathbb{E}\left[XY\middle|Z\right]\right] - \mathbb{E}\left[\mathbb{E}\left[X\middle|Z\right]\right]\mathbb{E}\left[\mathbb{E}\left[Y\middle|Z\right]\right]
\end{equation}
Using the definition of conditional covariance, an alternative expression for the inner term of the first expectation gives
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\operatorname{Cov}\left(X, Y\middle| Z\right) + \mathbb{E}\left[X\middle|Z\right]\mathbb{E}\left[Y\middle|Z\right]\right] - \mathbb{E}\left[\mathbb{E}\left[X\middle|Z\right]\right]\mathbb{E}\left[\mathbb{E}\left[Y\middle|Z\right]\right]
\end{equation}
Then using the linearity of expectation
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\operatorname{Cov}\left(X, Y\middle| Z\right)\right] + \mathbb{E}\left[\mathbb{E}\left[X\middle|Z\right]\mathbb{E}\left[Y\middle|Z\right]\right] - \mathbb{E}\left[\mathbb{E}\left[X\middle|Z\right]\right]\mathbb{E}\left[\mathbb{E}\left[Y\middle|Z\right]\right]
\end{equation}
Finally, recognise that the last two terms give the covariance between the random variables $\mathbb{E}\left[X\middle|Z\right]$ and $\mathbb{E}\left[Y\middle|Z\right]$.
\begin{equation}
\operatorname{Cov}\left(X, Y\right) = \mathbb{E}\left[\operatorname{Cov}\left(X, Y\middle|Z\right)\right] + \operatorname{Cov}\left(\mathbb{E}\left[X\middle|Z\right], \mathbb{E}\left[Y\middle|Z\right]\right)
\end{equation}
The Law of Total Covariance is a special case of the Law of Total Variance where $X = Y$.

\section{Independence}

\subsection{Independent Events}

Two events $A$ and $B$ are independent if any information about the occurrence of $B$  does not affect the probability of occurrence of $A$. That is, 
\begin{equation}
\operatorname{Pr}\left(A\middle|B\right) = \operatorname{Pr}\left(A\right)
\end{equation}
The same will then be true about $B$ given $A$, because by using Bayes' rule
\begin{align}
\operatorname{Pr}\left(B\middle|A\right) &= \dfrac{\operatorname{Pr}\left(A\middle|B\right)\operatorname{Pr}\left(B\right)}{\operatorname{Pr}\left(A\right)} \\
&= \dfrac{\operatorname{Pr}\left(A\right)\operatorname{Pr}\left(B\right)}{\operatorname{Pr}\left(A\right)} \\
&= \operatorname{Pr}\left(B\right)
\end{align}

Then by using the definition of conditional probabilities,
\begin{gather}
\operatorname{Pr}\left(A\middle|B\right) = \dfrac{\operatorname{Pr}\left(A\cap B\right)}{\operatorname{Pr}\left(B\right)} \\
\operatorname{Pr}\left(A\right) = \dfrac{\operatorname{Pr}\left(A\cap B\right)}{\operatorname{Pr}\left(B\right)} \\
\operatorname{Pr}\left(A\cap B\right) = \operatorname{Pr}\left(A\right)\operatorname{Pr}\left(B\right)
\end{gather}
This says the joint probabilities of $A$ and $B$ can be found by multiplying their marginal probabilities.

\subsection{Independent Random Variables}

Subadditivity of the standard deviation is also more straightforward to show in the case where $X$ and $Y$ are independent (or simply uncorrelated). We firstly derive what is known as the subadditivity property of the square root function. For any two scalars $a, b \geq 0$:
\begin{align}
\left(\sqrt{a} + \sqrt{b}\right)^{2} &= a + 2\sqrt{ab} + b \\
\sqrt{a} + \sqrt{b} &= \sqrt{a + 2\sqrt{ab} + b} \geq \sqrt{a + b}
\end{align}
Since $\sqrt{ab} \geq 0$, then
\begin{equation}
\sqrt{a + b} \leq \sqrt{a} + \sqrt{b}
\end{equation}
In fact, we can tell this only holds with equality if and only if $a = 0$ or $b = 0$. The variance of $Z$ is given by $\operatorname{Var}\left(Z\right) = \operatorname{Var}\left(X\right) + \operatorname{Var}\left(Y\right)$ so
\begin{equation}
\sqrt{\operatorname{Var}\left(Z\right)} \leq \sqrt{\operatorname{Var}\left(X\right)} + \sqrt{\operatorname{Var}\left(Y\right)}
\end{equation}
which yields the subadditivity property for the standard deviation.

\subsubsection{Independent and Identically Distributed Random Variables}

\subsection{Conditional Independence}

Two events $A$ and $B$ are conditionally independent on a third event $C$ if the knowledge of $C$ makes events $A$ and $B$ independent (hence their joint probability conditional on $C$ can be obtained by multiplying the marginal conditional probabilities):
\begin{equation}
\operatorname{Pr}\left(A\cap B\middle|C\right) = \operatorname{Pr}\left(A\middle|C\right)\operatorname{Pr}\left(B\middle|C\right)
\end{equation}
By rearranging this and applying the definition of conditional probability, an alternative expression can be obtained.
\begin{gather}
\dfrac{\operatorname{Pr}\left(A\cap B\middle|C\right)}{\operatorname{Pr}\left(B\middle|C\right)} = \operatorname{Pr}\left(A\middle|C\right) \\
\operatorname{Pr}\left(A\middle|B \cap C\right) = \operatorname{Pr}\left(A\middle|C\right)
\end{gather}
Note that $A$ and $B$ may or may not be independent without $C$. Also, if $A$ and $B$ are not conditionally independent on $C$, then they are conditionally dependent on $C$. \\

Similarly, two random variables $X$ and $Y$ are conditionally independent on a third random variable $Z$ if the conditional random variables $X|Z$ and $Y|Z$ are independent. 

\subsection{Orthogonality \cite{Yates2005}}

Two random variables $X$ and $Y$ are said to be orthogonal if $\mathbb{E}\left[XY\right] = 0$. If $X$ and $Y$ are zero-mean and uncorrelated, then they are also orthogonal.

\chapter{Introductory Statistics}

\section{Descriptive Statistics}

\subsection{Measures of Central Tendency}

\subsubsection{Mean}

\subsubsection{Median}

\subsubsection{Mode}

\subsection{Measures of Dispersion}

\subsubsection{Sample Variance}

Suppose we knew the value of the population mean. Then it should be clear that in order to compute an unbiased estimate of the population variance from the sample variance, one should first compute the sum of squared deviations about the population mean $\sum_{i = 1}^{n}{\left(x_{i} - \mu\right)^{2}}$ and then divide by the sample size $n$. However, typically only the sample mean $\bar{x}$ is known, which itself is computed from the sample. So this inherently introduces bias if we were to compute sample variance the same way (we are measuring squared deviations of a sample from a mean which is itself determined by the sample). So to `compensate' for this, we divide by $n - 1$ instead of $n$ to give an unbiased estimate of the sample variance.

\subsubsection{Coefficient of Variation}

The coefficient of variation is a standardised measure of dispersion relative to the mean. It is the ratio of standard deviation to the mean.
\begin{equation}
\mathrm{COV} = \dfrac{\sigma}{\mu}
\end{equation}

\subsubsection{Signal-to-noise Ratio}

The signal-to-noise ratio is defined as the inverse of the coefficient of variation.
\begin{equation}
\mathrm{SNR} = \dfrac{\mu}{\sigma}
\end{equation}

\subsubsection{Contingency Tables}

\section{Inferential Statistics}

\subsection{Confidence Intervals}

Suppose for a random variable $X$, we know it has a population mean $\mathbb{E}\left[X\right] = \mu$ (we may not actually know the mean itself), however we do know the standard deviation $\operatorname{sd}\left(X\right) = \sigma$. Also suppose that we do know enough such that the probability of $X$ being with $t > 0$ standard deviations within the mean is given by $1 - \alpha$, where $\alpha \in \left[0, 1\right]$. That is,
\begin{equation}
\operatorname{Pr}\left(\mu - t\sigma \leq X \leq \mu + t\sigma\right) = 1 - \alpha
\end{equation}
We can rearrange the above event as follows.
\begin{gather}
\mu - t\sigma \leq X \leq \mu + t\sigma \\
\mu \leq X + t\sigma \cap X - t\sigma \leq \mu \\
X - t\sigma \leq \mu \leq X + t\sigma
\end{gather}
Hence
\begin{equation}
\operatorname{Pr}\left(X - t\sigma \leq \mu \leq X + t\sigma\right) = 1 - \alpha
\end{equation}
That is, if we obtain a realisation of $X$ and construct the interval $\left[X - t\sigma, X + t\sigma\right]$, then there is $1 - \alpha$ probability that the interval will contain the population mean $\mu$. Note that this probability refers to before the random variable $X$ is realised. Suppose we have already obtained a realisation (call this $x$), then the matter of whether the interval 
$\left[x - t\sigma, x + t\sigma\right]$ contains $\mu$ is no longer down to probability (it either will contain $\mu$ or it wont. Hence the interval $\left[x - t\sigma, x + t\sigma\right]$ is considered a $\left(1 - \alpha\right)\times 100\%$ confidence interval.

\subsection{Hypothesis Tests}

\subsubsection{Statistical Power}

If we knew all the information about the population and data-generating process, then in principle it is possible to compute power of a particular test. In special cases (ie. if the distributions meet the right assumptions) we can come up with `nice' expressions for power (like an integral) that a computer can evaluate. However in some cases the distributions might be too `unconventional' that we need to resort to asymptotic theory and/or simulations to make estimates. \\

However, the above assumes we know all about the population and data-generating process, which is highly idealised. If we already knew everything, this would defeat the point of using statistics! Also, if we knew that the null were actually true, then the whole notion of power becomes ill-defined. So in practice, while we don't normally come up with numerical estimates of power, we can still reason about how changing our experimental design will affect power. Eg. we'd think that increasing the sample size or choosing a larger $\alpha$ should increase power.

\section{Simple Linear Regression}

\subsection{Confidence Intervals of the Conditional Mean}

\subsection{Prediction Intervals of the Observed Response}

\subsection{Deming Regression}

\section{Analysis of Variance}

\section{Method of Moments}

\chapter{Intermediate Probability}

\section{Families of Continuous Univariate Probability Distributions}

\subsection{Dirac Delta Distribution}

\subsection{Continuous Uniform Distribution}

\subsection{Exponential Distribution}

\subsection{Gaussian Distribution}

\subsubsection{Truncated Gaussian Distribution}

\subsubsection{Folded Gaussian Distribution}

\subsubsection{Half-Gaussian Distribution}

\subsubsection{Mixture Gaussian Distribution}

\subsection{Laplace Distribution}

\subsection{Cauchy Distribution}

\subsection{Gamma Distribution}

\subsubsection{Gamma Function}

The (complete) Gamma function is an interpolation of the factorials defined by
\begin{equation}
\Gamma\left(z\right) = \int_{0}^{\infty}y^{z - 1}e^{-y}dy
\end{equation}
which is continuous over the positive reals, and satisfies the recurrence relation
\begin{gather}
\Gamma\left(1\right) = 1 \\
\Gamma\left(z + 1\right) = z\Gamma\left(z\right)
\end{gather}
Thus, the relation between the Gamma function and the factorials (for non-negative integer values of $z$) is
\begin{equation}
z! = \Gamma\left(z + 1\right)
\end{equation}

\subsubsection{Incomplete Gamma Functions}

The incomplete Gamma functions are defined as integrals with the same integrand as the Gamma function, but with incomplete limits. The lower incomplete Gamma function (sometimes known as the incomplete Gamma function of the first kind) is defined as
\begin{equation}
\gamma\left(z, x\right) = \int_{0}^{x}y^{z - 1}e^{-y}dy
\end{equation}
while the upper incomplete Gamma function (sometimes known as the incomplete Gamma function of the second kind) is defined as
\begin{equation}
\Gamma\left(z, x\right) = \int_{x}^{\infty}y^{z - 1}e^{-y}dy
\end{equation}
where we can see that
\begin{equation}
\gamma\left(z, x\right) + \Gamma\left(z, x\right) = \Gamma\left(z\right)
\end{equation}

\subsubsection{Bayesian's Gamma Distribution}

From the definitions of the incomplete and complete Gamma functions we can see that $\dfrac{\gamma\left(z, x\right)}{\Gamma\left(z\right)}$ (sometimes known as the regularised Gamma function) is a valid cumulative distribution function in $x$ defined over the positive reals. More generally, by introducing a parameter $\beta >0$ and another parameter $\alpha = z > 0$, we can define a cumulative distribution function
\begin{equation}
F\left(x\right) = \dfrac{\gamma\left(\alpha, \beta x\right)}{\Gamma\left(\alpha\right)}
\end{equation}
since $\gamma\left(\alpha, \beta x\right) \leq \Gamma\left(\alpha\right)$ for all $\beta > 0$ and $x > 0$. This is known as the (Bayesian's) Gamma distribution with shape parameter $\alpha$ and scale parameter $\beta$. We can differentiate the cumulative distribution function using the Fundamental Theorem of Calculus to obtain the density function $f\left(x\right)$:
\begin{align}
f\left(x\right) &= \dfrac{dF\left(x\right)}{dx} \\
&= \dfrac{1}{\Gamma\left(\alpha\right)}\dfrac{d\gamma\left(\alpha, \beta x\right)}{dx} \\
&= \dfrac{1}{\Gamma\left(\alpha\right)}\dfrac{d}{dx}\int_{0}^{\beta x}y^{\alpha - 1}e^{-y}dy \\
&= \dfrac{1}{\Gamma\left(\alpha\right)}\cdot\dfrac{1}{\beta}\left(\beta x\right)^{\alpha - 1}e^{-\beta x} \\
&= \dfrac{\beta^{\alpha}x^{\alpha - 1}e^{-\beta x}}{\Gamma\left(\alpha\right)}
\end{align}

\subsubsection{Econometrician's Gamma Distribution}

The econometrician's Gamma distribution is an alternative parametrisation with $k = \alpha$ as the shape parameter and $\theta = \dfrac{1}{\beta}$ as the scale parameter. The PDF may be written as
\begin{equation}
f\left(x\right) = \dfrac{x^{k - 1}e^{-x/\theta}}{\theta^{k}\Gamma\left(k\right)}
\end{equation}

\subsection{Beta Distribution}

\subsubsection{Beta Function}

Also known as the Euler integral of the first kind, the Beta function is defined as
\begin{equation}
\operatorname{B}\left(z, y\right) = \int_{0}^{1}t^{z - 1}\left(1 - t\right)^{y - 1}dt
\end{equation}
for $\operatorname{Re}\left(y\right) > 0$ (the Beta function is defined on the complex numbers). The Beta function has a special relation with the Gamma function as follows:
\begin{equation}
\operatorname{B}\left(z, y\right) = \dfrac{\Gamma\left(z\right)\Gamma\left(y\right)}{\Gamma\left(z + y\right)}
\end{equation}
This can be shown by first writing out the definitions of the product of two Gamma functions:
\begin{align}
\Gamma\left(z\right)\Gamma\left(y\right) &= \left(\int_{0}^{\infty}u^{z - 1}e^{-u}du\right)\left(\int_{0}^{\infty}v^{y - 1}e^{-v}dv\right) \\
&= \int_{0}^{\infty}\int_{0}^{\infty}u^{z - 1}v^{y - 1}e^{-u - v}dudv
\end{align}
Introduce the change in variables $u = st$, $v = s\left(1 - t\right)$, which when rearranged becomes $t = \dfrac{u}{v + u}$ and $s = v + u$. Hence for all $u \in \left(0, \infty\right), v \in \left(0, \infty\right)$, we have $t \in\left(0, 1\right), s \in\left(0, \infty\right)$. Hence rewriting the integral gives
\begin{equation}
\Gamma\left(z\right)\Gamma\left(y\right)  = \int_{0}^{\infty}\int_{0}^{1}\left(st\right)^{z - 1}\left[s\left(1 - t\right)\right]^{y - 1}e^{-s}\det\left(\mathbf{J}\right)dtds
\end{equation}
where
\begin{equation}
\det\left(\mathbf{J}\right) = \det\left(\begin{bmatrix}\dfrac{\partial}{\partial t}st & \dfrac{\partial}{\partial s}st\\
\dfrac{\partial}{\partial t}s\left(1-t\right) & \dfrac{\partial}{\partial s}s\left(1-t\right)
\end{bmatrix}\right) = s\left(1 - t\right) + st = s
\end{equation}
So
\begin{align}
\Gamma\left(z\right)\Gamma\left(y\right) &= \int_{0}^{\infty}\int_{0}^{1}\left(st\right)^{z - 1}\left[s\left(1 - t\right)\right]^{y - 1}e^{-s}sdtds \\
&= \int_{0}^{\infty}\int_{0}^{1}\left(s^{z + y - 1}e^{-s}\right)t^{z - 1}\left(1 - t\right)^{y - 1}dtds \\
&= \left(\int_{0}^{\infty}\left(s^{z + y - 1}e^{-s}\right)ds\right)\left(\int_{0}^{1}t^{z - 1}\left(1 - t\right)^{y - 1}dt\right) \\
&= \Gamma\left(z + y\right)\operatorname{B}\left(z, y\right)
\end{align}
as required. From this property, we can also deduce the Beta function is symmetric, ie. $\operatorname{B}\left(z, y\right) = \operatorname{B}\left(y, z\right)$. The incomplete beta function is the Beta function with incomplete  limits, ie.
\begin{equation}
\operatorname{B}\left(x; z, y\right) = \int_{0}^{x}t^{z - 1}\left(1 - t\right)^{y - 1}dt
\end{equation}
The regularised incomplete Beta function is defined as $\dfrac{\operatorname{B}\left(x; z, y\right)}{\operatorname{B}\left(z, y\right)}$.

\subsubsection{Beta Distribution of the First Kind}

From the regularised incomplete Beta function we can see that it is a valid cumulative distribution function in $x$, defined on $\left[0, 1\right]$. Hence the Beta distribution (also known as the Beta distribution of the first kind) has cumulative distribution function
\begin{equation}
F\left(x\right) = \dfrac{\operatorname{B}\left(x; \alpha, \beta\right)}{\operatorname{B}\left(\alpha, \beta\right)}
\end{equation}
where $\alpha > 0$ and $\beta > 0$ are shape parameters. Differentiating the cumulative distribution using the Fundamental Theorem of Calculus yields the probability density function $f\left(x\right)$:
\begin{align}
f\left(x\right) &= \dfrac{dF\left(x\right)}{dx} \\
&= \dfrac{1}{\operatorname{B}\left(\alpha, \beta\right)}\dfrac{d\operatorname{B}\left(x; \alpha, \beta\right)}{dx} \\
&= \dfrac{1}{\operatorname{B}\left(\alpha, \beta\right)}\dfrac{d}{dx}\int_{0}^{x}t^{\alpha - 1}\left(1-t\right)^{\beta - 1}dt \\
&= \dfrac{x^{\alpha - 1}\left(1-x\right)^{\beta - 1}}{\operatorname{B}\left(\alpha, \beta\right)}
\end{align}
with an alternative form in terms of Gamma functions as
\begin{equation}
f\left(x\right) = \dfrac{\Gamma\left(\alpha + \beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}x^{\alpha - 1}\left(1-x\right)^{\beta - 1}
\end{equation}
If $\alpha$ and $\beta$ are positive integers, then the Beta distribution in terms of factorials is
\begin{equation}
f\left(x\right) = \dfrac{\left(\alpha + \beta - 1\right)!}{\left(\alpha - 1\right)!\left(\beta - 1\right)!}x^{\alpha - 1}\left(1-x\right)^{\beta - 1}
\end{equation}
By letting $n = \alpha + \beta + 2$ and $r = \alpha - 1$, an alternative parametrisation is
\begin{equation}
f\left(x\right) = \dfrac{\left(n + 1\right)!}{r!\left(n - r\right)!}x^{\alpha - 1}\left(1-x\right)^{\beta - 1}
\end{equation}

\subsection{Chi-Squared Distribution}

\subsection{$F$-Distribution}

\subsection{Student $t$'s Distribution}

\subsection{Lognormal Distribution}

\subsection{Pareto Distribution}

\subsection{Weibull Distribution}

\subsection{Gumbel Distribution}

\section{Families of Discrete Univariate Probability Distributions}

\subsection{Kronecker Delta Distribution}

\subsection{Discrete Uniform Distribution}

\subsection{Bernoulli Distribution}

\subsection{Radermacher Distribution}

\subsection{Binomial Distribution}

\subsection{Poisson Distribution}

\subsection{Skellam Distribution}

\subsection{Negative Binomial Distribution}

The negative binomial distribution is a discrete distribution on support $x \in \left\{0, 1, 2, 3, \dots\right\}$ for the number of successes of i.i.d. Bernoulli trials (with success probability $p$) before $r$ failures occur. For example, if we are interested in 1 success before $r$ failures, the probability of this is given by
\begin{equation}
P\left(X = 1\right) = \mathstrut^{r}C_{1}\left(1 - p\right)^{r}p^{1}
\end{equation}
We know that the sequence will be of length $n = r + 1$, and that the final trial in the sequence is fixed as a failure. Hence the term $\mathstrut^{r}C_{1} = \mathstrut^{n - 1}C_{1}$ gives the number of combinations where we can place $1$ success among $n - 1$ trials. Generally, the probability mass function is given by
\begin{equation}
P\left(X = x\right) = \mathstrut^{r + x - 1}C_{x}\left(1 - p\right)^{r}p^{x}
\end{equation}
The distribution has a mean of
\begin{equation}
\mu = \dfrac{pr}{1 - p}
\end{equation}
and a variance of
\begin{equation}
\sigma^{2} = \dfrac{pr}{\left(1 - p\right)^{2}}
\end{equation}
In practice, the negative binomial distribution may be used as an alternative to the Poisson distribution when the data appears too `overdispersed' for the Poisson distribution. Since the negative binomial has two parameters ($r$ and $p$), we can fit the distribution better to the data. The negative binomial can also be seen as a mixture of Poissons with Gamma-distributed mixing weights. Suppose we know $\mu$ and $\sigma^{2}$, then solving the equations above for $p$ and $r$ gives the equations
\begin{gather}
p = 1 - \dfrac{\mu}{\sigma^{2}} \\
r = \dfrac{\mu^{2}}{\sigma^{2} - \mu}
\end{gather}
which can be used as the moment estimators.

\subsection{Geometric Distribution}

\subsection{Hypergeometric Distribution}

\subsection{Beta-Binomial Distribution}

\subsection{Benford Distribution}

\section{Random Vectors}

\subsection{Covariance Matrices}
The covariance matrix $C$ of a random vector $\mathbf{x}$ can be defined by
\begin{equation}
C = \operatorname{E}\left[\left(\mathbf{x} - \bar{\mathbf{x}}\right)\left(\mathbf{x} - \bar{\mathbf{x}}\right)^{\top}\right]
\end{equation}
Covariance matrices of real random vectors are always positive semi-definite. We can show this by considering an arbitrary real vector $\mathbf{u}$. We have
\begin{align}
\mathbf{u}^{\top}C\mathbf{u} &= \mathbf{u}^{\top}\operatorname{E}\left[\left(\mathbf{x} - \bar{\mathbf{x}}\right)\left(\mathbf{x} - \bar{\mathbf{x}}\right)^{\top}\right]\mathbf{u} \\
&= \operatorname{E}\left[\mathbf{u}^{\top}\left(\mathbf{x} - \bar{\mathbf{x}}\right)\left(\mathbf{x} - \bar{\mathbf{x}}\right)^{\top}\mathbf{u}\right]
\end{align}
Let $s := \mathbf{u}^{\top}\left(\mathbf{x} - \bar{\mathbf{x}}\right)$ be a zero-mean scalar random variable with variance $\sigma^{2} \geq 0$. Hence
\begin{align}
\mathbf{u}^{\top}C\mathbf{u} &= \operatorname{E}\left[ss^{\top}\right] \\
&= \operatorname{E}\left[\left(s - \operatorname{E}\left[s\right]\right)^{2}\right] \\
&= \operatorname{E}\left[s^{2}\right] \\
&= \sigma^{2} \\
&\geq 0
\end{align}
Therefore $C$ satisfies the property of a positive semi-definite matrix.

\section{Multivariate Probability Distributions}

\subsection{Multivariate Gaussian Distribution}

\subsection{Multinomial Distribution}

\subsection{Multivariate Hypergeometric Distribution}

\subsection{Dirichlet Distribution}

\subsection{Bivariate Cumulative Discrete Distribution Functions}

Suppose we have the discrete bivariate cumulative distribution function $F\left(x, y\right) := P\left(X\leq x, Y \leq y\right)$. We are interested in deriving the probability mass function $f\left(x, y\right) = P\left(X = x, Y = y\right)$ from this. Assume that $X$ and $Y$ are integer valued. We can derive this using the inclusion-exclusion principle. First define the events
\begin{gather}
A: \left\{X \leq x, Y < y\right\} \\
B: \left\{X < x, Y \leq y\right\}
\end{gather}
Then logically
\begin{gather}
A \cup B: \left\{X \leq x, Y \leq y\right\}\setminus\left\{X = x, Y = y\right\} \\
A \cap B: \left\{X < x, Y < y\right\}
\end{gather}
Thus the probability mass function can be expressed as
\begin{align}
P\left(X = x, Y = y\right) &= P\left(X \leq x, Y \leq y\right) - P\left(A \cup B\right) \\
&= P\left(X \leq x, Y \leq y\right) - P\left(A\right) - P\left(B\right) + P\left(A \cap B\right) \\
&= P\left(X \leq x, Y \leq y\right) - P\left(X \leq x, Y < y\right) - P\left(X < x, Y \leq y\right) + P\left(X < x, Y < y\right) \\
&= \begin{multlined} P\left(X \leq x, Y \leq y\right) - P\left(X \leq x, Y \leq y - 1\right) - P\left(X \leq x - 1, Y \leq y\right) \\
+ P\left(X \leq x - 1, Y \leq y - 1\right) \end{multlined} \\
&= F\left(x, y\right) - F\left(x, y - 1\right) - F\left(x - 1, y\right) + F\left(x - 1, y - 1\right)
\end{align}

\section{Transformations of Random Variables}

\subsection{Sums of Random Variables}

Consider the sum of two continuous random variables $W = X + Y$. The PDF of $W$ can be defined as
\begin{equation}
f_{W}\left(w\right) = \int\int_{\left\{x,y: x + y = w\right\}}f_{XY}\left(x, y\right)dxdy
\end{equation}
We can then just integrate along the straight line $x + y = w$ and make a substitution of either $x = w - y$ or $y = w - x$ (which influences the integrating variable). This gives rise to two alternative formulas.
\begin{gather}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{XY}\left(x, w-x\right)dx \\
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{XY}\left(w - y, y\right)dy
\end{gather}
If $X$ and $Y$ are independent, then
\begin{gather}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(w-x\right)dx \\
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(w - y\right)f_{Y}\left(y\right)dy
\end{gather}
Note that that $f_{W}\left(\cdot\right)$ is a convolution of $f_{X}\left(\cdot\right)$ and $f_{Y}\left(\cdot\right)$. \\

For the case of the sum of two discrete random variables $W = X + Y$, the analogous formulae are
\begin{gather}
\operatorname{Pr}\left(W = w\right) = \sum_{x = -\infty}^{\infty}\operatorname{Pr}\left(X = x, Y = w - x\right) \\
\operatorname{Pr}\left(W = w\right) = \sum_{y = -\infty}^{\infty}\operatorname{Pr}\left(X = w - y, Y = y\right)
\end{gather}
and in the independent case
\begin{gather}
\operatorname{Pr}\left(W = w\right) = \sum_{x = -\infty}^{\infty}\operatorname{Pr}\left(X = x\right)\operatorname{Pr}\left(Y = w - x\right) \\
\operatorname{Pr}\left(W = w\right) = \sum_{y = -\infty}^{\infty}\operatorname{Pr}\left(X = w - y\right)\operatorname{Pr}\left(Y = y\right)
\end{gather}

Consider the difference of two continuous random variables $W = X - Y$. Analogously (by making substitutions $x = w + y$ or $y = -w + x$) we can show
\begin{gather}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{XY}\left(x, -w + x\right)dx \\
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{XY}\left(w + y, y\right)dy
\end{gather}
and in the independent case
\begin{gather}
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(-w + x\right)dx \\
f_{W}\left(w\right) = \int_{-\infty}^{\infty}f_{X}\left(w + y\right)f_{Y}\left(y\right)dy
\end{gather}
\begin{theorem}
If $X$ and $Y$ are independent and identically distributed continuous random variables, than the distribution of their difference $W = X - Y$ is symmetric.
\end{theorem}
\begin{proof}
Note that $W$ will necessarily have a mean of zero (assuming $X$ and $Y$ have finite mean). It suffices to show $f_{W}\left(w\right) = f_{W}\left(-w\right)$. Using the formulas above in the independent case,
\begin{equation}
f_{W}\left(-w\right) = \int_{-\infty}^{\infty}f_{X}\left(x\right)f_{Y}\left(w + x\right)dx
\end{equation}
Since $X$ and $Y$ are identical, $f_{X}\left(x\right) = f_{Y}\left(x\right)$ so
\begin{equation}
f_{W}\left(-w\right) = \int_{-\infty}^{\infty}f_{Y}\left(x\right)f_{X}\left(w + x\right)dx
\end{equation}
`Renaming' the integrating variable to $y$ yields
\begin{equation}
f_{W}\left(-w\right) = \int_{-\infty}^{\infty}f_{Y}\left(y\right)f_{X}\left(w + y\right)dy = f_{W}\left(w\right)
\end{equation}
\end{proof}

\subsection{Strictly Monotonic Transformations}

Let $X$ be a random variable with probability density $f_{X}\left(x\right)$, and let $A$ be the set $A = \left\{x \in\mathbb{R}\middle| f_{X}\left(x\right) > 0\right\}$. Suppose that $Y$ is a transformation of $X$ given by $Y = g\left(X\right)$ such that $g$ is strictly monotonic (either increasing or decreasing) over $A$. We derive the probability density of $Y$, denoted $f_{Y}\left(y\right)$, in terms of $f_{X}\left(x\right)$ and $g$. \\

First assume $g$ is monotonically increasing, so $g^{-1}$ will also be monotonically increasing. The cumulative density function of $Y$ is given by
\begin{align}
F_{Y}\left(y\right) &= P\left(Y \leq y\right) \\
&= P\left(g\left(X\right) \leq y\right) \\
&= P\left(X \leq g^{-1}\left(y\right)\right) \\
&= F_{X}\left(g^{-1}\left(y\right)\right)
\end{align}
Differentiating $F_{Y}$ gives the density of $Y$:
\begin{align}
f_{Y}\left(y\right) &= \dfrac{d}{dy}F_{Y}\left(y\right)\\
&= \dfrac{d}{dy}F_{X}\left(g^{-1}\left(y\right)\right)
\end{align}
Using the chain rule,
\begin{align}
f_{Y}\left(y\right) &= \dfrac{d}{dx}F_{X}\left(g^{-1}\left(y\right)\right)\cdot\dfrac{d}{dy}g^{-1}\left(y\right) \\
&= f_{X}\left(g^{-1}\left(y\right)\right)\cdot\dfrac{d}{dy}g^{-1}\left(y\right)
\end{align}
Now assume $g$ is monotonically decreasing, so $g^{-1}$ is also monotonically decreasing and has the ability to flip an inequality sign. Similar to before, we can write
\begin{align}
F_{Y}\left(y\right) &= P\left(Y \leq y\right) \\
&= P\left(g\left(X\right) \leq y\right) \\
&= P\left(X \geq g^{-1}\left(y\right)\right) \\
&= 1 - F_{X}\left(g^{-1}\left(y\right)\right)
\end{align}
Now differentiating $F_{Y}$ gives 
\begin{align}
f_{Y}\left(y\right) &= \dfrac{d}{dy}F_{Y}\left(y\right) \\
&= -\dfrac{d}{dy}F_{X}\left(g^{-1}\left(y\right)\right) \\
&= -\dfrac{d}{dx}F_{X}\left(g^{-1}\left(y\right)\right)\cdot\dfrac{d}{dy}g^{-1}\left(y\right) \\
&= -f_{X}\left(g^{-1}\left(y\right)\right)\cdot\dfrac{d}{dy}g^{-1}\left(y\right)
\end{align}
Since by monotonically decreasing $g^{-1}$, we have $\dfrac{d}{dy}g^{-1}\left(y\right) < 0$ hence this density is positive. Combining the two cases, we get
\begin{equation}
f_{Y}\left(y\right) =  f_{X}\left(g^{-1}\left(y\right)\right)\cdot\left|\dfrac{d}{dy}g^{-1}\left(y\right)\right|
\end{equation}

\subsection{Inverse Transform Sampling}

\subsection{Gauss' Approximation Theorem \cite{Blom1989}}

For a differentiable function $g\left(X\right)$ of a single random variable $X$, we can make the approximations
\begin{gather}
\operatorname{E}\left[g\left(X\right)\right] \approx g\left(\operatorname{E}\left[X\right]\right) \\
\operatorname{Var}\left(g\left(X\right)\right) \approx \operatorname{Var}\left(X\right)g'\left(\operatorname{E}\left[X\right]\right)^{2}
\end{gather}
This is achieved using a first-order Taylor series expansion of $g\left(X\right)$ about $m := \operatorname{E}\left[X\right]$ as follows
\begin{equation}
g\left(X\right) \approx g\left(m\right) + \left(X - m\right)g'\left(m\right)
\end{equation}
Taking the expectations of both sides gives
\begin{align}
\operatorname{E}\left[g\left(X\right)\right] &\approx \operatorname{E}\left[g\left(m\right)\right] + \operatorname{E}\left[\left(X - m\right)g'\left(m\right)\right] \\
&\approx \operatorname{E}\left[g\left(\operatorname{E}\left[X\right]\right)\right] + \operatorname{E}\left[\left(X - \operatorname{E}\left[X\right]\right)\right]g'\left(\operatorname{E}\left[X\right]\right) \\
&\approx g\left(\operatorname{E}\left[X\right]\right) + \left(\operatorname{E}\left[X\right] - \operatorname{E}\left[X\right]\right)g'\left(\operatorname{E}\left[X\right]\right) \\
&\approx g\left(\operatorname{E}\left[X\right]\right)
\end{align}
Taking the variance of both sides of the expansion gives
\begin{align}
\operatorname{Var}\left(g\left(X\right)\right) &\approx \operatorname{Var}\left(g\left(m\right) + \left(X - m\right)g'\left(m\right)\right) \\
&\approx \operatorname{Var}\left(\left(X - m\right)g'\left(m\right)\right) \\
&\approx \operatorname{Var}\left(X - m\right)g'\left(m\right)^{2} \\
&\approx \operatorname{Var}\left(X\right)g'\left(m\right)^{2} \\
&\approx \operatorname{Var}\left(X\right)g'\left(\operatorname{E}\left[X\right]\right)^{2} 
\end{align}

\subsection{Independence of Transformed Random Variables}

\section{Inequalities and Limit Theorems}

\subsection{Jensen's Inequality}

A function $f\left(x\right)$ is convex if and only Jensen's inequality holds for any $x$ and $y$ in the domain of $f$:
\begin{equation}
f\left(\theta x + \left(1 - \theta\right)y\right) \leq \theta f\left(x\right) + \left(1 - \theta\right)f\left(y\right)
\end{equation}
where $\theta \in \left[0, 1\right]$ and $\theta x + \left(1 - \theta\right)y$ is termed as a `convex combination' of $x$ and $y$. Intuitively speaking, no part of a secant joining any two points on the graph of $f\left(x\right)$ can lie below the graph. A convex combination can be extended to more than two points, and so can Jensen's inequality:
\begin{equation}
f\left(\theta_{1}x_{1} + \dots + \theta_{k}x_{k}\right) \leq \theta_{1}f\left(x_{1}\right) + \dots + \theta_{k}f\left(x_{k}\right)
\end{equation}
where $\theta_{1}, \dots, \theta_{k} \geq 0$ and $\theta_{1} + \dots + \theta_{k} = 1$. We may extend this to infinite sums, ie. integrals over probability distributions:
\begin{equation}
f\left(\int p\left(x\right) x dx\right) \leq \int f\left(x\right)p\left(x\right) dx
\end{equation}
where $p\left(x\right) \geq 0$ and $\int p\left(x\right) dx = 1$. Recognise that the above inequality pertains to the expected value of a random variable $X$ with probability density $p\left(x\right)$, and a convex function $f\left(x\right)$. We can write
\begin{equation}
f\left(\mathbb{E}\left[X\right]\right) \leq \mathbb{E}\left[f\left(X\right)\right]
\end{equation}

\subsection{Markov's Inequality}

Let $X$ be a non-negative random variable (it cannot take on negative values), and let $a > 0$ be some constant. Then
\begin{equation}
\operatorname{P}\left(X\geq a\right) \leq \dfrac{\operatorname{E}\left[X\right]}{a}
\end{equation}
or alternatively, if we let $\tilde{a}$ be some constant such that $a = \tilde{a}\operatorname{E}\left[X\right]$, then making this substitution gives
\begin{equation}
\operatorname{P}\left(X\geq \tilde{a}\operatorname{E}\left[X\right]\right) \leq \dfrac{1}{\tilde{a}}
\end{equation}
\begin{proof}
For a continuous non-negative random variable, the expectation is defined as
\begin{align}
\operatorname{E}\left[X\right] &= \int_{0}^{\infty}xf\left(x\right)dx \\
&= \int_{0}^{a}xf\left(x\right)dx + \int_{a}^{\infty}xf\left(x\right)dx \\
&\geq \int_{a}^{\infty}xf\left(x\right)dx
\end{align}
We then have
\begin{align}
\int_{a}^{\infty}af\left(x\right)dx &\leq \int_{a}^{\infty}xf\left(x\right)dx \\
a\int_{a}^{\infty}f\left(x\right)dx &\leq \operatorname{E}\left[X\right] \\
a\operatorname{P}\left(X \geq a\right) &\leq \operatorname{E}\left[X\right] \\
\operatorname{P}\left(X \geq a\right) &\leq \dfrac{\operatorname{E}\left[X\right]}{a}
\end{align}
If $X$ is a non-negative discrete random variable on support $\left\{0, 1, 2, \dots\right\}$, then
\begin{align}
\operatorname{E}\left[X\right] &= \sum_{x = 0}^{\infty}x\operatorname{P}\left(X = x\right) \\
&= \sum_{x = 0}^{a - 1}x\operatorname{P}\left(X = x\right) + \sum_{x = a}^{\infty}x\operatorname{P}\left(X = x\right) \\
&\geq \sum_{x = a}^{\infty}x\operatorname{P}\left(X = x\right)
\end{align}
Then similar to before,
\begin{align}
\sum_{x = a}^{\infty}a\operatorname{P}\left(X = x\right) &\leq \sum_{x = a}^{\infty}x\operatorname{P}\left(X = x\right) \\
a\sum_{x = a}^{\infty}\operatorname{P}\left(X = x\right) &\leq \operatorname{E}\left[X\right] \\
a\operatorname{P}\left(X \geq a\right) &\leq \operatorname{E}\left[X\right] \\
\operatorname{P}\left(X \geq a\right) &\leq \dfrac{\operatorname{E}\left[X\right]}{a}
\end{align}
\end{proof}
An alternative proof is given below:
\begin{proof}
Let $I_{X \geq a}$ be an indicator random variable where $I_{X \geq a} = 1$ if the event $X \geq a$ occurs, and $I_{X \geq a}$ if the event $X < a$ occurs. Then we can write
\begin{equation}
aI_{X \geq a} \leq X
\end{equation}
This can be seen as follows:
\begin{itemize}
\item Suppose $X \geq a$, then $I_{X \geq a} = 1$ and the inequality above holds.
\item Suppose $X < a$, then $I_{X \geq a} = 0$ and the inequality above is satisfied because $X$ is non-negative.
\end{itemize}
Take the expectation operator of both sides. The expectation operator is monotonically increasing (ie. $\operatorname{E}\left[c\right] = c$ for some constant $c$) so this preserves the inequality.
\begin{equation}
\operatorname{E}\left[aI_{X \geq a}\right] \leq \operatorname{E}\left[X\right]
\end{equation}
Taking the constant $a$ out of the expectation:
\begin{gather}
a\operatorname{E}\left[I_{X \geq a}\right] \leq \operatorname{E}\left[X\right] \\
\operatorname{E}\left[I_{X \geq a}\right] \leq \dfrac{\operatorname{E}\left[X\right]}{a}
\end{gather}
We can evaluate $\operatorname{E}\left[I_{X \geq a}\right]$
\begin{equation}
\operatorname{E}\left[I_{X \geq a}\right] = 1\times\operatorname{P}\left(X\geq a\right) + 0\times\operatorname{P}\left(X < a\right) = \operatorname{P}\left(X\geq a\right)
\end{equation}
Hence
\begin{equation}
\operatorname{P}\left(X\geq a\right) \leq \dfrac{\operatorname{E}\left[X\right]}{a}
\end{equation}

\end{proof}
We can use this result to bound probabilities of $X$ being greater than some value in relation to the mean. For example, suppose the population of $X$ is income (which we assume to be non-negative). Then if we choose $a$ to be 10 times the average income, we can make the statement that the proportion of people earning at least 10 times the average income is no more than 10\%.

\subsection{Chebychev's Inequality}

Let $X$ be a random variable with finite mean $\mu$ and non-zero standard deviation $\sigma$. Then for any $k > 0$
\begin{equation}
\operatorname{P}\left(\left|X - \mu\right| \geq k\sigma\right) \leq \dfrac{1}{k^{2}}
\end{equation}
In words, this states that the probability of $X$ being at least $k$ standard deviations away from the mean can be no more than $\dfrac{1}{k^{2}}$.
\begin{proof}
Let a random variable $Y$ be $Y = \left(X - \mu\right)^{2}$ and let $a = \left(k\sigma\right)^{2}$. Then applying Markov's inequality
\begin{gather}
\operatorname{P}\left(Y \geq a\right) \leq \dfrac{\operatorname{E}\left[Y\right]}{a} \\
\operatorname{P}\left(\left(X - \mu\right)^{2} \geq k^{2}\sigma^{2}\right) \leq \dfrac{\operatorname{E}\left[\left(X - \mu\right)^{2}\right]}{k^{2}\sigma^{2}} \\
\end{gather}
The event inside the probability is equivalent to $\left|X - \mu\right| \geq k\sigma$ and the numerator on the right hand side is the definition of variance $\sigma^{2}$. So
\begin{equation}
\operatorname{P}\left(\left|X - \mu\right| \geq k\sigma\right) \leq \dfrac{1}{k^{2}}
\end{equation}
\end{proof}
Note that this inequality is only useful for when $k > 1$, because when $0 < k \leq 1$, the term $\dfrac{1}{k^{2}} \geq 1$ so Chebychev's inequality is trivially satisfied. An alternative form of the inequality is to let $k = \varepsilon/\sigma$. Then we have
\begin{equation}
\operatorname{P}\left(\left|X - \mu\right| \geq \varepsilon\right) \leq \dfrac{\sigma^{2}}{\varepsilon^{2}}
\end{equation}
Chebychev's inequality can be used to bound probabilities within a certain number of standard deviations from the mean, for very general distributions. Obtaining more knowledge of the distribution (such as information the distribution is normal) will often lead to tighter (ie. smaller) bounds.

\subsection{Weak Law of Large Numbers}

Let $X_{1}, \dots, X_{n}$ be a sequence of independent and identically distributed random variables, with mean $\operatorname{E}\left[X_{i}\right] = \mu$ and standard deviation $\sigma$. Then the sample mean $\overline{X}$ is defined as
\begin{equation}
\overline{X} = \dfrac{X_{1} + \dots + X_{n}}{n}
\end{equation}
The Weak Law of Large Numbers states that $\overline{X} \underset{prob}{\rightarrow} \mu$ as $n \rightarrow \infty$. That is, the sample mean converges in probability to the population mean as the sample size increases to infinity. We can also write this as
\begin{equation}
\lim_{n\rightarrow\infty}\operatorname{P}\left(\left|\overline{X} - \mu\right|\geq \varepsilon \right) = 0
\end{equation}
for all $\varepsilon > 0$.
\begin{proof}
From Chebychev's inequality
\begin{equation}
\operatorname{P}\left(\left|\overline{X} - \mu\right| \geq \varepsilon\right) \leq \dfrac{\operatorname{Var}\left(\overline{X}\right)}{\varepsilon^{2}}
\end{equation}
Using $\operatorname{Var}\left(\overline{X}\right) = \sigma^{2}/n$
\begin{equation}
\operatorname{P}\left(\left|\overline{X} - \mu\right| \geq \varepsilon\right) \leq \dfrac{\sigma^{2}}{n\varepsilon^{2}}
\end{equation}
Taking the limit
\begin{equation}
\lim_{n\rightarrow\infty}\operatorname{P}\left(\left|\overline{X} - \mu\right|\geq \varepsilon \right) = 0
\end{equation}
\end{proof}

\subsection{Strong Law of Large Numbers}

The Strong Law of Large Numbers is a stronger version of the Weak Law of Large Numbers which says that the sample mean converges almost surely to the population mean. That is,
\begin{equation}
\operatorname{P}\left(\lim_{n\rightarrow\infty}\overline{X} = \mu\right) = 1
\end{equation}

\section{Notions of Convergence}

\subsection{Convergence in Distribution}

Let $\left\{X_{1}, X_{2}, \dots, X_{n} \right\}$ be a sequence of real-valued random variables. The sequence of random variables is said to converge in distribution to a random variable $X$ if
\begin{equation}
\lim_{n\to \infty}F_{n}\left(x\right) = F\left(x\right)
\end{equation}
where $F_{n}\left(x\right)$ is the cumulative distribution function of $X_{n}$ and $F\left(x\right)$ is the cumulative distribution function of $X$. \\

Let $\mathbf{X}_{1}, \mathbf{X}_{2}, \dots$ be real valued random vectors and let $\left\{\mathbf{X}_{1}, \mathbf{X}_{2}, \dots, \mathbf{X}_{n} \right\}$ be a sequence of random vectors. The sequence is said to converge in distribution to a random vector $\mathbf{X}$ if
\begin{equation}
\lim_{n\to\infty}\operatorname{Pr}\left(\mathbf{X}_{n}\in A\right) = \operatorname{Pr}\left(\mathbf{X}\in A\right)
\end{equation}
for every $A\subset \mathbb{R}^{n}$ which is a continuity set of $\mathbf{X}$. Note that it is stronger condition to say it converges in distribution if the cumulative distribution functions converge. If 
\begin{equation}
\lim_{n\to \infty}F_{n}\left(x\right) = F\left(x\right)
\end{equation}
then this implies convergence in distribution.

\subsection{Convergence in Mean}

A sequence of random variables $\left\{X_{1}, X_{2}, \dots, X_{n} \right\}$ converges in mean towards random variable $X$ if
\begin{equation}
\lim_{n\to\infty}\mathbb{E}\left[\left|X_{n} - X\right|\right] = 0
\end{equation}
A sequence of random vectors $\left\{\mathbf{X}_{1}, \mathbf{X}_{2}, \dots, \mathbf{X}_{n} \right\}$ converges in mean towards random vector $\mathbf{X}$ if
\begin{equation}
\lim_{n\to\infty}\mathbb{E}\left[\left\Vert\mathbf{X}_{n} - \mathbf{X}\right\Vert\right] = 0
\end{equation}

\subsection{Convergence in Probability}

A sequence of random variables $\left\{X_{1}, X_{2}, \dots, X_{n} \right\}$ converges in probability towards random variable $X$ if
\begin{equation}
\lim_{n\to \infty}\operatorname{Pr}\left(\left|X_{n} - X\right| > \varepsilon\right) = 0
\end{equation} 
for all $\varepsilon > 0$. If $X$ is a constant, then the sequence is said to converge in probability to a constant.\\

A sequence of random vectors $\left\{\mathbf{X}_{1}, \mathbf{X}_{2}, \dots, \mathbf{X}_{n} \right\}$ converges in probability towards random vector $\mathbf{X}$ if
\begin{equation}
\lim_{n\to \infty}\operatorname{Pr}\left(\left\Vert \mathbf{X}_{n} - \mathbf{X}\right\Vert > \varepsilon\right) = 0
\end{equation} 
for all $\varepsilon > 0$.

\subsection{Pointwise Convergence in Probability}

Pointwise convergence in probability extends the notion of convergence in probability to sequences which are parametrised by some parameter $\theta$ from a parameter space $\Theta$. For example, the sequence $\left\{X_{n}\right\}$ can be thought of as estimators and $\theta$ is a parameter of the data generating process. A formal definition is given as follows. \\
 
Denote by $X$ a random variable obtained from an experiment parametrised by $\theta\in\Theta$, and denote by $X_{\theta}$ the random variable obtained by fixing $\theta$ during the experiment. Denote by $\left\{X_{1, \theta}, X_{2, \theta}, \dots, X_{n, \theta}\right\}$ a sequence of random variables obtained by fixing $\theta$ during the experiment. Then the sequence is said to be pointwise convergent in probability to $X$ if for each $\theta\in\Theta$
\begin{equation}
\lim_{n\to \infty}\operatorname{Pr}\left(\left|X_{n, \theta} - X_{\theta}\right| > \varepsilon\right) = 0
\end{equation}
for all $\varepsilon > 0$. That is, convergence in probability holds for any value of $\theta\in\Theta$. An alternative equivalent definition is that given any $\varepsilon > 0$, $\delta > 0$ and $\theta\in\Theta$, one can find an integer $n_{0}$ (possibly dependent on $\varepsilon, \delta, \theta$) such that
\begin{equation}
\operatorname{Pr}\left(\left|X_{n, \theta} - X_{\theta}\right| > \varepsilon\right) < \delta
\end{equation}
if $n > n_{0}$. Pointwise convergence in probability can be analogously defined for sequences of random vectors (using $\left\Vert\cdot\right\Vert$ rather than $\left|\cdot\right|$).

\subsection{Uniform Convergence in Probability}

Uniform convergence in probability is a stronger condition of pointwise convergence in probability. Denote by $X$ a random variable obtained from an experiment parametrised by $\theta\in\Theta$, and denote by $X_{\theta}$ the random variable obtained by fixing $\theta$ during the experiment. Denote by $\left\{X_{1, \theta}, X_{2, \theta}, \dots, X_{n, \theta}\right\}$ a sequence of random variables obtained by fixing $\theta$ during the experiment. Then the sequence is said to be uniformly convergent in probability to $X$ if
\begin{equation}
\lim_{n\to \infty}\operatorname{Pr}\left(\sup_{\theta\in\Theta}\left|X_{n, \theta} - X_{\theta}\right| > \varepsilon\right) = 0
\end{equation}
That is, $\sup_{\theta\in\Theta}\left|X_{n, \theta} - X_{\theta}\right|$ converges in probability to zero. By examining an alternative equivalent definition, we have that given any $\varepsilon > 0$, $\delta > 0$, one can find an integer $n_{0}$ (possibly dependent on $\varepsilon, \delta$) such that 
\begin{equation}
\operatorname{Pr}\left(\left|X_{n, \theta} - X_{\theta}\right| > \varepsilon\right) < \delta
\end{equation}
for all $\theta\in\Theta$ if $n > n_{0}$. The difference from pointwise convergence in probability is that one must now relax dependence on $\theta$. So we need to find a $n_{0}$ for a given $\varepsilon, \delta$ that works for all $\theta\in\Theta$. Uniform convergence in probability can be analogously defined for sequences of random vectors (using $\left\Vert\cdot\right\Vert$ rather than $\left|\cdot\right|$).

\subsection{Almost Sure Convergence}

A sequence of random variables $\left\{X_{1}, X_{2}, \dots, X_{n} \right\}$ converges almost surely towards random variable $X$ if
\begin{equation}
\operatorname{Pr}\left(\lim_{n\to\infty}X_{n} = X\right) = 1
\end{equation}
A sequence of random vectors $\left\{\mathbf{X}_{1}, \mathbf{X}_{2}, \dots, \mathbf{X}_{n} \right\}$ converges almost surely towards random vector $\mathbf{X}$ if
\begin{equation}
\operatorname{Pr}\left(\lim_{n\to\infty}\mathbf{X}_{n} = \mathbf{X}\right) = 1
\end{equation}
where the equality is evaluated element-wise. \\

Almost sure convergence implies convergence in probability, however convergence in probability does not imply almost sure convergence. For example, consider the sequence where $X_{n}$ takes the value $1$ with probability $\dfrac{1}{n}$, and takes the value of $0$ otherwise. Although this sequence converges in probability to $0$, since the series $\sum_{n = 1}^{\infty}\operatorname{Pr}\left(X_{n} = 1\right) = \sum_{n = 1}^{\infty}\dfrac{1}{n}$ diverges, and each $X_{n}$ is independent, we have by the converse Borel-Cantelli lemma that there is probability 1 that the event $X_{n} = 1$ occurs infinitely many times. Therefore the sequence does not almost surely converge to $0$.

\subsection{Complete Convergence}

A sequence of random variables $\left\{X_{1}, X_{2}, \dots \right\}$ is said to be completely convergent to $X$ if
\begin{equation}
\sum_{n = 1}^{\infty}\operatorname{Pr}\left(\left|X_{n} - X\right| > \varepsilon\right) < \infty
\end{equation}
for all $\varepsilon > 0$. If $X_{n}$ are independent, then complete convergence is equivalent to almost sure convergence.
\begin{proof}
To show that complete convergence implies almost sure convergence, let event $E_{n}$ be $\left\{\left|X_{n} - X\right| > \varepsilon\right\}$ for some $\varepsilon > 0$. Then since $\sum_{n = 1}^{\infty}\operatorname{Pr}\left(\left|X_{n} - X\right| > \varepsilon\right) < \infty$ by complete convergence, the Borel-Cantelli lemma tells us that
\begin{equation}
\operatorname{Pr}\left(\limsup_{n\to\infty}\left\{\left|X_{n} - X\right| > \varepsilon\right\}\right) = 0
\end{equation}
Since $\varepsilon$ can be arbitrarily small, then
\begin{align}
\operatorname{Pr}\left(\limsup_{n\to\infty}\left\{\left|X_{n} - X\right| = 0\right\}\right) &= 1 \\
\operatorname{Pr}\left(\lim_{n\to\infty}X_{n} = X\right) &= 1
\end{align}
To show that almost sure convergence implies complete convergence, first suppose we have almost sure convergence but not complete converge. This means there exists an $\varepsilon > 0$ such that
\begin{equation}
\sum_{n = 1}^{\infty}\operatorname{Pr}\left(\left|X_{n} - X\right| > \varepsilon\right) = \infty
\end{equation}
Given that the sequence $\left\{X_{n}\right\}$ is independent, we can verify that $\left\{X_{n} - X\right\}$ is also independent as follows. Since $X_{n} \to X$, then $X$ is almost sure constant. Hence $\left\{X_{n} - X\right\}$ is independent. By the converse Borel-Cantelli lemma, this gives for that particular $\varepsilon > 0$:
\begin{equation}
\operatorname{Pr}\left(\limsup_{n\to\infty}\left\{\left|X_{n} - X\right| > \varepsilon\right\}\right) = 1
\end{equation}
This contradicts almost sure convergence, hence almost sure convergence implies complete convergence.
\end{proof}

\subsection{With High Probability}

An event $E$ occurs with high probability if the probability $\operatorname{Pr}\left(E\right)$ depends on some number $n$, and $\operatorname{Pr}\left(E\right) \to 1$ as $n \to \infty$. That is, we can make the probability as close as desired to $1$ by making $n$ big enough.

\subsection{Slutsky's Theorem}

\subsection{Continuous Mapping Theorem}

\chapter{Intermediate Statistics}

\section{Maximum Likelihood Hypothesis Testing}

\subsection{Neyman-Pearson Lemma}

\subsection{Receiver Operating Characteristic}

\section{Maximum Likelihood Estimation}

\subsection{Fisher Information}

Let $f\left(x;\theta\right)$ be a probability density function for the random variable $X$ parametrised by $\theta$. Note that $f\left(X; \theta\right)$ is a random variable for the density of $X$, and that $f\left(x;\theta\right)$ is also a likelihood function for $\theta$. The partial derivative of the log likelihood with respect to $\theta$ is called the score $\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)$. It can be shown that the expected value of the score with respect to $X$ is zero:
\begin{align}
\mathbb{E}_{X}\left[\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right] & \begin{multlined}= \mathbb{E}_{X}\left[\dfrac{1}{f\left(X;\theta\right)}\dfrac{\partial}{\partial \theta}f\left(X;\theta\right)\right] \\
\text{\blue by the chain rule\black}
\end{multlined} \\
&= \int_{-\infty}^{\infty}\dfrac{1}{f\left(x;\theta\right)}\cdot\dfrac{\partial}{\partial \theta}f\left(x;\theta\right)\cdot f\left(x;\theta\right) dx \\
&= \int_{-\infty}^{\infty}\dfrac{\partial}{\partial \theta}f\left(x;\theta\right) dx \\
&= \dfrac{\partial}{\partial \theta}\left(\int_{-\infty}^{\infty}f\left(x;\theta\right) dx\right) \\
&= \dfrac{\partial}{\partial \theta}\left(1\right) \\
&= 0
\end{align}
The Fisher information $\mathcal{I}\left(\theta\right)$ is defined as the variance of the score
\begin{align}
\mathcal{I}\left(\theta\right) &= \operatorname{Var}\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right) \\
&= \mathbb{E}_{X}\left[\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2}\right] - \green\cancelto{0}{\black\mathbb{E}_{X}\left[\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right]^{2}} \\
&= \mathbb{E}_{X}\left[\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2}\right]
\end{align}
We can show that
\begin{align}
\dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right) & \begin{multlined}=\dfrac{\partial}{\partial \theta}\left(\dfrac{1}{f\left(X;\theta\right)}\cdot\dfrac{\partial}{\partial \theta}f\left(X;\theta\right)\right) \\
\text{\blue by the chain rule \black} \end{multlined} \\
& \begin{multlined}= -\dfrac{\dfrac{\partial}{\partial \theta}f\left(X;\theta\right)}{f\left(X;\theta\right)^{2}}\cdot\dfrac{\partial}{\partial \theta}f\left(X;\theta\right) + \dfrac{1}{f\left(X;\theta\right)}\cdot \dfrac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right) \\
\text{\blue by the product rule \black} \end{multlined} \\
&= -\left(\dfrac{\frac{\partial}{\partial \theta}f\left(X;\theta\right)}{f\left(X;\theta\right)}\right)^{2} + \dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)} \\
& \begin{multlined}= \dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)} - \left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2} \\
\text{\blue using }\blue\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right) = \dfrac{\frac{\partial}{\partial \theta}f\left(X;\theta\right)}{f\left(X;\theta\right)}\black
\end{multlined}
\end{align}
Hence
\begin{equation}
\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2} = \dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)} - \dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right)
\end{equation}
Additionally, we can show
\begin{align}
\mathbb{E}_{X}\left[\dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)}\right] &= \int_{-\infty}^{\infty}\dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(x;\theta\right)}{f\left(x;\theta\right)}\cdot f\left(x; \theta\right) dx \\
&= \int_{-\infty}^{\infty}\dfrac{\partial^{2}}{\partial \theta^{2}}f\left(x;\theta\right) dx \\
&= \dfrac{\partial^{2}}{\partial \theta^{2}}\left(\int_{-\infty}^{\infty}f\left(x;\theta\right) dx\right) \\
&= \dfrac{\partial^{2}}{\partial \theta^{2}}\left(1\right) \\
&= 0
\end{align}
Returning to the Fisher information
\begin{align}
\mathcal{I}\left(\theta\right) &= \mathbb{E}_{X}\left[\left(\dfrac{\partial}{\partial \theta}\log f\left(X;\theta\right)\right)^{2}\right] \\
&= \mathbb{E}_{X}\left[\dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)} - \dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right)\right] \\
&= \green\cancelto{0}{\black \mathbb{E}_{X}\left[\dfrac{\frac{\partial^{2}}{\partial \theta^{2}}f\left(X;\theta\right)}{f\left(X;\theta\right)}\right]}\black - \mathbb{E}_{X}\left[\dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right)\right] \\
&= - \mathbb{E}_{X}\left[\dfrac{\partial^{2}}{\partial \theta^{2}}\log f\left(X;\theta\right)\right]
\end{align}
This shows that the Fisher information is equal to the negative of the curvature of the log likelihood, assuming the log likelihood is twice differentiable with respect to $\theta$.

\section{James-Stein Estimators}

\section{Generalised Linear Models}

\subsection{Poisson Regression}

\subsection{Logistic Regression}

When the response variable $Y$ can take on a $0$ or $1$ (ie. success of fail), the probability that the response equals $1$ can be modelled using logistic regression. For a parameter vector $\beta$ and predictors $x$, this probability takes the form
\begin{equation}
P\left(Y = 1\middle| X = x\right) = \dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}
\end{equation}
An alternative form is
\begin{equation}
P\left(Y = 1\middle| X = x\right) = \dfrac{\exp\left(\beta^{\top}x\right)}{\exp\left(\beta^{\top}x\right) + 1}
\end{equation}
Notice that this probability will be bounded between $0$ and $1$. Denoting $p\left(x\right) := P\left(Y = 1\middle| X = x\right)$, we can rearrange for a `linear form', given as
\begin{equation}
\ln\left(\dfrac{p\left(x\right)}{1 - p\left(x\right)}\right) = \beta^{\top}x
\end{equation}
To fit a logistic regression to some data, we first construct the likelihood (assuming independent samples) as
\begin{align}
L\left(\beta\middle|x\right) &= P\left(Y\middle|X = x, \beta\right) \\
&= \prod_{i = 1}^{n} P\left(y_{i}\middle|X = x_{i}, \beta\right) \\
&= \prod_{i = 1}^{n} \left(\dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}\right)^{y_{i}}\left(1 - \dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}\right)^{1 - y_{i}}
\end{align}
Note that since $y_{i}$ can only be $0$ or $1$, each multiplicand `selects' either the probability of occurrence of $P\left(Y = 1\middle|X = x, \beta\right)$ if $y_{i} = 1$ or $P\left(Y = 0\middle|X = x, \beta\right)$ if $y_{i} = 0$. The log-likelihood is
\begin{equation}
\log L\left(\beta\middle|x\right) = \sum_{i = 1}^{n} y_{i}\log\left(\dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}\right) + \sum_{i = 1}^{n}\left(1 - y_{i}\right)\log\left(1 - \dfrac{1}{1 + \exp\left(-\beta^{\top}x\right)}\right)
\end{equation}
Hence the maximum-likelihood estimate is the solution to
\begin{equation}
\min_{\beta} \left\{-\log L\left(\beta\middle|x\right)\right\}
\end{equation}

\subsection{Probit Regression}

\subsection{Multinomial Logistic Regression}

\chapter{Advanced Probability}

\section{Moments}

\subsection{Moment Generating Functions}

The moment generating function of a random variable $X$ is defined as
\begin{equation}
\phi_{X}\left(s\right) =\mathbb{E}\left[e^{sX}\right]
\end{equation}
That is, it is the expectation of the random variable $e^{sX}$ as a function of $s$.

\subsection{Chernoff Bound}

Let $X$ be a random variable and let $\phi_{X}\left(s\right)$ be the moment generating function of $X$. Then for all $s\geq 0$, 
\begin{equation}
\operatorname{Pr}\left(X \geq a\right) \leq e^{-sa}\phi_{X}\left(s\right)
\end{equation}
which also means that 
\begin{equation}
\operatorname{Pr}\left(X \geq a\right) \leq \min_{s\geq 0}\left\{e^{-sa}\phi_{X}\left(s\right)\right\}
\end{equation}
\begin{proof}
From the Markov inequality, we have for some random variable $Y$ and any $b > 0$, 
\begin{equation}
\operatorname{Pr}\left(Y \geq b\right) \leq \dfrac{\mathbb{E}\left[Y\right]}{b}
\end{equation}
Let $b = e^{sa} > 0$ for any $a\in\mathbb{R}$, and let $Y = e^{sX}$. Hence
\begin{equation}
\operatorname{Pr}\left(e^{sX} \geq e^{sa}\right) \leq \dfrac{\mathbb{E}\left[e^{sX}\right]}{e^{sa}}
\end{equation}
Since $s\geq 0$, then $e^{sX} \geq e^{sa}$ is equivalent to $X \geq a$. Also, $\mathbb{E}\left[e^{sX}\right]$ is the definition of the moment generating function of $X$. Therefore
\begin{equation}
\operatorname{Pr}\left(X \geq a\right) \leq e^{-sa}\phi_{X}\left(s\right)
\end{equation}
\end{proof}

\section{Probability Generating Functions}

\section{Characteristic Functions}

\section{Cumulants}

\subsection{Cumulant Generating Functions}

\subsection{Law of Total Cumulance}

\subsection{Edgeworth Series}

\section{Multivariate Gaussian Identities}

\subsection{Conditional Gaussian Densities}

A multivariate $D$-dimensional Gaussian distribution can be denoted by the density function
\begin{equation}
p\left(x;\mu,\Sigma\right)=\left(2\pi\right)^{-D/2}\left|\Sigma\right|^{-1/2}\exp\left(-\dfrac{1}{2}\left(x-\mu\right)^{\top}\Sigma^{-1}\left(x-\mu\right)\right)
\end{equation}
where $x$ is a random vector, and $\mu$ and $\Sigma$ are the mean and covariance respectively. Here, $\left|\cdot\right|$ denotes determinant. We can also denote a Gaussian using $x\sim\mathcal{N}\left(\mu, \Sigma\right)$. For a joint distribution between two Gaussian random vectors $x$ and $y$, we can denote it as
\begin{equation}
\begin{bmatrix}x\\
y
\end{bmatrix}\sim\mathcal{N}\left(\begin{bmatrix}\mu_{x}\\
\mu_{y}
\end{bmatrix},\begin{bmatrix}A & C\\
C^{\top} & B
\end{bmatrix}\right)=\mathcal{N}\left(\begin{bmatrix}\mu_{x}\\
\mu_{y}
\end{bmatrix},\begin{bmatrix}\tilde{A} & \tilde{C}\\
\tilde{C}^{\top} & \tilde{B}
\end{bmatrix}^{-1}\right)
\end{equation}
where $x\sim\mathcal{N}\left(\mu_{x},A\right)$, $y\sim\mathcal{N}\left(\mu_{y},B\right)$, and the inverse of the block partitioned matrix can be expressed using the block inversion formula:
\begin{equation}
\begin{bmatrix}\tilde{A} & \tilde{C}\\
\tilde{C}^{\top} & \tilde{B}
\end{bmatrix}=\begin{bmatrix}A^{-1}+A^{-1}C\left(B-C^{\top}A^{-1}C\right)^{-1}C^{\top}A^{-1} & -A^{-1}C\left(B-C^{\top}A^{-1}C\right)^{-1}\\
-\left(B-C^{\top}A^{-1}C\right)^{-1}C^{\top}A^{-1} & \left(B-C^{\top}A^{-1}C\right)^{-1}
\end{bmatrix}
\end{equation}
If we want the conditional distribution of $x$ on $y$, this is
\begin{equation}
\left.x\middle|y\right.\sim\mathcal{N}\left(\mu_{x}+CB^{-1}\left(y-\mu_{y}\right),A-CB^{-1}C^{\top}\right) =\mathcal{N}\left(\mu_{x}-\tilde{A}^{-1}\tilde{C}\left(y-\mu_{y}\right),\tilde{A}^{-1}\right)
\end{equation}

\subsection{Product of Gaussian Densities}
Suppose we take the product of two Gaussian distributions. Note that this is different from the distribution of two Gaussian random variables, which will not be Gaussian distributed. The product of two Gaussian distributions will be another (unormalised) Gaussian:
\begin{equation}
\mathcal{N}\left(x; a,A\right)\mathcal{N}\left(x;b,B\right)=Z^{-1}\mathcal{N}\left(x;c,C\right)
\end{equation}
where
\begin{gather}
C=\left(A^{-1}+B^{-1}\right)^{-1} \\
c=C\left(A^{-1}a+B^{-1}b\right)
\end{gather}
and the normalising factor looks like a Gaussian:
\begin{equation}
Z^{-1}=\left(2\pi\right)^{-D/2}\left|A+B\right|^{-1/2}\exp\left(-\dfrac{1}{2}\left(a-b\right)^{\top}\left(A+B\right)^{-1}\left(a-b\right)\right)
\end{equation}
We can show the above. First replace the distributions using the definition of their density functions, and substitute the definitions of $Z$, $C$ and $c$. We get
\begin{multline}
\dfrac{\exp\left[-\dfrac{1}{2}\left(x-a\right)^{\top}A^{-1}\left(x-a\right)\right]}{\left(2\pi\right)^{D/2}\left|A\right|^{1/2}}\cdot\dfrac{\exp\left[-\dfrac{1}{2}\left(x-b\right)^{\top}B^{-1}\left(x-b\right)\right]}{\left(2\pi\right)^{D/2}\left|B\right|^{1/2}} = \\
\dfrac{\exp\left[-\dfrac{1}{2}\left(a-b\right)^{\top}\left(A+B\right)^{-1}\left(a-b\right)\right]}{\left(2\pi\right)^{D/2}\left|A+B\right|^{1/2}}\times \\
\dfrac{\exp\left[-\dfrac{1}{2}\left(x-\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right)^{\top}\left(A^{-1}+B^{-1}\right)\left(x-\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right)\right]}{\left(2\pi\right)^{D/2}\left|\left(A^{-1}+B^{-1}\right)^{-1}\right|^{1/2}}
\end{multline}
First let's show equivalence over the denominators. Equating the denominators gives
\begin{gather}
\left(2\pi\right)^{D/2}\left|A\right|^{1/2}\left(2\pi\right)^{D/2}\left|B\right|^{1/2}=\left(2\pi\right)^{D/2}\left|A+B\right|^{1/2}\left(2\pi\right)^{D/2}\left|\left(A^{-1}+B^{-1}\right)^{-1}\right|^{1/2} \\
\left|A\right|^{1/2}\left|B\right|^{1/2}=\left|A+B\right|^{1/2}\left|\left(A^{-1}+B^{-1}\right)^{-1}\right|^{1/2} \\
\left|A\right|\left|B\right|=\left|A+B\right|\left|\left(A^{-1}+B^{-1}\right)^{-1}\right|
\end{gather}
We can use a property of determinants that the determinant of the inverse is the inverse of the determinant. Hence
\begin{equation}
\left|A+B\right|=\left|A\right|\left|B\right|\left|A^{-1}+B^{-1}\right|
\end{equation}
We can apply the determinant form of the matrix inversion lemma:
\begin{equation}
\left|Z+UWV^{\top}\right|=\left|Z\right|\left|W\right|\left|W^{-1}+V^{\top}Z^{-1}U\right|
\end{equation}
(note that this $Z$ is not the same as the one defined above). Letting $Z = A$, $W = B$, $U = I$, $V = I$, we have
\begin{equation}
\left|A+B\right|=\left|A\right|\left|B\right|\left|A^{-1}+B^{-1}\right|
\end{equation}
Thus we get the same as above, hence the denominators are equal. We now begin the more tedious process of proving equivalence over the numerators. Grouping exponentials, the terms inside should be equal, ie. (after taking out the $-1/2$)
\begin{multline}
\left(x-a\right)^{\top}A^{-1}\left(x-a\right)+\left(x-b\right)^{\top}B^{-1}\left(x-b\right) \\
=\left(a-b\right)^{\top}\left(A+B\right)^{-1}\left(a-b\right)+ \\
\left(x-\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right)^{\top}\left(A^{-1}+B^{-1}\right)\left(x-\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right)
\end{multline}
Expand the quadratics to get lengthy expressions for the LHS and RHS
\begin{multline}
RHS = a^{\top}\left(A+B\right)^{-1}a-b^{\top}\left(A+B\right)^{-1}a-a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+ \red x^{\top}\left(A^{-1}+B^{-1}\right)x\black \\
-\left[\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right]^{\top}\left(A^{-1}+B^{-1}\right)x-x^{\top}\left(A^{-1}+B^{-1}\right)\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right) \\
+\left[\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)\right]^{\top}\left(A^{-1}+B^{-1}\right)\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)
\end{multline}
\begin{multline}
LHS=\red x^{\top}A^{-1}x\black -a^{\top}A^{-1}x-x^{\top}A^{-1}a+a^{\top}A^{-1}a+ \red x^{\top}B^{-1}x\black -b^{\top}B^{-1}x-x^{\top}B^{-1}b+b^{\top}B^{-1}b
\end{multline}
The highlighted terms in \red red \black cancel out. So we have (in addition to grouping some similar terms such as $b^{\top}\left(A+B\right)^{-1}a$ and $a^{\top}\left(A+B\right)^{-1}b$ together since they are scalar and $A$, $B$ are symmetric):
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b \\
-2x^{\top}\blue\left(A^{-1}+B^{-1}\right)\left(A^{-1}+B^{-1}\right)^{-1}\black \left(A^{-1}a+B^{-1}b\right) \\
+\left(A^{-1}a+B^{-1}b\right)^{\top}\blue\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}+B^{-1}\right)\black \left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)
\end{multline}
\begin{equation}
LHS=-a^{\top}A^{-1}x-x^{\top}A^{-1}a+a^{\top}A^{-1}a-b^{\top}B^{-1}x-x^{\top}B^{-1}b+b^{\top}B^{-1}b
\end{equation}
The highlighted terms in \blue blue \black cancel out because they are the inverses of each other. Also by grouping similar terms in the LHS, we get
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b \\
\green -2x^{\top}\left(A^{-1}a+B^{-1}b\right)\black +\left(A^{-1}a+B^{-1}b\right)^{\top}\left(A^{-1}+B^{-1}\right)^{-1}\left(A^{-1}a+B^{-1}b\right)
\end{multline}
\begin{equation}
LHS=\green -2x^{\top}A^{-1}a\black +a^{\top}A^{-1}a \green -2x^{\top}B^{-1}b\black +b^{\top}B^{-1}b
\end{equation}

Note now that another set of highlighted terms in \green green \black cancel out on both sides. So
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+ \\
\left(A^{-1}a+B^{-1}b\right)^{\top}\red\left(A^{-1}+B^{-1}\right)^{-1}\black\left(A^{-1}a+B^{-1}b\right)
\end{multline}
\begin{equation}
LHS=a^{\top}A^{-1}a+b^{\top}B^{-1}b
\end{equation}

We deal with the highlighted term in \red red \black above using the matrix inversion lemma:
\begin{equation}
\left(A^{-1}+B^{-1}\right)^{-1}=A-A\left(A+B\right)^{-1}A
\end{equation}
Substituting this into the RHS:
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b \\
+\left(a^{\top}A^{-1}+b^{\top}B^{-1}\right)\left(A-A\left(A+B\right)^{-1}A\right)\left(A^{-1}a+B^{-1}b\right)
\end{multline}
Expanding out the quadratic, this gives the very length expression
\begin{multline}
RHS=a^{\top}\left(A+B\right)^{-1}a-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+a^{\top}A^{-1}AA^{-1}a \\
+a^{\top}A^{-1}AB^{-1}b+b^{\top}B^{-1}AA^{-1}a+b^{\top}B^{-1}AB^{-1}b-a^{\top}A^{-1}A\left(A+B\right)^{-1}AA^{-1}a \\
-b^{\top}B^{-1}A\left(A+B\right)^{-1}AA^{-1}a-a^{\top}A^{-1}A\left(A+B\right)^{-1}AB^{-1}b-b^{\top}B^{-1}A\left(A+B\right)^{-1}AB^{-1}b
\end{multline}
We can notice instances where $A^{-1}A$ or $AA^{-1}$ appears and cancel them out.
\begin{multline}
RHS=\blue a^{\top}\left(A+B\right)^{-1}a\black -2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+ \green a^{\top}A^{-1}a \black \\
+a^{\top}B^{-1}b+b^{\top}B^{-1}a+b^{\top}B^{-1}AB^{-1}b- \blue a^{\top}\left(A+B\right)^{-1}a\black -b^{\top}B^{-1}A\left(A+B\right)^{-1}a \\
-a^{\top}\left(A+B\right)^{-1}AB^{-1}b-b^{\top}B^{-1}A\left(A+B\right)^{-1}AB^{-1}b
\end{multline}
We can cancel out some \blue blue \black terms within the RHS, as well as cancel the $\green a^{\top}A^{-1}a \black$ from the LHS. We are left with
\begin{multline}
RHS=-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+a^{\top}B^{-1}b+b^{\top}B^{-1}a+b^{\top}B^{-1}AB^{-1}b \\
-b^{\top}B^{-1}A\left(A+B\right)^{-1}a-a^{\top}\left(A+B\right)^{-1}AB^{-1}b-b^{\top}B^{-1}A\left(A+B\right)^{-1}AB^{-1}b
\end{multline}
\begin{equation}
LHS=b^{\top}B^{-1}b
\end{equation}
We can also group similar terms in the RHS, giving
\begin{multline}
RHS=-2a^{\top}\left(A+B\right)^{-1}b+b^{\top}\left(A+B\right)^{-1}b+2a^{\top}B^{-1}b+ \\
b^{\top}B^{-1}AB^{-1}b-2a^{\top}\left(A+B\right)^{-1}AB^{-1}b-b^{\top}B^{-1}A\left(A+B\right)^{-1}AB^{-1}b
\end{multline}
Next we subtract the LHS from the RHS and factorise out $a^{\top}\left(\cdot\right)b$ and $b^{\top}\left(\cdot\right)b$:
\begin{multline}
RHS-LHS=a^{\top}\green\underbrace{\black\left(-2\left(A+B\right)^{-1}+2B^{-1}-2\left(A+B\right)^{-1}AB^{-1}\right)}_{\green=0}\black b \\
+b^{\top}\green\underbrace{\black\left(\left(A+B\right)^{-1}+B^{-1}AB^{-1}-B^{-1}A\left(A+B\right)^{-1}AB^{-1}\right)}_{\green =0}\black b
\end{multline}
Thus to show equivalence, we need to show that the terms inside the brackets are equal to zero. This requires carrying out some manipulations. Starting from the first term:
\begin{align}
-2\left(A+B\right)^{-1}+2B^{-1}-2\left(A+B\right)^{-1}AB^{-1}&=0 \\
-\left(A+B\right)^{-1}+B^{-1}-\left(A+B\right)^{-1}AB^{-1}&=0 \\
B^{-1}&=\left(A+B\right)^{-1}\left(I+AB^{-1}\right) \\
A+B&=\left(I+AB^{-1}\right)B \\
A+B&=B+A
\end{align}
For the second term:
\begin{align}
\left(A+B\right)^{-1}+B^{-1}AB^{-1}-B^{-1}A\left(A+B\right)^{-1}AB^{-1}&=0 \\
B^{-1}\left(AB^{-1}-A\left(A+B\right)^{-1}AB^{-1}-I\right)&=-\left(A+B\right)^{-1} \\
\left(AB^{-1}-A\left(A+B\right)^{-1}AB^{-1}-I\right)\left(A+B\right)&=-B \\
AB^{-1}A-A\left(A+B\right)^{-1}AB^{-1}A-A+A-A\left(A+B\right)^{-1}A-B&=-B \\
AB^{-1}A-A\left(A+B\right)^{-1}AB^{-1}A-A\left(A+B\right)^{-1}A&=0 \\
A\left(B^{-1}-\left(A+B\right)^{-1}AB^{-1}-\left(A+B\right)^{-1}\right)A&=0
\end{align}
Since $A \neq 0$ (we have already taken the inverse of $A$ up until this stage):
\begin{align}
B^{-1}-\left(A+B\right)^{-1}AB^{-1}-\left(A+B\right)^{-1}&=0 \\
B^{-1}&=\left(A+B\right)^{-1}\left(AB^{-1}+I\right) \\
A+B&=\left(AB^{-1}+I\right)B \\
A+B&=A+B
\end{align}
Hence the terms inside the brackets are zero, so the LHS equals the RHS. This finally shows equivalence over the numerators.

\subsection{Marginalisation of Gaussians}

Suppose we have distributions $p\left(x\right)$ and $p\left(y\middle|x\right)$, and we want to obtain the distribution $p\left(y\right)$. We can do this by first computing the joint distribution $p\left(x, y\right) = p\left(x\right)p\left(y\middle|x\right)$ and integrating over $x$ as follows
\begin{equation}
p\left(y\right) = \int p\left(x, y\right)dx
\end{equation}
This is known as marginalisation. Suppose $p\left(x\right)$ and $p\left(y\middle|x\right)$ are Gaussians in the sense that
\begin{gather}
p\left(x\right) = \mathcal{N}_{x}\left(a, A^{-1}\right) \\
p\left(y\middle|x\right) = \mathcal{N}_{y}\left(C^{\top}x, B^{-1}\right)
\end{gather}
Note that $x$ and $y$ do not have to be of the same dimension. The general technique to evaluate the integral is from the joint distribution, decompose it into $p\left(x, y\right) = p\left(y\right)p\left(x\middle|y\right)$. Then
\begin{align}
\int p\left(x, y\right)dx &= \int p\left(y\right)p\left(x\middle|y\right)dx
\\
&= p\left(y\right) \int p\left(x\middle|y\right)dx \\
&= p\left(y\right)
\end{align}
since the integral of $p\left(x\middle|y\right)$ evaluates to 1. Without worrying about normalising constants, we substitute the exponential expressions for the Gaussian distributions
\begin{align}
p\left(y\right) &\propto \int\mathcal{N}_{x}\left(a,A^{-1}\right)\mathcal{N}_{y}\left(C^{\top}x,B^{-1}\right)dx \\
&\propto \int\exp\left[-\dfrac{1}{2}\left(\left(x-a\right)^{\top}A\left(x-a\right)+\left(y-C^{\top}x\right)^{\top}B\left(y-C^{\top}x\right)\right)\right]dx \\
&\propto \int\exp\left[-\dfrac{1}{2}\left(x^{\top}Ax-2a^{\top}Ax+a^{\top}Aa+y^{\top}By-2y^{\top}BC^{\top}x+x^{\top}CBC^{\top}x\right)\right]dx \\
&\propto \int\exp\left[-\dfrac{1}{2}\left(x^{\top}\left(A+CBC^{\top}\right)x-2\left(a^{\top}A+y^{\top}BC^{\top}\right)x+a^{\top}Aa+y^{\top}By\right)\right]dx
\end{align}
To decompose the distributions, we complete the square:
\begin{lemma}[Completing the Square]
\begin{equation}
\dfrac{1}{2}x^{\top}Mx + d^{\top}x + e = \dfrac{1}{2}\left(x - m\right)^{\top}M\left(x - m\right) + v
\end{equation}
where
\begin{gather}
m = -C^{-1}d \\
v= e - \dfrac{1}{2}d^{\top}M^{-1}d
\end{gather}
\end{lemma}
So by letting $M = A + CBC^{\top}$, $d = -Aa - CBy$, and $e = \dfrac{1}{2}a^{\top}Aa + \dfrac{1}{2}y^{\top}By$, this gives
\begin{gather}
m = \left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right) \\
v = \dfrac{1}{2}a^{\top}Aa + \dfrac{1}{2}y^{\top}By - \dfrac{1}{2}\left(Aa + CBy\right)^{\top}\left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right)
\end{gather}
So we can write
\begin{equation}
p\left(y\right)\propto\int\exp\left[-\dfrac{1}{2}\left(\left(x-m\right)^{\top}M\left(x-m\right)+v\right)\right]dx
\end{equation}
and from this we can see that
\begin{align}
p\left(x\middle|y\right) &= \mathcal{N}_{x}\left(m, M^{-1}\right) \\
&= \mathcal{N}_{x}\left(\left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right), \left(A + CBC^{\top}\right)^{-1}\right) \\
\end{align}
For the remaining terms, these can be taken out of the integral
\begin{align}
p\left(y\right) &\propto \exp\left(-\dfrac{1}{2}v \right)\int \mathcal{N}_{x}\left(\left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right), \left(A + CBC^{\top}\right)^{-1}\right) dx \\
&\propto \exp\left(-\dfrac{1}{2}v \right) \\
&\propto \exp\left[-\dfrac{1}{2}\left(a^{\top}Aa + y^{\top}By- \left(Aa + CBy\right)^{\top}\left(A + CBC^{\top}\right)^{-1}\left(Aa + CBy\right)\right) \right] \\
&\begin{multlined} \propto \exp\left[-\dfrac{1}{2}\left(y^{\top}By-y^{\top}BC^{\top}\left(A+CBC^{\top}\right)^{-1}CBy-2a^{\top}A\left(A+CBC^{\top}\right)^{-1}CBy\right.\right. \\
\left.\left. +a^{\top}Aa-a^{\top}A\left(A+CBC^{\top}\right)^{-1}Aa\right)\right]
\end{multlined}
\end{align}
Getting rid of the terms which don't depend on $y$
\begin{align}
p\left(y\right) &\propto \exp\left[-\dfrac{1}{2}\left(y^{\top}By-y^{\top}BC^{\top}\left(A+CBC^{\top}\right)^{-1}CBy-2a^{\top}A\left(A+CBC^{\top}\right)^{-1}CBy\right)\right] \\
&\propto \exp\left[-\dfrac{1}{2}\left(y^{\top}\left(B - BC^{\top}\left(A+CBC^{\top}\right)^{-1}CB\right)y-2a^{\top}A\left(A+CBC^{\top}\right)^{-1}CBy\right)\right]
\end{align}
Note by the matrix inversion lemma, $B - BC^{\top}\left(A+CBC^{\top}\right)^{-1}CB = \left(B^{-1} + C^{\top}A^{-1}C\right)^{-1}$. This yields the simplification
\begin{equation}
p\left(y\right) \propto \exp\left[-\dfrac{1}{2}\left(y^{\top}\left(B^{-1} + C^{\top}A^{-1}C\right)^{-1}y-2a^{\top}A\left(A+CBC^{\top}\right)^{-1}CBy\right)\right]
\end{equation}
Once again we can complete the square in $y$. Doing so (and ignoring the terms which don't depend on $y$ due to proportionality) gives
\begin{multline}
p\left(y\right) \propto \exp\left[-\dfrac{1}{2}\left(y - \left(B^{-1} + C^{\top}A^{-1}C\right)BC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa\right)^{\top}\cdot\left(B^{-1} + C^{\top}A^{-1}C\right)^{-1} \right. \\
\left. \cdot\left(y - \left(B^{-1} + C^{\top}A^{-1}C\right)BC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa\right)\right]
\end{multline}
This gives the distribution for $p\left(y\right)$
\begin{equation}
p\left(y\right) = \mathcal{N}_{y}\left(\left(B^{-1} + C^{\top}A^{-1}C\right)BC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa, B^{-1} + C^{\top}A^{-1}C\right)
\end{equation}
To simplify the mean,we can show that
\begin{equation}
\left(B^{-1} + C^{\top}A^{-1}C\right)BC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa = C^{\top}a
\end{equation}
Expanding the LHS gives
\begin{align}
C^{\top}\left(A+CBC^{\top}\right)^{-1}Aa + C^{\top}A^{-1}CBC^{\top}\left(A+CBC^{\top}\right)^{-1}Aa &= C^{\top}a \\
C^{\top}\green\underbrace{\black\left[\left(A+CBC^{\top}\right)^{-1}A + A^{-1}CBC^{\top}\left(A+CBC^{\top}\right)^{-1}A\right]}_{\green=I}\black a &= C^{\top}a
\end{align}
So we require $\left(A+CBC^{\top}\right)^{-1}A + A^{-1}CBC^{\top}\left(A+CBC^{\top}\right)^{-1}A = I$ by hypothesis. Some manipulation yields
\begin{gather}
\left(I + A^{-1}CBC^{\top}\right)\left(A+CBC^{\top}\right)^{-1}A = I \\
I + A^{-1}CBC^{\top} = A^{-1}\left(A+CBC^{\top}\right) \\
I + A^{-1}CBC^{\top} = I + A^{-1}CBC^{\top}
\end{gather}
Therefore we finally have
\begin{equation}
p\left(y\right) = \mathcal{N}_{y}\left(C^{\top}a, B^{-1} + C^{\top}A^{-1}C\right)
\end{equation}

\section{Exponential Families}

\section{Random Matrices}

\subsection{Wishart Distribution}

\section{Stochastic Processes}

\subsection{Second-order Processes}

Second-order processes have finite second moments, ie. for a continuous time process:
\begin{equation}
\mathbb{E}\left[X\left(t\right)^{2}\right] < \infty
\end{equation}
for all $t$.

\subsection{Strict Stationarity}

\subsection{Wide-Sense Stationarity}

A wide-sense stationary process does not need to be strictly stationary. For example, a sequence of uncorrelated random variables with the same mean and variance is wide-sense stationary by definition, but it will not be strictly stationary if these random variables are not identically distributed. \\

A strictly stationary process does not always also imply it is wide-sense stationary. For example, a sequence of i.i.d Cauchy random variables is strictly stationary, but not wide-sense stationary because the Cauchy distribution does not have a finite second moment. Generally however, second-order processes which are strictly stationary are also wide-sense stationary.

\chapter{Bayesian Probability \& Statistics}

\section{Cox's Theorem}

\section{Bayesian Updating}

\subsection{Rule of Succession}

\subsection{Odds Ratio}

\subsection{Log Odds}

\section{Bayesian Networks}

\subsection{Factor Graphs}

\section{Bayesian Inference}

\subsection{Bayesian Priors}

\subsubsection{Principle of Indifference}

The principle of indifference says that if an experiment yields $n$ mutually exclusive outcomes, and nothing else is known, then the prior probability of each outcome should be assigned $1/n$.

\subsubsection{Uninformative Priors}

An uninformative prior gives no subjective information. An improper prior may be used as an uninformative prior. An improper prior is a prior whose sum or integral does not necessarily sum to 1. From Bayes' theorem, it can be seen that scaling all prior probabilities or densities by some constant will still give the same result for the posterior. So it is still possible to use improper priors in Bayesian inference. An example of an improper prior is a uniform distribution over an infinite interval.

\subsection{Maximum a Posteriori}

\subsection{Credible Intervals}

\subsection{Bayesian Linear Regression \cite{Rasmussen2006}}

\subsection{Conjugate Priors}

\subsection{Hierarchical Bayes \cite{Rasmussen2006}}

Consider a model over parameters $\mathbf{w}$ (eg. weights in a neural network), hyperparameters $\boldsymbol{\theta}$ (eg. regularisation in a cost function) and a discrete set of structures $\mathcal{H}_{i}$ (eg. number of layers and nodes). We can conduct inference on these using a hierarchical approach. On the lowest level (level 1) is inference over the weights, given inputs $X$, outputs $\mathbf{y}$ and $\boldsymbol{\theta}$, $\mathcal{H}_{i}$.
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|\mathbf{w}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)p\left(\mathbf{w}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)}
\end{equation}
As a common assumption is that $\mathbf{w}$ and $X$ are independent
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|\mathbf{w}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)p\left(\mathbf{w}\middle|\boldsymbol{\theta}, \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)}
\end{equation}
A property of hyperparameters is that they can be selected before we gather training data and before we begin training. It follows that $\boldsymbol{\theta}$ should be independent of $\mathbf{y}$, giving
\begin{equation}
p\left(\mathbf{w}\middle|\mathbf{y}, X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|\mathbf{w}, X, \mathcal{H}_{i}\right)p\left(\mathbf{w}\middle|\boldsymbol{\theta}, \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right)}
\end{equation}
In level 2, we conduct inference over $\boldsymbol{\theta}$.
\begin{equation}
p\left(\boldsymbol{\theta}\middle|\mathbf{y}, X, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) p\left(\boldsymbol{\theta}\middle| X, \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right)}
\end{equation}
Again, we argue that $\boldsymbol{\theta}$ should be independent of the data $X$, so
\begin{equation}
p\left(\boldsymbol{\theta}\middle|\mathbf{y}, X, \mathcal{H}_{i}\right) = \dfrac{p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) p\left(\boldsymbol{\theta}\middle| \mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right)}
\end{equation}
Note that the marginal likelihood in level 1 can be expressed as in integral over $\mathbf{w}$:
\begin{equation}
p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) = \int p\left(\mathbf{y}\middle|\mathbf{w}, X, \mathcal{H}_{i}\right)p\left(\mathbf{w}\middle|\boldsymbol{\theta}, \mathcal{H}_{i}\right) d\mathbf{w}
\end{equation}
Notice that this level 1 marginal likelihood is the same as the level 2 likelihood. In practical terms, we will conduct inference on $\boldsymbol{\theta}$ by integrating over $\mathbf{w}$ using some prior for $\mathbf{w}$ to find the likelihood. We also require a hyperprior $p\left(\boldsymbol{\theta}\middle|\mathcal{H}_{i}\right)$ for $\boldsymbol{\theta}$. Once this is done, then we may perform inference on $\mathbf{w}$. \\

In level 3, inference is performed over the model structures. As $\mathcal{H}_{i}$ was defined to be discrete, we use $P\left(\cdot\right)$ to denote the probability mass function (in contrast to $p\left(\cdot\right)$ for the probability density function). Bayes' theorem for the posterior of $\mathcal{H}_{i}$ gives
\begin{equation}
P\left(\mathcal{H}_{i}\middle|\mathbf{y}, X\right) = \dfrac{p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right)P\left(\mathcal{H}_{i}\right)}{p\left(\mathbf{y}\middle|X\right)}
\end{equation}
The likelihood function $p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right)$ is the same as the marginal likelihood for level 2, and can be computed using the integral
\begin{equation}
p\left(\mathbf{y}\middle|X, \mathcal{H}_{i}\right) = \int p\left(\mathbf{y}\middle|X, \boldsymbol{\theta}, \mathcal{H}_{i}\right) p\left(\boldsymbol{\theta}\middle| \mathcal{H}_{i}\right) d\boldsymbol{\theta}
\end{equation}
As $P\left(\mathcal{H}_{i}\right)$ is a probability mass function, the level 3 marginal likelihood is computed using the sum
\begin{equation}
p\left(y\middle|X\right) = \sum_{i}p\left(y\middle|X, \mathcal{H}_{i}\right)P\left(\mathcal{H}_{i}\right)
\end{equation}
ie. it is a weighted average of some probability density functions, and acts as a normalising constant for the posterior of $\mathcal{H}_{i}$.

\section{Bayesian Classifiers}

\chapter{Markov Processes}

\section{Markov Chains}

\section{Hidden Markov Models}

\subsection{Baum-Welch Algorithm}

\section{Markov Decision Processes}

\subsection{Partially Observable Markov Decision Processes}

\section{Markov Chain Monte Carlo}

\subsection{Gibbs Sampling}

\subsection{Metropolis-Hastings Algorithm}

\chapter{Measure Theoretic Probability}

\section{Probability Spaces}

\subsection{Concepts in Probability Spaces}

\subsubsection{Elementary Events}
An elementary event is an event which contains only a single outcome in the sample space. For example, in the experiment that a coin is flipped twice, the outcomes are $\left\{H, T\right\}$, $\left\{T, H\right\}$, $\left\{H, H\right\}$ and $\left\{T, T\right\}$. The event `heads followed by tails' is an elementary event because there is only a single outcome associated to it: $\left\{H, T\right\}$. However, the event `at least one heads' is not an elementary event because there are three outcomes associated to it: $\left\{H, T\right\}$, $\left\{T, H\right\}$ and $\left\{H, H\right\}$.

\subsubsection{Power Sets}
The power set of a set $S$ is the set of all subsets of $S$, including the empty set and $S$ itself. If $S$ is a finite set with cardinality $\left|S\right| = n$, then the number of subsets of $S$ is $2^{n}$ (related to the binomial theorem). This motivates the notation for the power set of $S$ as $2^{S}$. \\

The power set of all functions from $Y$ to $X$ can be denoted $X^{Y}$.

\subsubsection{Countable Sets}
A countable set $S$ has the same cardinality $\left|S\right|$ as some subset of the natural numbers. Intuitively speaking, a set is countable we can in some way assign numbers $1, 2, 3, \dots$ to each element in $S$ (ie. we can `enumerate' the elements of $S$). A finite set will always be countable, but some infinite sets can still be countably infinite. For example, the set of natural numbers itself is trivially countably infinite. We can also enumerate through the set of rational numbers in the following way:
\begin{equation*}
\begin{array}{ccccccccc}
1/1 &  & 1/2 & \rightarrow & 1/3 &  & 1/4 & \rightarrow & 1/5\\
\downarrow & \nearrow &  & \swarrow &  & \nearrow &  & \swarrow\\
2/1 &  & 2/2 &  & 2/3 &  & 2/4 &  & \iddots\\
 & \swarrow &  & \nearrow &  & \swarrow &  & \nearrow\\
3/1 &  & 3/2 &  & 3/3 &  & 3/4\\
\downarrow & \nearrow &  & \swarrow &  & \nearrow\\
4/1 &  & 4/2 &  & 4/3\\
 & \swarrow &  & \nearrow\\
5/1 &  & 5/2\\
\downarrow & \nearrow\\
6/1
\end{array}
\end{equation*}

Similarly, the set of all rational numbers is also countable. One possible enumeration is that we can imagine starting at 0, and then enumerating back a forth between a second `layer' of a grid of negative rational numbers behind the grid of positive rational numbers. \\

An example of an uncountable set is the set of real numbers.

\subsubsection{Measurable Sets}
An example of sets which are not measurable are Vitali sets. A Vitali set $V$ is a subset of the interval $\left[0, 1\right]$ such that for each real number $r \in \mathbb{R}$, there is exactly one number $v \in V$ such that $v - r$ is a rational number. A numerical example of this is with the real number $10 + 1/\sqrt{2}$, we can choose $1/\sqrt{2} \in \left[0, 1\right]$ such that $1/\sqrt{2} - \left(10 - 1/\sqrt{2}\right)$ is rational. There are uncountably many Vitali sets, but all Vitali sets will satisfy this property. \\

We can show that Vitali sets are unmeasureable in the following way. Define the enumeration of all rational numbers in $\left[-1, 1\right]$ to be $q_{1}, q_{2}, \dots$. Define the translated Vitali sets $V_{k} := V + q_{k} = \left\{v + q_{k}: v \in V\right\}$ for $k = 1, 2, \dots$ so that $v_{k} = v + q_{k} \in V_{k}$. These sets must be disjoint in order to satisfy the definition that there is exactly one $v \in V$ for each real number. If shifting $V$ by $q_{k}$ causes $V$ and $V_{k}$ to have elements in common, then it implies $V$ is not a Vitali set. Another way to state this is that there is no gap between any two elements in $V$ equal to a rational number. If this were the case, then there could be more than one $v \in V$ that could make $v - r$ rational. Hence shifting $V$ by a rational number $q_{k}$ will ensure $V_{k}$ and $V$ will be disjoint. We can write the following inclusion
\begin{equation}
\bigcup_{k}V_{k} \subseteq \left[-1, 2\right]
\end{equation}
since $v \in \left[0, 1\right]$ and $q_{k} \in \left[-1, 1\right]$, then $-1 \leq v + q_{k} \leq 2$. Furthermore, consider any real number $v_{i} \in \left[0, 1\right]$. Then by definition there will be exactly one $v \in \left[0, 1\right]$ such that $v - v_{i} = -q_{i}$, where $q_{i} \in \left[-1,1\right]$ since $-1 \leq v - v_{i} \leq 1 \Rightarrow -1 \leq q_{i} \leq 1$. Therefore $v_{i} \in V_{i}$ and we can write
\begin{equation}
\left[0, 1\right] \subseteq \bigcup_{k}V_{k} \subseteq \left[-1, 2\right]
\end{equation}
Suppose we can take the Lebesque measure $\lambda\left(\cdot\right)$ of these inclusions.
\begin{equation}
1 \leq \sum_{k = 1}^{\infty}\lambda\left(V_{k}\right) \leq 3
\end{equation}
Since the Lebesque measure is translation invariant, we have $\lambda\left(V_{k}\right) = \lambda\left(V\right)$ (a constant) and then
\begin{equation}
1 \leq \sum_{k = 1}^{\infty}\lambda\left(V\right) \leq 3
\end{equation}
This results in a contradiction. The infinite sum must either be zero or infinity, but neither is between 1 and 3. So a Vitali set is not Lebesgue measurable.

\subsubsection{Borel Sets}

A Borel set is a set in topological space $\Omega$ (eg. sample space) that can be formed from the operations of countable union, countable intersection, and relative complement (ie. the relative complement of $A$ in $B$ is denoted $A\setminus B$) of open (or equivalently, of closed sets) in $\Omega$.

\subsubsection{$\sigma$-algebras}

The $\sigma$-algebra of a set $\Omega$ is a collection of subsets of $\Omega$ which:
\begin{itemize}
\item includes the empty subset.
\item is closed under complement (ie. the complement of a member of the $\sigma$-algebra is also a member of the $\sigma$-algebra).
\item is closed under countable unions (ie. the countable union of members of the $\sigma$-algebra is also a member of the $\sigma$-algebra).
\item is closed under countable intersections (ie. the countable intersection of members of the $\sigma$-algebra is also a member of the $\sigma$-algebra).
\end{itemize}
In general, the $\sigma$-algebra of $\Omega$ is a subset of the power set of $\Omega$.

\subsubsection{Sub-$\sigma$-algebras}

Suppose $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$. Then another $\sigma$-algebra $\mathcal{F}_{1}$ on $\Omega$ is said to be a sub-$\sigma$-algebra of $\mathcal{F}$ if $\mathcal{F}_{1} \subseteq \mathcal{F}$.

\subsubsection{Borel $\sigma$-algebras}

The Borel $\sigma$-algebra of a set $\Omega$ is the collection of all Borel sets of $\Omega$. The Borel $\sigma$-algebra gives the smallest $\sigma$-algebra containing all open sets (or equivalently, all closed sets) of $\Omega$.
\begin{itemize}
\item In the case where $\Omega$ is a countable set, then the power set is identical to the Borel $\sigma$-algebra.
\item If $\Omega$ is the real line $\mathbb{R}$, then the Borel $\sigma$-algebra includes all `reasonable' (ie. measurable) open and closed intervals, as well as their countable union/intersection and relative complement. 
\end{itemize}

\subsubsection{Boundary Sets}

The boundary set of $\mathcal{B}$ is the set of points in the closure of $\mathcal{B}$ but not in the interior of $\mathcal{B}$. The boundary set is denoted $\partial\mathcal{B}$.

\subsubsection{Continuity Sets}

For a random variable (or vector) $X$, a Borel set $\mathcal{B}$ is called a continuity set if
\begin{equation}
\mathbb{P}\left(X\in \partial\mathcal{B}\right) = 0
\end{equation}
For example, if $X$ is a Bernoulli random variable, then $\left[0, 1\right]$ nor $\left(0, 1\right)$ are considered continuity sets, however $\left(-1, -0.5\right)$, $\left[0.1, 0.9\right)$ and $\left[1.1, \infty\right)$ would be considered continuity sets. A continuity set can be made of any points at which the cumulative distribution function of $X$ is continuous.

\subsection{Probability Triple $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$}
In measure theoretic probability, a probability space is a measure space denoted with the three-tuple $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$ where $\Omega$ is the sample space, $\mathcal{F}$ is the event space and $\mathbb{P}$ is a probability measure. \\

The set $\Omega$ contains each elementary event. The event space $\mathcal{F}$ is formed by taking a $\sigma$-algebra of $\Omega$. A typical choice of $\sigma$-algebra is the Borel $\sigma$-algebra. The function $\mathbb{P}: \mathcal{F} \to \left[0, 1\right]$ is called a probability measure, which maps events to probabilities.

\subsubsection{Lebesgue Integration}

A real valued function $f: \Omega \to \mathbb{R}$ is said to be measurable if for every $B\in\mathcal{B}$ (where $\mathcal{B}$ is a $\sigma$-algebra of $\mathbb{R}$), the preimage of $B$ under $f$ is a $\sigma$-algebra of $\Omega$, denoted $\mathcal{F}$. Explicitly,
\begin{equation}
f^{-1}\left(B\right) := \left\{x \in \Omega \middle| f\left(x\right) \in B\right\} \in \mathcal{F}
\end{equation}
for all $B \in\mathcal{B}$. Measurability is subject to the choice of $\sigma$-algebras $\mathcal{B}$ and $\mathcal{F}$, but we normally take these to be the Borel $\sigma$-algebras. An example of a non-measurable function is an indicator function for a non-measurable set.\\

In a probability space $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$, the Lebesgue integral of a measurable function $f\left(x\right)$ over a measurable subset $E$ of $\Omega$ with respect to the measure $\mathbb{P}$ may be denoted as
\begin{equation}
\int_{E}f\left(x\right)\mathrm{d}\mathbb{P}\left(x\right)
\end{equation}

\subsection{Probability Axioms}

Let $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$ be a probability space with sample space $\Omega$, event space $\mathcal{F}$ and probability measure $\mathbb{P}$.
\begin{enumerate}
\item The probability of an event $E \in \mathcal{F}$ is a nonnegative real number
\begin{equation}
0 \leq \mathbb{P}\left(E\right) \in \mathbb{R}
\end{equation}
for all $E \in \mathcal{F}$.
\item The probability that at least one of the elementary events in the entire sample space will occur is 1.
\begin{equation}
\mathbb{P}\left(\Omega\right) = 1
\end{equation}
\item Any countable sequence of disjoint (mutually exclusive) events $E_{1}$, $E_{2}$, $\dots$ satisfies
\begin{equation}
\mathbb{P}\left(\bigcup_{i = 1}^{\infty}E_{i}\right) = \sum_{i = 1}^{\infty}\mathbb{P}\left(E_{i}\right)
\end{equation}
\end{enumerate}
From the axioms, we can deduce further properties
\begin{itemize}
\item If $A \subseteq B$, the we have the monotonicity property
\begin{equation}
\mathbb{P}\left(A\right) \leq \mathbb{P}\left(B\right)
\end{equation}
This can be seen by defining events $E_{1} = A$, $E_{2} = B\setminus A$ where $A\subseteq B$, and $E_{i} = \O$ for all $i\geq 3$. These sets are all disjoint (we may alternatively define disjoint sets to be sets whose intersection is the empty set). Additionally, we have $\bigcup_{i = 1}^{\infty}E_{i} = B$. By the third axiom
\begin{align}
\sum_{i = 1}^{\infty}\mathbb{P}\left(E_{i}\right) &= \mathbb{P}\left(A\right) + \mathbb{P}\left(B\setminus A\right) + \sum_{i = 3}^{\infty}\mathbb{P}\left(\O\right) \\
&= \mathbb{P}\left(B\right)
\end{align}
The terms $\mathbb{P}\left(B\setminus A\right)$ and $\sum_{i = 3}^{\infty}\mathbb{P}\left(\O\right)$ are non-negative, hence $\mathbb{P}\left(A\right) \leq \mathbb{P}\left(B\right)$.
\item The probability of the empty set is zero.
\begin{equation}
\mathbb{P}\left(\O\right) = 0
\end{equation}
In deriving the above, we saw that the sum in $\mathbb{P}\left(A\right) + \mathbb{P}\left(B\setminus A\right) + \sum_{i = 3}^{\infty}\mathbb{P}\left(\O\right)$ was convergent. Therefore it must be that $\mathbb{P}\left(\O\right) = 0$, otherwise the sum would be infinite.
\item The numeric bound applies to any event $E \in \mathcal{F}$:
\begin{equation}
0 \leq \mathbb{P}\left(E\right) \leq 1
\end{equation}
This can be shown by applying the non-negativity axiom and the monotonicity property to any subset of $\Omega$ since $\mathbb{P}\left(\Omega\right) = 1$.
\end{itemize}

\subsection{Random Variables}

In measure theoretic probability, a real valued random variable $X$ is defined as a measurable function $X: \Omega \to \mathbb{R}$. For each $\omega \in \Omega$, the function $X\left(\omega\right)$ assigns a number to each outcome in the sample space. For example, in an experiment involving a sequence of coin tosses, let the sample space be $\Omega = \left\{H, T, HH, HT, TH, TT\right\}$. We can define the random variable $X\left(\omega\right)$ to be the number of heads tossed. In that case, we have $X\left(H\right) = 1$, $X\left(T\right) = 0$, $X\left(HH\right) = 2$, $X\left(HT\right) = 1$, $X\left(TH\right) = 1$, $X\left(TT\right) = 0$. \\

The function $X^{-1}: \mathcal{B} \to \mathcal{F}$ is a mapping from the Borel $\sigma$-algebra of $\mathbb{R}$ to a $\sigma$-algebra of $\Omega$. It is defined to be
\begin{equation}
X^{-1}\left(B\right) = \left\{\omega\in\Omega\middle|X\left(\omega\right)\in B\right\}
\end{equation}
That is, given a Borel set $B$, the function finds all the elementary events $\omega\in\Omega$ which lead to the random variable $X$ being in $B$. For the example sample space $\Omega$ above, we can say that $X^{-1}\left(\left\{2\right\}\right) = \left\{HH\right\}$ and $X^{-1}\left(\left\{1\right\}\right) = \left\{H, HT, TH\right\}$.

\subsubsection{$\sigma$-algebra Generated by a Random Variable}

Let $X: \Omega\to\mathbb{R}$ be a random variable. Denote by $\mathcal{B}$ the Borel $\sigma$-algebra of $\mathbb{R}$. Then the $\sigma$-algebra generated by $X$, denoted $\sigma\left(X\right)$, is defined as
\begin{equation}
\sigma\left(X\right) = \left\{X^{-1}\left(B\right)\middle|B\in\mathcal{B}\right\}
\end{equation}
That is, for every Borel set of $\mathbb{R}$ we find the subset of $\Omega$ which leads to $X$ being in the Borel set. The collection of all these subsets is known as the $\sigma$-algebra generated by $X$. This will also be known as the smallest $\sigma$-algebra for which $X$ is measurable.

\subsection{Expectation}

The expectation of a random variable $X\left(\omega\right)$ is defined using the Lebesgue integral
\begin{equation}
\mathbb{E}\left[X\right] = \int_{\Omega}X\left(\omega\right)\mathrm{d}\mathbb{P}\left(\omega\right)
\end{equation}

\section{Borel-Cantelli Lemma}

\subsection{Limit Supremum of a Sequence of Events}

Let $E_{1}, E_{2}, \dots$ be a sequence of events in some probability space. The limit supremum of the sequence of events is the set of all outcomes where the event $E_{n}$ occurs infinitely many times in the infinite sequence of events. That is,
\begin{align}
\limsup_{n\to\infty}E_{n} &= \bigcup_{k = 1}^{\infty}E_{k} \cap \bigcup_{k = 2}^{\infty}E_{k} \cap \dots \\
&= \bigcap_{n = 1}^{\infty}\bigcup_{k = n}^{\infty}E_{k}
\end{align}

\subsection{Statement of Lemma in Probability Spaces}

For a sequence of events $E_{1}, E_{2}, \dots$ in a probability space, if the sum of probabilities is finite
\begin{equation}
\sum_{n = 1}^{\infty}\mathbb{P}\left(E_{n}\right) < \infty
\end{equation}
then the probability the event occurs infinitely often is zero:
\begin{equation}
\mathbb{P}\left(\limsup_{n\to\infty}E_{n}\right) = 0
\end{equation}
\begin{proof}
Since $\sum_{n = 1}^{\infty}\mathbb{P}\left(E_{n}\right) < \infty$, then the series of probabilities converges, meaning $\sum_{n = N}^{\infty}\mathbb{P}\left(E_{n}\right) \to 0$ as $N \to \infty$. Hence taking the greatest lower bound of the series gives
\begin{equation}
\inf_{N \geq 1}\sum_{n = N}^{\infty}\mathbb{P}\left(E_{n}\right) = 0
\end{equation}
Evaluating $\mathbb{P}\left(\limsup_{n\to\infty}E_{n}\right)$ yields
\begin{equation}
\mathbb{P}\left(\limsup_{n\to\infty}E_{n}\right) = \mathbb{P}\left(\bigcap_{N = 1}^{\infty}\bigcup_{n = N}^{\infty}E_{n}\right)
\end{equation}
Note that by the chain rule of probability, $\operatorname{Pr}\left(A \cap B\right) = \operatorname{Pr}\left(B \middle| A\right)\operatorname{Pr}\left(A\right) \leq \operatorname{Pr}\left(A\right)$ so we can use the generalisation this argument to state
\begin{align}
\mathbb{P}\left(\bigcap_{N = 1}^{\infty}\bigcup_{n = N}^{\infty}E_{n}\right) &\leq \inf_{N \geq 1}\mathbb{P}\left(\bigcup_{n = N}^{\infty}E_{n}\right) \\
&\leq \inf_{N \geq 1}\sum_{n = N}^{\infty}\mathbb{P}\left(E_{n}\right) = 0
\end{align}
where the latter inequality comes from the generalisation of the concept $\operatorname{Pr}\left(A \cup B\right) \leq \operatorname{Pr}\left(A\right) + \operatorname{Pr}\left(B\right)$ (this is called Boole's inequality). Hence $\mathbb{P}\left(\limsup_{n\to\infty}E_{n}\right) = 0$
\end{proof}
An alternative proof is provided:
\begin{proof}
Let $\mathbb{I}_{n}$ denote the indicator function for the event $E_{n}$. Then by the linearity of expectation
\begin{align}
\mathbb{E}\left[\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right] &= \sum_{n = 1}^{\infty}\mathbb{E}\left[\mathbb{I}_{n}\right] \\
&= \sum_{n = 1}^{\infty}\mathbb{P}\left(E_{n}\right) < \infty
\end{align}
Suppose there is a non-zero probability that $E_{n}$ occurs infinitely often, meaning $\mathbb{P}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty\right) > 0$. However if we attempt to find $\mathbb{E}\left[\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right]$ by Lebesgue integration this gives
\begin{align}
\mathbb{E}\left[\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right] &= \int_{\left\{\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty\right\}}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right)\mathrm{d}\mathbb{P} + \int_{\left\{\sum_{n = 1}^{\infty}\mathbb{I}_{n} \neq \infty\right\}}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right)\mathrm{d}\mathbb{P} \\
&\geq \int_{\left\{\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty\right\}}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right)\mathrm{d}\mathbb{P}
\end{align}
Since there is a non-zero probability that $\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty$, then the integral is infinite, ie.
\begin{equation}
\int_{\left\{\sum_{n = 1}^{\infty}\mathbb{I}_{n} = \infty\right\}}\left(\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right)\mathrm{d}\mathbb{P} = \infty
\end{equation}
This results in $\mathbb{E}\left[\sum_{n = 1}^{\infty}\mathbb{I}_{n}\right] \geq \infty$, which is a contradiction.
\end{proof}

\subsection{Converse Borel-Cantelli Lemma}

We have the converse result that if the events $E_{1}, E_{2}, \dots$ are independent and the sum of the probabilities diverges to infinity, ie.
\begin{equation}
\sum_{n = 1}^{\infty}\mathbb{P}\left(E_{n}\right) = \infty
\end{equation}
then then probability that $E_{n}$ occurs infinitely many times is $1$, ie.
\begin{equation}
\mathbb{P}\left(\limsup_{n \to \infty}E_{n}\right) = 1
\end{equation}
\begin{proof}
We can show that there is zero probability that $E_{n}$ will occur a finite amount of times, ie.
\begin{equation}
1 - \mathbb{P}\left(\limsup_{n \to \infty}E_{n}\right) = 0
\end{equation}
This probability can be rewritten as
\begin{equation}
1 - \mathbb{P}\left(\limsup_{n \to \infty}E_{n}\right) = \lim_{N \to \infty}\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right)
\end{equation}
where $\overline{E}_{n}$ is the complement of $E_{n}$. Intuitively, this expresses the probability that there is some integer large enough such that any subsequent $E_{n}$ can no longer occur. Since the $E_{n}$ are independent, we can show
\begin{align}
\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right) &= \prod_{n = N}^{\infty}\mathbb{P}\left(\overline{E}_{n}\right) \\
&= \prod_{n = N}^{\infty}\left(1 - \mathbb{P}\left(E_{n}\right)\right) \\
&\leq \prod_{n = N}^{\infty}\exp\left(-\mathbb{P}\left(E_{n}\right)\right)
\end{align}
where we have used the fact that $1 - x \leq e^{-x}$ for $x \geq 0$. Hence
\begin{align}
\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right) &\leq \prod_{n = N}^{\infty}\exp\left(-\mathbb{P}\left(E_{n}\right)\right) \\
&\leq \exp\left(-\sum_{n = N}^{\infty}\mathbb{P}\left(E_{n}\right)\right) \\
&\leq 0 \\
\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right) &= 0
\end{align}
Therefore $1 - \mathbb{P}\left(\limsup_{n \to \infty}E_{n}\right) = \lim_{N \to \infty}\mathbb{P}\left(\bigcap_{n = N}^{\infty}\overline{E}_{n}\right) = 0$, completing the proof.
\end{proof}

\section{Dominated Convergence Theorem}

\section{Radon-Nikodym Theorem}

\subsection{Smoothing Law}

\section{Karhunen-Lo\`eve Theorem}


\chapter{Advanced Statistics}

\section{Copulas}

A copula is a multivariate probability distribution on support the unit hypercube, with uniform marginal distributions. 	

\begin{figure}[H]
\includegraphics[width=0.7\textwidth]{figures/copula}\centering
\caption{A copula in two dimensions. From this it is easier to see that the marginals will be uniform.}
\end{figure}

An example of a copula is the Frank distribution, with cumulative distribution function
\begin{equation}
H\left(x, y; \psi\right) = \begin{cases}
\begin{array}{c}
\dfrac{1}{\psi}\log\left(1+\dfrac{\left(e^{\psi x}-1\right)\left(e^{\phi y}-1\right)}{e^{\psi}-1}\right)\\
xy,
\end{array} & \begin{array}{c}
\psi\neq0\\
\psi=0
\end{array}\end{cases}
\end{equation}
where $\psi$ is a parameter which determines the dependence structure of the copula. We can verify that the following properties hold for this distribution: $H\left(0, 0; \psi\right) = 0$, $H\left(1, 1; \psi\right) = 1$. Additionally, we can verify that $H\left(1, y; \psi\right) = y$, $H\left(x, 1; \psi\right) = x$ which are the cumulative distribution functions of the uniform marginal distributions. \\

The usefulness of copulas in statistics starts with the property that for some continuous random variable $X$ with cumulative distribution function $F_{X}\left(x\right)$, then the transformed random variable $F_{X}\left(X\right)$ will be uniformly distributed on $\left[0, 1\right]$. The reason for this is related to the inverse transform sampling theorem. Thus if we observe some data and suspect that their marginal distributions follow some distributions (eg. $F_{X}\left(x\right)$ and $F_{Y}\left(y\right)$), then we can fit the data to a copula. \\

For example with the Frank copula, we can fit it to a distribution of the form $H\left(F_{X}\left(x\right), F_{Y}\left(y\right); \psi\right)$. Since by construction the random variables $F_{X}\left(X\right)$ and $F_{Y}\left(Y\right)$ have uniform marginals, then the random variables $X$ and $Y$ have cumulative distribution functions $F_{X}\left(x\right)$ and $F_{Y}\left(y\right)$ respectively. The parameter $\psi$ can then be used to model the dependencies of the variables based on what is observed in the data. The maximum likelihood technique is typically used for estimating the parameters of the copula, which can be jointly estimated with any parameters of the marginal distributions.

\section{Asymptotic Theory}

\section{Resampling Methods}

\subsection{Jackknife}

Given a sample of size $n$, the Jackknife method for an estimator involves aggregating the estimates for each size $n - 1$ subsample. Suppose the parameter to be estimated is the population mean. Formally, the Jackknife estimate involves first taking $n$ sample means with each observation removed:
\begin{equation}
\bar{x}_{i} = \dfrac{1}{n - 1}\sum_{j = 1, j\neq i}^{n}x_{j}
\end{equation}
for $i = 1, \dots, n$. The Jackknife estimate of the population mean is then the mean of all the subsample means:
\begin{equation}
\hat{\theta} = \dfrac{1}{n}\sum_{i = 1}^{n}\bar{x}_{i}
\end{equation}
The Jackknife estimate of the variance of the estimator can also be calculated using the distribution of $\bar{x}_{i}$ as follows:
\begin{equation}
\hat{\operatorname{Var}}\left(\hat{\theta}\right) = \dfrac{n - 1}{n}\sum_{i = 1}^{n}\left(\bar{x}_{i} - \hat{\theta}\right)^{2}
\end{equation}
This estimator is an unbiased estimator of the variance of the sample mean.
\begin{proof}
Notice that $\bar{x}_{i} = \dfrac{n\bar{x} - x_{i}}{n - 1} \Rightarrow \left(n - 1\right)\bar{x}_{i} = n\bar{x} - x_{i}$ where $\bar{x}$ is the sample mean. The term $\bar{x}_{i} - \hat{\theta}$ can be manipulated to become
\begin{align}
\bar{x}_{i} - \hat{\theta} &= \dfrac{n\bar{x} - x_{i}}{n - 1} - \dfrac{1}{n}\sum_{i = 1}^{n}\bar{x}_{i} \\
&= \dfrac{1}{n - 1}\left(n\bar{x} - x_{i} - \dfrac{1}{n}\sum_{i = 1}^{n}\left(n - 1\right)\bar{x}_{i}\right) \\
&= \dfrac{1}{n - 1}\left(n\bar{x} - x_{i} - \dfrac{1}{n}\sum_{i = 1}^{n}\left(n\bar{x} - x_{i}\right)\right) \\
&= \dfrac{1}{n - 1}\left(n\bar{x} - x_{i} - n\bar{x} + \dfrac{1}{n}\sum_{i = 1}^{n}x_{i}\right) \\
&= \dfrac{1}{n - 1}\left(\bar{x} - x_{i}\right) \\
\end{align}
Hence
\begin{align}
\hat{\operatorname{Var}}\left(\hat{\theta}\right) &= \dfrac{n - 1}{n}\sum_{i = 1}^{n}\left(\bar{x}_{i} - \hat{\theta}\right)^{2} \\
&= \dfrac{1}{n\left(n - 1\right)}\sum_{i = 1}^{n}\left(\bar{x} - x_{i}\right)^{2}
\end{align}
Recall that $\dfrac{1}{\left(n - 1\right)}\sum_{i = 1}^{n}\left(\bar{x} - x_{i}\right)^{2}$ is an unbiased estimator of the population variance, and so $\dfrac{1}{n\left(n - 1\right)}\sum_{i = 1}^{n}\left(\bar{x} - x_{i}\right)^{2}$ is an unbiased estimator of the variance of the sample mean.
\end{proof}

\subsection{Bootstrap}



\section{Gauss-Markov Theorem}

\subsection{Cramer-Rao Bound}

\section{Principal Component Analysis}

\subsection{Principal Component Regression}

\section{Survival Analysis}

\section{Rao-Blackwell Estimators}

\chapter{Stochastic Calculus}

\section{Martingales}

\section{It\^{o} Calculus}

\subsection{It\^{o} Integral}

\section{Kolmogorov-Chentsov Continuity Theorem}

\section{Stochastic Differential Equations}

\chapter{Combinatorial Probability}

\section{Inclusion-Exclusion Principle}

\part{Applications}

\chapter{Information Theory}

\section{Entropy}

For a discrete random variable $X$ with probability mass function $P\left(x\right)$, the self-information of event $\left\{X = x\right\}$ is defined as
\begin{equation}
\operatorname{I}\left(x\right) = \log\dfrac{1}{P\left(x\right)}
\end{equation}
The `information entropy' of a discrete random variable $X$ with probability mass function $P\left(x\right)$ is defined as the expected self-information:
\begin{align}
\operatorname{H}\left[X\right] &= \operatorname{E}\left[-\log P\left(X\right)\right] \\
&= -\sum_{i = 1}^{n}P\left(x_{i}\right)\log P\left(x_{i}\right) \\
&= \sum_{i = 1}^{n}P\left(x_{i}\right)\log \dfrac{1}{P\left(x_{i}\right)}
\end{align}
Note that since $0 \leq P\left(x_{i}\right) \leq 1$, then entropy is non-negative (and when $P\left(x_{i}\right) \rightarrow 0$, the term in the summation approaches zero). If the log used is base 2, then the units of entropy is measured in bits. If the log is the natural log, then the units are in `nats'. There are various interpretations of entropy.
\begin{itemize}
\item Entropy can be thought of as a measure of uncertainty (in a separate way to variance). The larger the entropy, the more uncertainty in the random variable. A deterministic variable is the most certain kind of random variable, and has an entropy of zero (minimum uncertainty) since $\log 1 = 0$.
\item Entropy measured in bits can be thought of as the lower bound on the number of bits it requires to transmit/store the outcome of an experiment. For example, in the deterministic case we require no bits because we already know the outcome of all experiments. Consider the outcome of a fair coin toss, with probability mass function
\begin{equation}
P\left(X = x\right) = \begin{cases}
\begin{array}{c}
0.5,\\
0.5,\\
0,
\end{array} & \begin{array}{c}
x=0\\
x=1\\
\text{elsewhere}
\end{array}\end{cases}
\end{equation}
The calculation of entropy yields $\operatorname{H}\left[X\right] = 1$ bit. This agrees with the intuition that it requires a minimum of 1 bit to convey the outcome of a fair coin toss (ie. 0 for tails, 1 for heads). In the case of an unfair coin, for example with the probability mass function
\begin{equation}
P\left(X = x\right) = \begin{cases}
\begin{array}{c}
0.25,\\
0.75,\\
0,
\end{array} & \begin{array}{c}
x=0\\
x=1\\
\text{elsewhere}
\end{array}\end{cases}
\end{equation}
The calculation of entropy for this distribution yields $\operatorname{H}\left[X\right] = 0.8113$ bits. This is a lower bound, so rounded up gives 1 bit. We still require at least 1 bit to convey the outcome of an unfair coin toss because the outcome is still binary (either we get heads or we don't). \\

Now imagine an experiment with four possible outcomes distributed uniformly (eg. two fair coin tosses). The calculation of entropy does not depend on the events, only the associated probabilities. So in the case the probabilities are $\left\{0.25, 0.25, 0.25, 0.25\right\}$, the entropy works out to be $2$ bits. Once again this agrees with the intuition that it would take a minimum of $2$ bits to store the outcome of two fair coin tosses.
\item Lastly, entropy can also be thought of as the expected value of a random variable that expresses the amount of information contained in each event (the random variable in question being $-\log P\left(X\right)$). To illustrate, consider the random variable with distribution
\begin{equation}
P\left(X = x\right) = \begin{cases}
\begin{array}{c}
0.05,\\
0.95,\\
0,
\end{array} & \begin{array}{c}
x=0\\
x=1\\
\text{elsewhere}
\end{array}\end{cases}
\end{equation}
We regard the occurrence of the event $X = 0$ as containing more information than the occurrence of the event $X = 1$, since it is rarer and hence more of a `surprise' (the occurrence of $X = 0$ says more about the experiment than the occurrence $X = 1$). This is reflected in $\log\dfrac{1}{0.05} > \log\dfrac{1}{0.95}$.
\end{itemize}

\subsection{Joint Entropy}

For a pair of discrete random variables $X$ and $Y$ with joint probability mass function $p\left(x, y\right)$, the joint entropy is defined as
\begin{align}
\operatorname{H}\left[X, Y\right] &= -\mathbb{E}\left[\log p\left(X, Y\right)\right] \\
&= \sum_{i}\sum_{j}p\left(i, j\right)\log\dfrac{1}{p\left(i, j\right)}
\end{align}

\subsection{Conditional Entropy}

For a pair of discrete random variables $X$ and $Y$ with joint probability mass function $p\left(x, y\right)$ and conditional distribution $p\left(y\middle|x\right)$, the conditional entropy is defined as
\begin{align}
\operatorname{H}\left[Y\middle|X\right] &= - \mathbb{E}\left[p\left(Y\middle|X\right)\right] \\
&= \sum_{i}\sum_{j}p\left(i, j\right)\log\dfrac{1}{p\left(j\middle|i\right)}
\end{align}

\subsection{Chain Rule for Entropy}

The chain rule for entropy says that
\begin{equation}
\operatorname{H}\left[X, Y\right] = \operatorname{H}\left[X\right]+ \operatorname{H}\left[Y\middle|X\right]
\end{equation}
\begin{proof}
\begin{align}
\operatorname{H}\left[X, Y\right] &= \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{1}{p\left(x, y\right)} \\
&= \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{1}{p\left(y\middle|x\right)p\left(x\right)} \\
&= -\sum_{x}\sum_{x}p\left(x, y\right)\log p\left(x\right) -\sum_{x}\sum_{y}p\left(x, y\right)\log p\left(y\middle|x\right) \\
&= -\sum_{x}p\left(x\right)\log p\left(x\right) -\sum_{x}\sum_{y}p\left(x, y\right)\log p\left(y\middle|x\right) \\
&= \operatorname{H}\left[X\right]+ \operatorname{H}\left[Y\middle|X\right]
\end{align}
\end{proof}
A corollary is that if $X$ and $Y$ are independent,
\begin{equation}
\operatorname{H}\left[X, Y\right] = \operatorname{H}\left[X\right]+ \operatorname{H}\left[Y\right]
\end{equation}
and if $X$ and $Y$ are independent and identical,
\begin{equation}
\operatorname{H}\left[X, Y\right] = 2\operatorname{H}\left[X\right]
\end{equation}

\subsection{Cross Entropy}

For two probability mass functions $p\left(x_{i}\right)$ and $q\left(x_{i}\right)$, where $q\left(x_{i}\right)$ is the approximating distribution of $p\left(x_{i}\right)$, the cross entropy between $p$ and $q$ is defined as
\begin{align}
\operatorname{H}_{p, q} &= -\mathbb{E}\left[\log q\left(x_{i}\right)\right]\\
&= \sum_{i}p\left(x_{i}\right)\log\dfrac{1}{q\left(x_{i}\right)}
\end{align}
Similar properties of joint entropies apply to cross entropies of multivariate distributions. Suppose we have multivariate distributions $p\left(x_{1}, \dots, x_{n}\right)$ and the approximating distribution $q\left(x_{1}, \dots, x_{n}\right)$ (note the slight abuse in notation from before). Then the cross entropy is
\begin{equation}
\operatorname{H}_{p_{1:n}, q_{1:n}} = \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}, \dots, x_{n}\right)\log\dfrac{1}{q\left(x_{1}, \dots, x_{n}\right)}
\end{equation}
If $X_{1}, \dots, X_{n}$ are independent (subsequently this also implies $q\left(x_{1}, \dots, x_{n}\right) = q\left(x_{1}\right)\dots q\left(x_{n}\right)$ if the knowledge of independence is also incorporated into the approximating distribution), then
\begin{align}
\operatorname{H}_{p_{1:n}, q_{1:n}} &= \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}\right)\dots p\left(x_{n}\right)\log\dfrac{1}{q\left(x_{1}\right)\dots q\left(x_{n}\right)} \\
&= - \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}\right)\dots p\left(x_{n}\right)\log q\left(x_{1}\right)\dots q\left(x_{n}\right) \\
&= - \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}\right)\dots p\left(x_{n}\right)\log q\left(x_{1}\right) - \dots - \sum_{x_{1}}\cdots\sum_{x_{n}}p\left(x_{1}\right)\dots p\left(x_{n}\right)\log q\left(x_{n}\right) \\
&= - \sum_{x_{1}}p\left(x_{1}\right)\log q\left(x_{1}\right) - \dots - \sum_{x_{n}}p\left(x_{n}\right)\log q\left(x_{n}\right) \\
&= \sum_{i = 1}^{n}\sum_{x_{i}}p\left(x_{i}\right)\log\dfrac{1}{q\left(x_{i}\right)} \\
&= \sum_{i = 1}^{n}\operatorname{H}_{p_{i},q_{i}}
\end{align}

\subsection{Entropy Rate}

Let $\mathcal{X} = \left\{X_{1}, X_{2}, \dots, X_{n}\right\}$ be a random sequence. Then the entropy rate is defined as
\begin{equation}
\operatorname{H}\left[\mathcal{X}\right] = \lim_{n\to\infty}\dfrac{1}{n}\operatorname{H}\left[X_{1}, X_{2}, \dots, X_{n}\right]
\end{equation}
if the limit exists. If the sequence is i.i.d., then the entropy rate is
\begin{equation}
\operatorname{H}\left[\mathcal{X}\right] = \lim_{n\to\infty}\dfrac{1}{n}\operatorname{H}\left[X_{1}, X_{2}, \dots, X_{n}\right] = \lim_{n\to\infty}\dfrac{n\operatorname{H}\left[X\right]}{n} = \operatorname{H}\left[X\right]
\end{equation}

\subsection{Differential Entropy}

The continuous analogue of information entropy is differential entropy, where the discrete sum is converted to an integral over the probability density function $f\left(x\right)$.
\begin{equation}
\operatorname{H}\left[X\right] = -\int f\left(x\right)\log f\left(x\right) dx
\end{equation}
Note however that this measure of entropy does not share all the same properties as information entropy defined above. One such property is non-negativity. Since $f\left(x\right)$ may be greater than 1, then it is possible for $-\log f\left(x\right) < 0$. Despite this, we can still use differential entropy as a measure of uncertainty.

\subsection{Asymptotic Equipartition Property}

The asymptotic equipartition property is the analogue of the weak law of large numbers for entropy. If $X_{1}, \dots, X_{n}$ are i.i.d. with joint probability mass function $p\left(X_{1}, \dots, X_{n}\right)$, then
\begin{equation}
\dfrac{1}{n}\log\dfrac{1}{p\left(X_{1}, \dots, X_{n}\right)} \overset{p}{\to} \operatorname{H}\left[X\right]
\end{equation}
as $n\to\infty$.
\begin{proof}
Because of i.i.d., we can write
\begin{equation}
\dfrac{1}{n}\log\dfrac{1}{p\left(X_{1}, \dots, X_{n}\right)} = -\dfrac{1}{n}\sum_{i = 1}^{n}\log p\left(X\right)
\end{equation}
which converges in probability to $-\mathbb{E}\left[\log p\left(X\right)\right] = \operatorname{H}\left[X\right]$ due to the weak law of large numbers.
\end{proof}
The asymptotic equipartition property also holds for continuous random variables and differential entropy. If $X_{1}, \dots, X_{n}$ are i.i.d. with joint probability density function $f\left(X_{1}, \dots, X_{n}\right)$, then
\begin{equation}
\dfrac{1}{n}\log\dfrac{1}{f\left(X_{1}, \dots, X_{n}\right)} \overset{p}{\to} \operatorname{H}\left[X\right]
\end{equation}
as $n\to\infty$.

\section{Kullback-Leibler Divergence}

The Kullback-Leibler (KL) Divergence is a measure of relative entropy between two distributions, which roughly speaking gives a measure of the amount of information lost when approximating one distribution with the other distribution. For discrete probability distributions $P\left(x\right)$ and $Q\left(x\right)$, the Kullback-Leibler Divergence from $Q$ to $P$ is defined as
\begin{align}
\operatorname{KL}\left(P\|Q\right) &= -\sum_{i}P\left(x_{i}\right)\log\dfrac{Q\left(x_{i}\right)}{P\left(x_{i}\right)} \\
&= \sum_{i}P\left(x_{i}\right)\log\dfrac{P\left(x_{i}\right)}{Q\left(x_{i}\right)} \\
&= \sum_{i}P\left(x_{i}\right)\left[\log P\left(x_{i}\right) - \log Q\left(x_{i}\right)\right]
\end{align}
Here, $Q$ is treated as the approximating distribution and $P$ is the `true' distribution. The KL divergence is finite only for when $Q\left(x_{i}\right) = 0$ implies $P\left(x_{i}\right) = 0$ for all $i$. If there exists an $i$ for which $Q\left(x_{i}\right) = 0$ and $P\left(x_{i}\right) \geq 0$, we take $\operatorname{KL}\left(P\|Q\right) = \infty$ \cite{Cover2006}. A property of the KL divergence is always non-negative, ie. $\operatorname{KL}\left(P\|Q\right) \geq 0$. This is known as Gibb's inequality.
\begin{proof}
\begin{align}
\operatorname{KL}\left(P\|Q\right) &= \sum_{P\left(x_{i}\right) > 0}P\left(x_{i}\right)\log\dfrac{P\left(x_{i}\right)}{Q\left(x_{i}\right)} \\
&= -\sum_{P\left(x_{i}\right) > 0}P\left(x_{i}\right)\log\dfrac{Q\left(x_{i}\right)}{P\left(x_{i}\right)}
\end{align}
As $-\log$ is a convex function, then using Jensen's inequality
\begin{equation}
\operatorname{KL}\left(P\|Q\right) \geq -\log\sum_{P\left(x_{i}\right) > 0}P\left(x_{i}\right)\dfrac{Q\left(x_{i}\right)}{P\left(x_{i}\right)} = -\log\sum_{P\left(x_{i}\right) > 0}Q\left(x_{i}\right)
\end{equation}
Or
\begin{equation}
-\operatorname{KL}\left(P\|Q\right) \leq \log\sum_{P\left(x_{i}\right) > 0}Q\left(x_{i}\right)
\end{equation}
Since $\sum Q\left(x_{i}\right) \leq 1$, then
\begin{equation}
-\operatorname{KL}\left(P\|Q\right) \leq \log 1 = 0
\end{equation}
Hence
\begin{equation}
\operatorname{KL}\left(P\|Q\right) \geq 0
\end{equation}
\end{proof}
Intuitively, there is always information loss when approximating one distribution with another distribution, with only no information loss occurring ($\operatorname{KL}\left(P\|Q\right) = 0$ when the distributions of $P$ and $Q$ are identical). Also note that in general, $\operatorname{KL}\left(P\|Q\right) \neq \operatorname{KL}\left(Q\|P\right)$. An alternative way to write the KL divergence is as
\begin{align}
\operatorname{KL}\left(P\|Q\right) &= \sum_{i}P\left(x_{i}\right)\log P\left(x_{i}\right) - \sum_{i}P\left(x_{i}\right)\log Q\left(x_{i}\right) \\
&= \left[- \sum_{i}P\left(x_{i}\right)\log Q\left(x_{i}\right)\right] - \left[-\sum_{i}P\left(x_{i}\right)\log P\left(x_{i}\right)\right] \\
&= \operatorname{H}_{P, Q} - \operatorname{H}_{P}
\end{align}
where $\operatorname{H}_{P, Q}$ is the cross entropy of $P$ and $Q$, and $\operatorname{H}_{P}$ is the entropy of $P$. \\

The KL divergence is also easily generalised to multivariate distributions; consider the joint pmf $p\left(x, y\right)$ and the approximating joint pmf $q\left(x, y\right)$.
\begin{equation}
\operatorname{KL}\left(p\middle\Vert q\right) = \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{p\left(x, y\right)}{q\left(x, y\right)}
\end{equation}

The continuous analogue of KL divergence for probability density functions $p$ and $q$ is
\begin{equation}
\operatorname{KL}\left(p\|q\right) = \int p\left(x\right)\log\dfrac{p\left(x\right)}{q\left(x\right)}dx
\end{equation}

\subsection{Mutual Information}

Consider two discrete random variables $X$, $Y$ with joint probability mass function $p\left(x, y\right)$ and marginal probability mass functions $p\left(x\right)$ and $p\left(y\right)$ respectively. The mutual information $\operatorname{MI}\left(X;Y\right)$ is the KL divergence between the joint distribution and the approximating distribution $q\left(x, y\right) = p\left(x\right)p\left(y\right)$.
\begin{equation}
\operatorname{MI}\left(X;Y\right) = \sum_{x}\sum_{y}p\left(x, y\right)\log\dfrac{p\left(x, y\right)}{p\left(x\right)p\left(y\right)}
\end{equation}
We can see that if $X$ and $Y$ are independent, ie. $p\left(x, y\right) = p\left(x\right)p\left(y\right)$, then the mutual information will be zero. In that sense, we can think of mutual information as information loss by assuming independence of random variables.

\subsection{Asymptotic Equipartition Property for the KL Divergence}

A version of the asymptotic equipartition property exists for the KL divergence, which can be proved in a similar fashion. Let $X_{1}, \dots, X_{n}$ be i.i.d. with joint probability mass function $p\left(X_{1}, \dots, X_{n}\right)$. Let $q\left(X_{1}, \dots, X_{n}\right)$ be any other probability mass function on the support of $X$ (which plays the part of the approximating distribution). Then
\begin{equation}
\dfrac{1}{n}\log\dfrac{p\left(X_{1}, \dots, X_{n}\right)}{q\left(X_{1}, \dots, X_{n}\right)} \overset{p}{\to} \operatorname{KL}\left(p\middle\Vert q\right)
\end{equation}
as $n\to\infty$.

\subsection{Equivalence Between Minimum KL Divergence and MLE}

We show that finding a parameter which minimises the KL divergence between the parametrised distribution and a empirical distribution is equivalent to finding the maximum likelihood estimate. This is shown for the case where the family of distributions is continuous, by the result analogously holds for families of discrete distributions. Suppose some i.i.d. data $x_{1}, \dots, x_{n}$ is collected, resulting in an empirical distribution function with density function $\tilde{p}\left(x\right)$.
\begin{equation}
\tilde{p}\left(x\right) = \sum_{i = 1}^{n}\dfrac{1}{n}\delta\left(x - x_{i}\right)
\end{equation}
where $\delta\left(\cdot\right)$ is the Dirac distribution (ie. the empirical density is just a continuous representation using impulses of the empirical mass function). Suppose there is a family of models with probability density $q\left(x;\theta\right)$ on support $\mathcal{X}$ (which is implicitly assumed to be a superset of all the data), parametrised by $\theta$. We seek to minimise the KL divergence between the empirical density function $\tilde{p}\left(x\right)$ and the approximating distribution $q\left(x;\theta\right)$. By definition of the KL divergence,
\begin{equation}
\operatorname{KL}\left(\tilde{p}\left(x\right)\middle\Vert q\left(x;\theta\right)\right) = \mathbb{E}\left[\log\dfrac{\tilde{p}\left(x\right)}{q\left(x;\theta\right)}\right]
\end{equation}
where it is understood that the expectation is taken over the empirical density. Rewriting the KL divergence in terms of cross entropy and entropy,
\begin{equation}
\operatorname{KL}\left(\tilde{p}\left(x\right)\middle\Vert q\left(x;\theta\right)\right) = -\mathbb{E}\left[\log q\left(x;\theta\right)\right] + \mathbb{E}\left[\log \tilde{p}\left(x\right)\right]
\end{equation}
Note that the entropy of $\tilde{p}\left(x\right)$ is unaffected by $\theta$. Evaluating the (negative) cross-entropy term $\mathbb{E}\left[\log q\left(x;\theta\right)\right]$:
\begin{align}
\mathbb{E}\left[\log q\left(x;\theta\right)\right] &= \int_{\mathcal{X}}\tilde{p}\left(x\right)\log q\left(x;\theta\right)dx \\
&= \int_{\mathcal{X}}\sum_{i = 1}^{n}\dfrac{1}{n}\delta\left(x - x_{i}\right)\log q\left(x;\theta\right)dx \\
&= \dfrac{1}{n}\sum_{i = 1}^{n}\int_{\mathcal{X}}\delta\left(x - x_{i}\right)\log q\left(x;\theta\right)dx \\
&= \dfrac{1}{n}\sum_{i = 1}^{n}\log q\left(x_{i};\theta\right)
\end{align}
Hence
\begin{equation}
\argmin_{\theta}\operatorname{KL}\left(\tilde{p}\left(x\right)\middle\Vert q\left(x;\theta\right)\right) = \argmin_{\theta}\left\{-\sum_{i = 1}^{n}\log q\left(x_{i};\theta\right)\right\}
\end{equation}
which is the minimiser of the negative log likelihood when the data is i.i.d.

\subsection{Symmetrised KL Divergence}

The symmetrised KL divergence is defined as
\begin{equation}
\operatorname{KL}_{\mathrm{sym}}\left(P\middle\|Q\right) = \operatorname{KL}\left(P\middle\|Q\right) + \operatorname{KL}\left(Q\middle\|P\right)
\end{equation}
where it attains the property that $\operatorname{KL}_{\mathrm{sym}}\left(P\middle\|Q\right) = \operatorname{KL}_{\mathrm{sym}}\left(Q\middle\|P\right)$.

\section{Maximum Entropy Distributions}

\subsection{Principle of Maximum Entropy}

The principle of maximum entropy roughly states that, given limited information about a prior, then the best choice of prior distribution (out of all possible priors which satisfy the limited information given) is the distribution with maximum entropy. Intuitively, this distribution makes the least number of assumptions about the variable in question.

\subsection{Maximum Entropy Distributions on Bounded Support}
For any discrete distribution on support $\left\{a, a + 1, \dots, b - 1, b\right\}$, the discrete uniform distribution has the maximum entropy. For any continuous distribution on support $\left[a, b\right]$, the continuous uniform distribution has the maximum entropy. This agrees with the intuition that the uniform distributions contain the most amount of `surprise' (ie. everything is equally likely0.

\subsection{Maximum Entropy Distributions on Unbounded Support}

For a prescribed mean, the exponential distribution has the maximum entropy among all continuous distributions supported on $\left[0, \infty\right)$. \\

For a prescribed mean and variance, the Gaussian distribution has the maximum entropy among all continuous distributions supported on $\left(-\infty, \infty\right)$. \\

For a prescribed mean, variance, skewness and kurtosis, the maximum entropy distribution among continuous distributions supported on $\left(-\infty, \infty\right)$ takes the form
\begin{equation}
f_{X}\left(x\right) \propto \exp\left(ax + bx^{2} + cx^{3} + dx^{4}\right)
\end{equation}
However there may be no solution (if the skewness and kurtosis lie in certain regions) and the solution (if it exists) can be a bimodal distribution \cite{Rockinger2002}.

\section{Coding Theory \cite{Cover2006}}

Suppose there is a random variable $X$ with discrete support $\mathcal{X}$. A \textit{source code} for $X$ is a mapping from $\mathcal{X}$ to a set $\mathcal{D}$ of finite length strings using symbols from an alphabet of length $D$ (called a $D$-ary alphabet). Each of these strings is called a codeword. A binary alphabet with symbols 0 and 1 is a case of $D = 2$. \\

A (source) code is said to be \textit{nonsingular} if every element in $\mathcal{X}$ maps uniquely to $\mathcal{D}$. The \textit{extension} of a code is the mapping from sequences of elements from $\mathcal{X}$ to concatenated strings from $\mathcal{D}$ corresponding to the sequence. A code is \textit{uniquely decodable} if its extension is non-singular (that is, it is enough to determine the sequence of events just by looking at the concatenated string). An example of uniquely decodable codes are \textit{prefix codes}, whereby no codeword is a prefix of any other codeword. Prefix codes can be decoded instantaneously, ie. without needing to look at the future sequence of codewords. \\

Based the distribution of $X$, denoted $p\left(x_{i}\right)$, an optimal code can be designed which minimises the expected codeword length. Denote these optimal lengths $\ell_{i}^{*}$. The entropy of $X$ (using log base $D$) gives a lower bound on the expected codeword length of the optimal prefix code using a $D$-ary alphabet \cite{Cover2006}.
\begin{equation}
\operatorname{H}\left[X\right] \leq \sum_{i}p\left(x_{i}\right)\ell_{i}^{*}
\end{equation}
Suppose however an optimal code is designed using an assumed/approximating distribution $q\left(x_{i}\right)$, yielding lengths $l_{i}^{*}$. Then the cross entropy between $p\left(x_{i}\right)$ and $q\left(x_{i}\right)$ gives the lower bound on actual expected codeword length of the optimal prefix code.
\begin{equation}
\operatorname{H}_{p, q} \leq \sum_{i}p\left(x_{i}\right)l_{i}^{*}
\end{equation}

\section{Likelihood Criteria}

\subsection{Fischer Information Metric}

\subsection{Akaike Information Criterion}

The AIC of a model parametrised by $\theta$ can be written in terms of its log likelihood $\log\mathcal{L}\left(\theta\right)$.
\begin{equation}
\operatorname{AIC}\left(\theta\right) = 2\max_{\theta}\left\{\log\mathcal{L}\left(\theta\right)\right\} - 2\dim\left(\theta\right)
\end{equation}
where $\dim\left(\theta\right)$ is the length of the parameter vector $\theta$. The AIC aims to trade off between a high likelihood and low model complexity (measured by the number of parameters) by introducing a penalty on the number of parameters, since too high model complexity may lead to overfitting or loss in interpretability of the model. The AIC can be used for model selection, where we select the model with the highest AIC. There also exist formulae for corrected AIC based on sample size $n$ \cite{Claeskens2008}.

\subsection{Deviance Information Criterion}

\subsection{Bayesian Information Criterion}

\section{$f$-Divergence}

\subsection{Total Variation Distance}

\subsection{Hellinger Distance}

\chapter{Econometrics}

\section{Model Specification}

\subsection{Log Models}

Suppose we have a simple linear model of the form
\begin{equation}
\log y = \beta_{0} + \beta_{1}x + u
\end{equation}
where $u$ is the error term. Then $\beta_{1}\times 100\%$ may be interpreted as the percentage increase in $y$ for a 1 unit increase in $x$. To see this, first note $\dfrac{\partial \log y}{y} = \dfrac{1}{y}$ and $\dfrac{\partial \log y}{\partial x} = \beta_{1}$. This gives $\beta_{1} = \dfrac{\partial y}{\partial x}\cdot\dfrac{1}{y}$. So for a 1 unit increase in $x$ we have $\beta_{1} \approx \dfrac{\Delta y}{y}$ which is the relative change in $y$.

\subsection{Log-Log Models}

Given a log-log specification between two variables, eg.
\begin{equation}
\log y = \beta_{0} + \beta_{1}\log x + u
\end{equation}
we can interpret $\beta_{1}$ (or its estimate) as ratio of percentage changes. This is also known as the $y$-elasticity of $x$. That is, for a 1\% increase in $x$, there is a $\beta_{1}$ percent increase in $y$, holding all else constant. To see this, first write
\begin{equation}
\dfrac{\Delta y}{y} \div \dfrac{\Delta x}{x} = \beta_{1} 
\end{equation}
Then for small changes in $x$ and $y$
\begin{gather}
\dfrac{dy}{y} = \beta_{1}\dfrac{dx}{x} \\
\int\dfrac{1}{y}dy = \beta_{1}\int\dfrac{1}{x}dx \\
\log\left|y\right| = \beta_{1}\log\left|x\right|
\end{gather}
Note that this implicitly requires $x$ and $y$ to be positive variables.

\section{Instrumental Variables}

\section{Panel Data Regression}

\section{Time-Series Models}

\subsection{Autoregressive (AR) Models}
An autoregressive model of order $p$ is denoted $AR\left(p\right)$ and is defined by
\begin{equation}
X_{t} = c + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + \varepsilon_{t}
\end{equation}
where $\varphi_{1}, \dots, \varphi_{2}$ are parameters of the model, $c$ is a constant term and $\varepsilon_{t}$ is white noise.

\subsection{Moving Average (MA) Models}
A moving average model of order $q$ is denoted $MA\left(q\right)$ and is defined by
\begin{equation}
X_{t} = \mu + \varepsilon_{t} + \sum_{i = 1}^{q}\theta_{i}\varepsilon_{t - i}
\end{equation}
where $\mu$ is the mean of the series, $\theta_{1}, \dots, \theta_{q}$ are the parameters of the model and $\varepsilon_{t}, \dots, \varepsilon_{t - q}$ are white noise error terms.

\subsection{Autoregressive Moving Average (ARMA) Models}
An ARMA model is a generalisation of the autoregressive and moving average models, and is denoted by $ARMA\left(p, q\right)$. The model is defined by
\begin{equation}
X_{t} = c + \varepsilon_{t} + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + \sum_{i = 1}^{q}\theta_{i}\varepsilon_{t - i}
\end{equation}

\subsection{Autoregressive Integrated Moving Average (ARIMA) Models}
An ARIMA model is a generalisation of the ARMA model, and is denoted by $ARIMA\left(p, d, q\right)$. Denoting $L$ as the lag operator (ie. $L^{i}X_{t} = X_{t - i}$), the model is defined by
\begin{equation}
\left(1 - \sum_{i = 1}^{p}\varphi_{i}L^{i}\right)\left(1 - L\right)^{d}X_{t} = \left(1 + \sum_{i = 1}^{d}\theta_{i}L^{i}\right)\varepsilon_{t}
\end{equation}
Using the binomial expansion, we can write
\begin{align}
\left(1 - \sum_{i = 1}^{p}\varphi_{i}L^{i}\right)\left(1 - L\right)^{d} &= \left(1 - \sum_{i = 1}^{p}\varphi_{i}L^{i}\right)\sum_{j = 0}^{d} \begin{pmatrix}
d \\ j
\end{pmatrix} \left(-L\right)^{d - j}  \\
&= \sum_{j = 0}^{d} \begin{pmatrix}
d \\ j
\end{pmatrix} \left(-1\right)^{d - j}L^{d - j} - \sum_{i = 1}^{p}\sum_{j = 0}^{d}\varphi_{i}\begin{pmatrix}
d \\ j
\end{pmatrix}\left(-1\right)^{d- j}L^{d - j + i}
\end{align}
Hence an alternative specification of the model is
\begin{equation}
\sum_{j = 0}^{d} \begin{pmatrix}
d \\ j
\end{pmatrix} \left(-1\right)^{d - j}X_{t - d + j} - \sum_{i = 1}^{p}\sum_{j = 0}^{d}\varphi_{i}\begin{pmatrix}
d \\ j
\end{pmatrix}\left(-1\right)^{d- j}X_{t - d + j - i} = \varepsilon_{t} + \sum_{i = 1}^{d}\theta_{i}\varepsilon_{t - i}
\end{equation}

\subsection{Autoregressive Moving Average with Exogenous Inputs (ARMAX) Models}
An ARMAX model is a generalisation of the ARMA model, and is denoted by $ARMAX\left(p, q, b\right)$. The model is defined by
\begin{equation}
X_{t} = \varepsilon_{t} + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + \sum_{i = 1}^{q}\theta_{i}\varepsilon_{t - i} + \sum_{i = 1}^{b}\eta_{i}d_{t - i}
\end{equation}
where $\eta_{1}, \dots, \eta_{b}$ are parameters of the exogeneous input $d_{t}$.

\subsection{Autoregressive Conditional Heteroskedasticity (ARCH) Models}
An ARCH model is a generalisation of the AR model, and is denoted by $ARCH\left(q\right)$. The model defines for the error term
\begin{equation}
\varepsilon_{t} = \sigma_{t}z_{t}
\end{equation}
where $z_{t}$ is white noise and $\sigma^{2}_{t}$ is modelled by
\begin{equation}
\sigma^{2}_{t} = \omega + \sum_{i = 1}^{q}\alpha_{i}\varepsilon^{2}_{t - i}
\end{equation}
where $\omega > 0$ is a constant and $\alpha_{1}, \dots, \alpha_{q} > 0$ are parameters. Hence the full model is defined by
\begin{equation}
X_{t} = c + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + z_{t}\sqrt{\omega + \sum_{i = 1}^{q}\alpha_{i}\varepsilon^{2}_{t - i}}
\end{equation}

\subsection{Generalised Autoregressive Conditional Heteroskedasticity (GARCH) Models}
A GARCH model is a generalisation of the ARCH model, and is denoted by $GARCH\left(p, q\right)$. The term $\sigma^{2}_{t}$ is now modelled by
\begin{equation}
\sigma^{2}_{t} = \omega + \sum_{i = 1}^{q}\alpha_{i}\varepsilon^{2}_{t - i} + \sum_{i = 1}^{p}\beta_{i}\sigma^{2}_{i - 1}
\end{equation}
where $\beta_{1}, \dots, \beta{p} > 0$ are parameters. The full model is defined by
\begin{equation}
X_{t} = c + \sum_{i = 1}^{p}\varphi_{i}X_{t - i} + z_{t}\sqrt{\omega + \sum_{i = 1}^{q}\alpha_{i}\varepsilon^{2}_{t - i} + \sum_{i = 1}^{p}\beta_{i}\sigma^{2}_{i - 1}}
\end{equation}

\subsection{Vector Autoregressive (VAR) Models}
A VAR model is a generalisation of the AR model, and is denoted by  $VAR\left(p\right)$. The model is defined by
\begin{equation}
X_{t} = c + \sum_{i = 1}^{p}A_{i}X_{t - i} + \varepsilon_{t}
\end{equation}
where $X_{t}$ is a stochastic vector, $c$ is a constant vector, $A_{1}, \dots, A_{p}$ are square matrix parameters, and $\varepsilon_{t}$ is a zero-mean vector error term with no correlation across time, ie. $\operatorname{E}\left[\varepsilon_{t}\varepsilon_{t - k}^{\top}\right] = 0$ for any non-zero $k$.

\subsection{Nonlinear Autoregressive Exogeneous (NARX) Models}
A NARX model is a generalisation of the AR model, and is denoted by $NARX\left(p, b\right)$. The model is defined by
\begin{equation}
X_{t} = F\left(X_{t - 1}, \dots, X_{t - p}, d_{t}, d_{t - 1}, \dots, d_{t - b}\right) + \varepsilon_{t}
\end{equation}
where $F$ is a nonlinear function.

\section{Time-Series Regression}

\subsection{Residual Autocorrelation}

Also known as serial correlation in the residuals, residual autocorrelation is when there is evidence of correlation between the residuals and past lags. This suggests that
\begin{equation}
\mathbb{E}\left[U_{t}\middle|\mathcal{F}_{t - 1}\right] \neq 0
\end{equation}
where $U_{t}$ is the error term and $\mathcal{F}_{t - 1}$ is information up to and including time $t - 1$. What this is saying that if errors are autocorrelated, then in principle it is possible to predict future errors from past information. However this goes against the definition of the error, which is
\begin{equation}
U_{t} = Y_{t} - \mathbb{E}\left[Y_{t}\middle|\mathcal{F}_{t - 1}\right]
\end{equation}
Taking $\mathbb{E}\left[\cdot\middle|\mathcal{F}_{t - 1}\right]$ of both sides, we get
\begin{equation}
\mathbb{E}\left[U_{t}\middle|\mathcal{F}_{t - 1}\right] = \mathbb{E}\left[Y_{t}\middle|\mathcal{F}_{t - 1}\right] - \mathbb{E}\left[Y_{t}\middle|\mathcal{F}_{t - 1}\right]
\end{equation}
The RHS evaluates to zero, however evidence of residual autocorrelation suggests the LHS is not zero, which results in a contradiction. Hence this gives the implication that the model specification is not the correct specification for the actual conditional mean.

\subsection{Granger Causality}

\subsection{Unit Root}

\subsubsection{Stationarity in AR(1) Models}

Consider an AR(1) model
\begin{equation}
\mathbb{E}\left[Y_{t} \middle|\mathcal{F}_{t-1}\right] = \beta_{0} + \beta_{1}Y_{t - 1}
\end{equation}
with $\left|\beta_{1}\right| < 1$. Using the Law of Expectations,
\begin{equation}
\mathbb{E}\left[Y_{t}\right] = \beta_{0} + \beta_{1}\mathbb{E}\left[Y_{t - 1}\right]
\end{equation}
Suppose this time series is weakly stationary, then we have $\mathbb{E}\left[Y_{t}\right] = \mathbb{E}\left[Y_{t - 1}\right] = \mu$. Solving for $\mu$ gives
\begin{equation}
\mu = \dfrac{\beta_{0}}{1 - \beta_{1}}
\end{equation}
Now also suppose we have homoskedasticity, ie. the conditional variance of $Y_{t}$ is a constant given by $\operatorname{Var}\left(Y_{t}\middle|\mathcal{F}_{t-1}\right) = \omega^{2}$. Then using the Law of Total Variance:
\begin{align}
\operatorname{Var}\left(Y_{t}\right) &= \mathbb{E}\left[\operatorname{Var}\left(Y_{t}\middle|\mathcal{F}_{t-1}\right)\right] + \operatorname{Var}\left( \mathbb{E}\left[Y_{t}\middle|\mathcal{F}_{t-1}\right]\right) \\
&= \mathbb{E}\left[\omega^{2}\right] + \operatorname{Var}\left(\beta_{0} + \beta_{1}Y_{t - 1}\right) \\
&= \omega^{2} + \beta_{1}\operatorname{Var}\left(Y_{t - 1}\right)
\end{align}
From weak stationarity we have $\operatorname{Var}\left(Y_{t}\right) = \operatorname{Var}\left(Y_{t - 1}\right) = \sigma^{2}$. Solving for $\sigma^{2}$ gives
\begin{equation}
\sigma^{2} = \dfrac{\omega^{2}}{1 - \beta_{1}^{2}}
\end{equation}
To determine $\operatorname{Cov}\left(Y_{t}, Y_{t - j}\right)$ for an arbitrary lag length $j$, for convenience define $Y_{t}' := Y_{t} - \mu$ so $\mathbb{E}\left[Y_{t}'\right] = 0$ which then implies
\begin{equation}
\mathbb{E}\left[Y_{t}'\middle|\mathcal{F}_{t-1}\right] = \beta_{1}Y_{t - 1}'
\end{equation}
and
\begin{equation}
\mathbb{E}\left[Y_{t}'\middle|\mathcal{F}_{t-j}\right] = \beta_{1}^{j}Y_{t - j}'
\end{equation}
Multiplying both sides by $Y_{t - j}'$ gives
\begin{equation}
Y_{t - j}'\mathbb{E}\left[Y_{t}'\middle|\mathcal{F}_{t-j}\right] = \beta_{1}^{j}Y_{t - j}'^{2}
\end{equation}
Since $Y_{t - j}' \in \mathcal{F}_{t- j}$ then
\begin{equation}
\mathbb{E}\left[Y_{t}'Y_{t - j}'\middle|\mathcal{F}_{t-j}\right] = \beta_{1}^{j}Y_{t - j}'^{2}
\end{equation}
Taking expectations of both sides yields
\begin{equation}
\mathbb{E}\left[\mathbb{E}\left[Y_{t}'Y_{t - j}'\middle|\mathcal{F}_{t-j}\right]\right] = \beta_{1}^{j}\mathbb{E}\left[Y_{t - j}'^{2}\right]
\end{equation}
Then by the Law of Iterated Expectations
\begin{equation}
\mathbb{E}\left[Y_{t}'Y_{t - j}'\right] = \beta_{1}^{j}\mathbb{E}\left[Y_{t - j}'^{2}\right]
\end{equation}
As $Y_{t}'$ and $Y_{t - j}'$ are zero mean, this can be rewritten as
\begin{equation}
\operatorname{Cov}\left(Y_{t}', Y_{t - j}'\right) = \beta_{1}^{j}\operatorname{Var}\left(Y_{t - j}\right)
\end{equation}
And finally by $\operatorname{Cov}\left(Y_{t}, Y_{t - j}\right) = \operatorname{Cov}\left(Y_{t}', Y_{t - j}'\right)$ and weak stationarity this becomes
\begin{align}
\operatorname{Cov}\left(Y_{t}, Y_{t - j}\right) &= \beta_{1}^{j}\operatorname{Var}\left(Y_{t}\right) \\
&= \dfrac{\beta_{1}^{j}\omega^{2}}{1 - \beta_{1}^{2}}
\end{align}

\chapter{Machine Learning}


\section{Classification Problems}

\subsection{Confusion Matrices}

\subsection{Vapnik-Chervonenkis Dimension}

\section{k-Nearest Neighbours}

\section{Neural Networks}
For an input vector $x\in\mathbb{R}^{Nx}$ with $x=\begin{bmatrix}x_{1} & \dots & x_{Nx}\end{bmatrix}^{\top}$, we have an associated (target/observed) output vector $y\in\mathbb{R}^{Ny}$ with $y=\begin{bmatrix}y_{1} & \dots & y_{Ny}\end{bmatrix}^{\top}$. Let there be $L + 1$ layers in the network, with structure given by
\begin{equation}
\left\{Nx,N1,N2,\dots,N\ell,\dots,NL\right\} \in \mathbb{N}^{L + 1}
\end{equation}
which denotes the number of nodes (size) of each layer. Note that $NL\equiv Ny$. For each layer excluding the input layer, there is an `intermediate output' $\mathstrut^{\ell}z\in\mathbb{R}^{N\ell}$, an activation $\mathstrut^{\ell}a\in\mathbb{R}^{N\ell}$, a bias $\mathstrut^{\ell}b\in\mathbb{R}^{N\ell}$, some weights $\mathstrut_{\ell-1}^{\ell}w\in\mathbb{R}^{N\ell\times N\left(\ell-1\right)}$ and an activation function $\mathstrut^{\ell}\sigma\left(\cdot\right)$. Their relationship in each layer from the one before it is given by
\begin{equation}
\mathstrut^{\ell}z=\mathstrut_{\ell-1}^{\ell}w\mathstrut^{\ell-1}a+\mathstrut^{\ell}b
\end{equation}
and $\mathstrut^{0}a$ can be taken to equal $x$. Denote
\begin{equation}
\mathstrut^{\ell}\boldsymbol{\sigma}\left(\mathstrut^{\ell}z\right)=\begin{bmatrix}\mathstrut^{\ell}\sigma\left(\mathstrut^{\ell} z_{1}\right)\\
\vdots\\
\mathstrut^{\ell}\sigma\left(\mathstrut^{\ell}z_{N}\right)
\end{bmatrix}
\end{equation}
so that
\begin{equation}
\mathstrut^{\ell}a=\mathstrut^{\ell}\boldsymbol{\sigma}\left(\mathstrut^{\ell}z\right)
\end{equation}
The feedforward function is
\begin{equation}
f\left(x\right)=\mathstrut^{L}a=\mathstrut^{L}\boldsymbol{\sigma}\left(\mathstrut_{L-1}^{L}w\mathstrut^{L-1}a+\mathstrut^{L}b\right)
\end{equation}
Suppose we have a training data set indexed by
\begin{gather}
X=\left\{ x\left[1\right],\dots,x\left[n\right]\right\} \\
Y=\left\{ y\left[1\right],\dots,y\left[n\right]\right\} 
\end{gather}
Use the cost function
\begin{equation}
C=\dfrac{1}{2n}\sum_{i=1}^{n}\left\Vert f\left(x\left[i\right]\right)-y\left[i\right]\right\Vert ^{2}
\end{equation}
This can be broken up into a cost for each training point:
\begin{equation}
C_{i}=\dfrac{1}{2}\left\Vert f\left(x\left[i\right]\right)-y\left[i\right]\right\Vert ^{2}=\dfrac{1}{2}\left(f_{1}\left(x\left[i\right]\right)-y_{1}\left[i\right]\right)^{2}+\dots+\dfrac{1}{2}\left(f_{Ny}\left(x\left[i\right]\right)-y_{Ny}\left[i\right]\right)^{2}
\end{equation}
so that
\begin{equation}
C=\dfrac{1}{n}\sum_{i=1}^{n}C_{i}
\end{equation}
Using this cost function, the gradient descent equations for the weights and biases in each layer are
\begin{gather}
\mathstrut_{\ell-1}^{\ell}w\rightarrow\mathstrut_{\ell-1}^{\ell}w-\eta\dfrac{\partial C}{\partial\mathstrut_{\ell-1}^{\ell}w}^{\top} \\
\mathstrut^{\ell}b\rightarrow\mathstrut^{\ell}b-\eta\dfrac{\partial C}{\partial\mathstrut^{\ell}b}^{\top}
\end{gather}
where $\eta$ is the learning rate. For ease of notation we will use for a given $i$: $c := C_{i}$, $f := f\left(x\right)\left[i\right]$ and $y:= y\left[i\right]$. For example, instead of using the cumbersome notation for the gradient vector
\begin{equation}
\nabla_{f}C_{i}=\dfrac{\partial C_{i}}{\partial f}^{\top}=\begin{bmatrix}\dfrac{\partial C_{i}}{\partial f_{1}} & \dots & \dfrac{\partial C_{i}}{\partial f_{Ny}}\end{bmatrix}^{\top}=\begin{bmatrix}f_{1}\left(x\left[i\right]\right)-y_{1}\left[i\right]\\
\vdots\\
f_{Ny}\left(x\left[i\right]\right)-y_{Ny}\left[i\right]
\end{bmatrix}
\end{equation}
we can instead write
\begin{equation}
\nabla_{f}c=\begin{bmatrix}f_{1}-y_{1}\\
\vdots\\
f_{Ny}-y_{Ny}
\end{bmatrix}
\end{equation}
To make our way to finding $\dfrac{\partial C}{\partial\mathstrut_{\ell-1}^{\ell}w}$ and $\dfrac{\partial C}{\partial\mathstrut^{\ell}b}$, first consider $\nabla_{\mathstrut^{L}z}c$. We have from the chain rule
\begin{equation}
\nabla_{\mathstrut^{L}z}c=\begin{bmatrix}\dfrac{\partial c}{\partial\mathstrut^{L}z_{1}}\\
\vdots\\
\dfrac{\partial c}{\partial\mathstrut^{L}z_{NL}}
\end{bmatrix}=\begin{bmatrix}\dfrac{\partial c}{\partial f_{1}}\times\dfrac{\partial f_{1}}{\partial\mathstrut^{L}z_{1}}\\
\vdots\\
\dfrac{\partial c}{\partial f_{NL}}\times\dfrac{\partial f_{NL}}{\partial\mathstrut^{L}z_{NL}}
\end{bmatrix}=\begin{bmatrix}\dfrac{\partial c}{\partial f_{1}}\cdot\mathstrut^{L}\sigma'\left(\mathstrut^{L}z\right)\\
\vdots\\
\dfrac{\partial c}{\partial f_{NL}}\cdot\mathstrut^{L}\sigma'\left(\mathstrut^{L}z\right)
\end{bmatrix}=\nabla_{f}c\odot\mathstrut^{L}\boldsymbol{\sigma}'\left(\mathstrut^{L}z\right)
\end{equation}
where $\odot$ is the Hadamard (element-wise) product. Denote the `error' in each layer as $\mathstrut^{\ell}\delta:=\nabla_{\mathstrut^{\ell}z}c$. For the final layer, we have
\begin{equation}
\mathstrut^{L}z=\mathstrut_{L-1}^{L}w\mathstrut^{L-1}a+\mathstrut^{L}b
\end{equation}
It can be easily seen that $\dfrac{\partial\mathstrut^{L}z}{\partial\mathstrut^{L}b}=I$, so this gives
\begin{equation}
\dfrac{\partial C}{\partial\mathstrut^{L}b}^{\top}=\nabla_{\mathstrut^{L}z}c=\mathstrut^{L}\delta
\end{equation}
and in general for any layer
\begin{equation}
\dfrac{\partial C}{\partial\mathstrut^{\ell}b}^{\top}=\nabla_{\mathstrut^{\ell}z}c=\mathstrut^{\ell}\delta
\end{equation}
For the following steps use for ease of notation: $z:=\mathstrut^{\ell}z$, $b:=\mathstrut^{\ell}b$,  $a:=\mathstrut^{\ell-1}a$ and $w:=\mathstrut_{\ell-1}^{\ell}w$. Then write out the following sums:
\begin{equation}
\begin{bmatrix}z_{1}\\
z_{2}\\
\vdots\\
z_{N}
\end{bmatrix}=\begin{bmatrix}w_{11}a_{1}+w_{12}a_{2}+\dots+w_{1N}a_{N}+b_{1}\\
w_{21}a_{1}+w_{22}a_{2}+\dots+w_{2N}a_{N}+b_{2}\\
\vdots\\
w_{N1}a_{1}+w_{N2}a_{2}+\dots+w_{NN}a_{N}+b_{N}
\end{bmatrix}
\end{equation}
This is helpful for seeing that if we compute the derivative of scalar by matrix $\dfrac{\partial z_{1}}{\partial w}$, this gives
\begin{equation}
\dfrac{\partial z_{1}}{\partial w}=\begin{bmatrix}\dfrac{\partial z_{1}}{\partial w_{11}} & \dfrac{\partial z_{1}}{\partial w_{21}} & \dots\\
\dfrac{\partial z_{1}}{\partial w_{12}} & \ddots\\
\vdots
\end{bmatrix}=\begin{bmatrix}a_{1} & 0 & \dots\\
a_{2} & 0 & \dots\\
\vdots & \vdots & \ddots
\end{bmatrix}
\end{equation}
The derivative of the cost with respect to the weights are
\begin{equation}
\dfrac{\partial c}{\partial w}=\begin{bmatrix}\dfrac{\partial c}{\partial w_{11}} & \dfrac{\partial c}{\partial w_{21}} & \dots\\
\dfrac{\partial c}{\partial w_{12}} & \ddots\\
\vdots
\end{bmatrix}
\end{equation}
Consider the first element, this will consist of all the contributions from elements of $z$ as so
\begin{equation}
\dfrac{\partial c}{\partial w_{11}}=\dfrac{\partial c}{\partial z_{1}}\dfrac{\partial z_{1}}{\partial w_{11}}+\dfrac{\partial c}{\partial z_{2}}\green\cancelto{0}{\black\dfrac{\partial z_{2}}{\partial w_{11}}}\black+\green\cancelto{0}{\black\left[\dots\right]}\black=\dfrac{\partial c}{\partial z_{1}}a_{1}
\end{equation}
The element $\dfrac{\partial c}{\partial w_{12}}$ is similarly given by
\begin{equation}
\dfrac{\partial c}{\partial w_{12}}=\dfrac{\partial c}{\partial z_{1}}\dfrac{\partial z_{1}}{\partial w_{12}}+\dfrac{\partial c}{\partial z_{2}}\green\cancelto{0}{\black\dfrac{\partial z_{2}}{\partial w_{12}}}\black+\green\cancelto{0}{\black\left[\dots\right]}\black=\dfrac{\partial c}{\partial z_{1}}a_{2}
\end{equation}
Thus
\begin{equation}
\dfrac{\partial c}{\partial w}^{\top}=\begin{bmatrix}\dfrac{\partial c}{\partial w_{11}} & \dfrac{\partial c}{\partial w_{12}} & \dots\\
\dfrac{\partial c}{\partial w_{21}} & \ddots\\
\vdots
\end{bmatrix}=\begin{bmatrix}\dfrac{\partial c}{\partial z_{1}}a_{1} & \dfrac{\partial c}{\partial z_{1}}a_{2} & \dots\\
\dfrac{\partial c}{\partial z_{2}}a_{1} & \ddots\\
\vdots
\end{bmatrix}=\begin{bmatrix}\dfrac{\partial c}{\partial z_{1}}\\
\dfrac{\partial c}{\partial z_{2}}\\
\vdots
\end{bmatrix}\begin{bmatrix}a_{1} & a_{2} & \dots\end{bmatrix}
\end{equation}
Reverting back to notation which specifies the layers, we have
\begin{equation}
\dfrac{\partial c}{\partial\mathstrut_{\ell-1}^{\ell}w}^{\top}=\mathstrut^{\ell}\delta\cdot\mathstrut^{\ell-1}a^{\top}
\end{equation}
Next, consider the relationship between $\mathstrut^{\ell - 1}\delta$ and $\mathstrut^{\ell}\delta$. Computing $\mathstrut^{\ell - 1}\delta$ from $\mathstrut^{\ell}\delta$ is known as backpropogation. First write
\begin{equation}
\mathstrut^{\ell}z=\mathstrut_{\ell-1}^{\ell}w\mathstrut^{\ell}\boldsymbol{\sigma}\left(\mathstrut^{\ell-1}z\right)+\mathstrut^{\ell}b
\end{equation}
For ease of notation, use $\sigma\left(\cdot\right):=\mathstrut^{\ell}\sigma\left(\cdot\right)$. Then writing out the sums arising from the matrix multiplication gives
\begin{equation}
\begin{bmatrix}\mathstrut^{\ell}z_{1}\\
\mathstrut^{\ell}z_{2}\\
\vdots
\end{bmatrix}=\begin{bmatrix}\mathstrut_{\ell-1}^{\ell}w_{11}\sigma\left(\mathstrut^{\ell-1}z_{1}\right)+\mathstrut_{\ell-1}^{\ell}w_{12}\sigma\left(\mathstrut^{\ell-1}z_{2}\right)+\dots+\mathstrut^{\ell}b_{1}\\
\mathstrut_{\ell-1}^{\ell}w_{21}\sigma\left(\mathstrut^{\ell-1}z_{1}\right)+\mathstrut_{\ell-1}^{\ell}w_{22}\sigma\left(\mathstrut^{\ell-1}z_{2}\right)+\dots+\mathstrut^{\ell}b_{2}\\
\vdots
\end{bmatrix}
\end{equation}
We want to find
\begin{equation}
\mathstrut^{\ell-1}\delta=\begin{bmatrix}\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}\\
\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{2}}\\
\vdots
\end{bmatrix}
\end{equation}
First consider $\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}$ as the contribution from all $\mathstrut^{\ell}z_{1}$, $\mathstrut^{\ell}z_{2}$, etc.
\begin{equation}
\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}=\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{1}}\cdot \red \dfrac{\partial\mathstrut^{\ell}z_{1}}{\partial\mathstrut^{\ell-1}z_{1}}\black +\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{2}}\cdot \blue \dfrac{\partial\mathstrut^{\ell}z_{2}}{\partial\mathstrut^{\ell-1}z_{1}}\black+\dots
\end{equation}
Calculating some of these terms using the chain rule gives
\begin{gather}
\red\dfrac{\partial\mathstrut^{\ell}z_{1}}{\partial\mathstrut^{\ell-1}z_{1}}\black=\mathstrut_{\ell-1}^{\ell}w_{11}\sigma'\left(\mathstrut^{\ell-1}z_{1}\right) \\
\blue\dfrac{\partial\mathstrut^{\ell}z_{2}}{\partial\mathstrut^{\ell-1}z_{1}}\black=\mathstrut_{\ell-1}^{\ell}w_{21}\sigma'\left(\mathstrut^{\ell-1}z_{1}\right) \\
\vdots
\end{gather}
So then
\begin{equation}
\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}=\left(\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{1}}\cdot\mathstrut_{\ell-1}^{\ell}w_{11}+\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{2}}\cdot\mathstrut_{\ell-1}^{\ell}w_{21}+\dots\right)\sigma'\left(\mathstrut^{\ell-1}z_{1}\right)
\end{equation}
and more generally,
\begin{equation}
\begin{bmatrix}\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{1}}\\
\dfrac{\partial c}{\partial\mathstrut^{\ell-1}z_{2}}\\
\vdots
\end{bmatrix}=\begin{bmatrix}\left(\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{1}}\cdot\mathstrut_{\ell-1}^{\ell}w_{11}+\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{2}}\cdot\mathstrut_{\ell-1}^{\ell}w_{21}+\dots\right)\sigma'\left(\mathstrut^{\ell-1}z_{1}\right)\\
\left(\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{1}}\cdot\mathstrut_{\ell-1}^{\ell}w_{12}+\dfrac{\partial c}{\partial\mathstrut^{\ell}z_{2}}\cdot\mathstrut_{\ell-1}^{\ell}w_{22}+\dots\right)\sigma'\left(\mathstrut^{\ell-1}z_{2}\right)\\
\vdots
\end{bmatrix}
\end{equation}
Notice that we may then write this as
\begin{equation}
\mathstrut^{\ell-1}\delta=\left(\mathstrut_{\ell-1}^{\ell}w^{\top}\cdot\mathstrut^{\ell}\delta\right)\odot\mathstrut^{\ell-1}\boldsymbol{\sigma}'\left(\mathstrut^{\ell-1}z\right)
\end{equation}
Now reverting back to notation that is indexed by each training point $i$: $\mathstrut^{\ell}\delta\left[i\right]=\dfrac{\partial C_{i}}{\partial\mathstrut^{\ell}z}$. Since we have found the derivative of each individual cost with respect to the weights, the derivative of the total cost with respect to the weights is
\begin{gather}
\dfrac{\partial C}{\partial\mathstrut_{\ell-1}^{\ell}w}^{\top} = \dfrac{1}{n}\sum_{i=1}^{n}\dfrac{\partial C_{i}}{\partial\mathstrut_{\ell-1}^{\ell}w}^{\top} = \dfrac{1}{n}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]\cdot\mathstrut^{\ell-1}a\left[i\right]^{\top} \\
\dfrac{\partial C}{\partial\mathstrut^{\ell}b}^{\top}= \dfrac{1}{n}\sum_{i=1}^{n}\dfrac{\partial C_{i}}{\partial\mathstrut^{\ell}b}^{\top} = \dfrac{1}{n}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]
\end{gather}
So the gradient descent equations using the entire data set are
\begin{gather}
\mathstrut_{\ell-1}^{\ell}w\rightarrow\mathstrut_{\ell-1}^{\ell}w-\eta\cdot\dfrac{1}{n}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]\cdot\mathstrut^{\ell-1}a\left[i\right]^{\top} \\
\mathstrut^{\ell}b\rightarrow\mathstrut^{\ell}b-\eta\cdot\dfrac{1}{n}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]
\end{gather}
If the number of training points is very large, then gradient descent can take a long time, so learning occurs slowly. An idea is to use stochastic gradient descent, where a random mini-batch of $m$ inputs is chosen to approximate the gradient of $C$ and update the weights according to
\begin{gather}
\mathstrut_{\ell-1}^{\ell}w\rightarrow\mathstrut_{\ell-1}^{\ell}w-\eta\cdot\dfrac{1}{m}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]\cdot\mathstrut^{\ell-1}a\left[i\right]^{\top} \\
\mathstrut^{\ell}b\rightarrow\mathstrut^{\ell}b-\eta\cdot\dfrac{1}{m}\sum_{i=1}^{n}\mathstrut^{\ell}\delta\left[i\right]
\end{gather}
Then another mini-batch is chosen to update the weights. This occurs until the entirety of the training set is exhausted, in which this is called one training epoch. We may then start over again with a new epoch to continue training.

\subsection{Cross Entropy Loss Function}

In a classification model, we have data with training examples from $1, \dots, n$, and the outputs are the class probabilities for each of $1, \dots, M$ different classes. Denote by $y_{i, j}$ the $i$\textsuperscript{th} class probability of the $j$\textsuperscript{th} training example. Each training example target vector $\mathbf{y}_{j}$ should really be (but in a more general setting is not restricted to) a `one-hot' vector, eg. $\left(0, 1, 0\right)$. Similarly denote the estimates of class probabilities by the model as $\hat{y}_{i, j}$. A choice of loss function $L$ is the cross entropy loss function, given by
\begin{equation}
L\left(\mathbf{y}, \hat{\mathbf{y}}\right) = -\sum_{i = 1}^{M}y_{i}\log\hat{y}_{i}
\end{equation}
which is just the cross entropy between the `true' distribution $\mathbf{y}$ and the approximating distribution $\hat{\mathbf{y}}$. Then because the difference between the cross entropy and the KL divergence is only an additive constant (which is the entropy of $\mathbf{y}$, which we treat as fixed in the data), then minimising the cross entropy is the same as minimising the KL divergence between the estimated distribution and the actual distribution. Over the entire dataset, by assuming independence we can just sum over the cross entropy for each training example, leading to the cost function
\begin{equation}
J = -\sum_{j = 1}^{n}\sum_{i = 1}^{M}y_{i, j}\log\hat{y}_{i, j}
\end{equation}
Now suppose the estimated vector $\hat{y}$ is applied via a softmax of some values $z_{1}, \dots, z_{M}$, ie.
\begin{equation}
\hat{y}_{i} = \dfrac{\exp{\left(z_{i}\right)}}{\sum_{l = 1}^{M}\exp\left(z_{l}\right)}
\end{equation}
Then to derive the derivative of the loss with respect with the values $z_{1}, \dots, z_{M}$ (for use in backpropagation as an example), we get (for a particular $z_{k}$):
\begin{align}
\dfrac{\partial L\left(\mathbf{y}, \hat{\mathbf{y}}\right)}{\partial z_{k}} &= \dfrac{\partial}{\partial z_{k}}\left(-\sum_{i = 1}^{M}y_{i}\log \hat{y}_{i}\right) \\
&= \dfrac{\partial}{\partial z_{k}}\left(-\sum_{i=1}^{M}y_{i}\log\left(\dfrac{\exp\left(z_{i}\right)}{\sum_{l=1}^{M}\exp\left(z_{l}\right)}\right)\right) \\
&= \dfrac{\partial}{\partial z_{k}}\left(-\sum_{i=1}^{M}y_{i}z_{i}+\sum_{i=1}^{M}y_{i}\log\left(\sum_{l=1}^{M}\exp\left(z_{l}\right)\right)\right) \\
&= -y_{k}+\sum_{i=1}^{M}y_{i}\dfrac{\exp\left(z_{k}\right)}{\sum_{l=1}^{M}\exp\left(z_{l}\right)} \\
&= -y_{k}+\hat{y}_{k}\sum_{i=1}^{M}y_{i} \\
&= -y_{k}+\hat{y}_{k}
\end{align}
Hence the gradient vector is
\begin{equation}
\nabla_{\mathbf{z}}L\left(\mathbf{y}, \hat{\mathbf{y}}\right) = - \mathbf{y} + \hat{\mathbf{y}}
\end{equation}
and the gradient vector of the cost function is the sum over the examples
\begin{equation}
\nabla_{\mathbf{z}} J = \sum_{j = 1}^{n}\left(\hat{\mathbf{y}}_{j} - \mathbf{y}_{j}\right)
\end{equation}
We can arrive at the cross entropy cost function by considering a maximum likelihood approach. Suppose $\bar{\mathbf{y}}_{j}$ gives the actual probability distribution for example $j$. The likelihood of the data $\mathbf{y}_{1}, \dots, \mathbf{y}_{n}$ given $\overline{\mathbf{y}}_{j}$ is denoted $\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right)$ and represented by
\begin{equation}
\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right) = \operatorname{Pr}\left(\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\middle|\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\right)
\end{equation}
In the case $\mathbf{y}_{j}$ are one-hot vectors and all independent, this becomes
\begin{equation}
\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right) = \prod_{j = 1}^{n}\overline{y}_{\left\{i: y_{i, j} = 1\right\}, j}
\end{equation}
where $\left\{i: y_{i, j} = 1\right\}$ express the index of the class for which was actually observed in the $j$\textsuperscript{th} training example. Hence $\overline{y}_{\left\{i: y_{i, j} = 1\right\}, j}$ is just the probability of the class that was actually observed. Note that if $\mathbf{y}_{j}$ were not one-hot vectors, this still generalises well to
\begin{equation}
\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right) = \prod_{j = 1}^{n}\prod_{i = 1}^{M}\overline{y}_{i, j}\mathstrut^{y_{i, j}}
\end{equation}
To illustrate, consider a $n = 100$ trials in an i.i.d. multinomial experiment with $M = 3$ for example. If we obtained 40 in class 1, 25 in class 2 and 35 in  class 3, then the likelihood would be written as
\begin{align}
\mathcal{L}\left(\overline{y}_{1}, \overline{y}_{2}, \overline{y}_{3}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{100}\right) &= \overline{y}_{1}^{40}\overline{y}_{2}^{25}\overline{y}_{3}^{35} \\
&= \left(\overline{y}_{1}^{0.4}\overline{y}_{2}^{0.25}\overline{y}_{3}^{0.35}\right)^{100}
\end{align}
Thus we can more generally view the likelihood as the product over all the training examples of the weighted geometric mean of the true class probabilities $\overline{\mathbf{y}}_{j}$ (weighted by the class probabilities for that training example). Taking the negative log likelihood gives
\begin{equation}
-\log\mathcal{L}\left(\overline{\mathbf{y}}_{1}, \dots, \overline{\mathbf{y}}_{n}\middle|\mathbf{y}_{1}, \dots, \mathbf{y}_{n}\right) = -\sum_{j = 1}^{n}\sum_{i = 1}^{M}\overline{y}_{i, j}\log{y_{i, j}}
\end{equation}
If we find the estimates $\hat{\mathbf{y}}_{j}$ (in terms of the parameters of the model) of $\overline{\mathbf{y}}_{j}$ which minimise this negative log likelihood, then it becomes the same problem as minimising the cross entropy.

\section{Ensemble Methods}

\subsection{Boosting}

\section{Random Forests}



\chapter{Statistical Signal Processing}

\section{Wiener Filtering}

\section{Kalman Filtering}

\subsection{Linearised Kalman Filter}

\subsection{Extended Kalman Filter}

\subsection{Unscented Kalman Filter}

\subsection{Kalman-Bucy Filter}

\section{Particle Filtering}

\section{Kalman Smoother}

\section{Viterbi Algorithm}

\section{Wavelets}

\chapter{Stochastic Control}

\section{Linear Quadratic Gaussian}

\section{Stochastic Model Predictive Control}

\section{Stochastic Stability}

\section{Reinforcement Learning}

\chapter{Quantitative Finance}

\section{Portfolio Optimisation}

\subsection{Kelly Criterion}

Suppose an opportunity to place a bet yields $b$ net odds (ie. for every \$1 wagered, a win results in $1 + b$ return, while a loss results in no return). The probability of winning the bet is given by $p$. The Kelly Criterion investigates the optimal betting amount for this bet. Assume the bettor has a log utility function of wealth, given by
\begin{equation}
u\left(W\right) = \log W
\end{equation}
The Kelly Criterion maximises the expected utility (hence expected log wealth) from an initial wealth $W_{0}$ and a wager $w$:
\begin{equation}
\mathbb{E}\left[u\left(W\right)\right] = p\log\left(W_{0} + wb\right) + \left(1 - p\right)\log\left(W_{0} - w\right)
\end{equation}
For further simplicity, denote the betting fraction $f = w/W_{0}$ so that
\begin{equation}
\mathbb{E}\left[u\left(W\right)\right] = \dfrac{p\log\left(1 + fb\right) + \left(1 - p\right)\log\left(1 - f\right)}{W_{0}}
\end{equation}
We find the optimal betting fraction $f^{*}$ to maximise expected utility. Note that the expected utility is a concave function. Taking the derivative yields
\begin{equation}
\dfrac{\partial \mathbb{E}\left[u\left(W\right)\right]}{\partial f} = \dfrac{1}{W_{0}}\left(\dfrac{pb}{1 + fb} + \dfrac{p - 1}{1 - f}\right)
\end{equation}
Setting this to zero gives
\begin{gather}
\dfrac{1}{W_{0}}\left(\dfrac{pb}{1 + f^{*}b} + \dfrac{p - 1}{1 - f^{*}}\right) = 0 \\
pb\left(1 - f^{*}\right) = \left(1 + f^{*}b\right)\left(1 - p\right) \\
f^{*} = \dfrac{pb + p - 1}{b}
\end{gather}
Letting $q := 1 - p$, we rewrite this as
\begin{equation}
f^{*} = \dfrac{pb - q}{b}
\end{equation}

\subsection{Markowitz Portfolio Theory}

\section{Black-Scholes Model}

\section{Optimal Stopping Theorem}

\chapter{Actuarial Science}

\section{Ruin Theory}

\chapter{Statistical Physics}

\section{Mean Sojourn Time}

\section{Mean Field Theory}

%\nocite{*} % adds all entries in the bib file to the bibliography
\bibliography{prob_stats_references}{}
\bibliographystyle{plain}

\end{document}

%\begin{figure}[H]
%\includegraphics[width=1\textwidth]{figures/pred_int_all}\centering
%\caption{A prediction interval is constructed about $\hat{\operatorname{E}}\left[Y\middle|X = x_{d}\right]$ which does contain the new observed value of $y$.}
%\end{figure}